2025-04-17 12:05:01,840 - asr_demo - INFO - Using device: cuda
2025-04-17 12:05:01,841 - asr_demo - INFO - Loading ASR model kamo-naoyuki/librispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp_valid.acc.ave (first run can take ~1 min)…
2025-04-17 12:12:36,951 - asr_demo - INFO - Model downloaded successfully to: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo
2025-04-17 12:12:36,952 - asr_demo - INFO - Model dict keys: ['asr_train_config', 'lm_train_config', 'asr_model_file', 'lm_file']
2025-04-17 12:12:36,954 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp/config.yaml
2025-04-17 12:12:37,796 - root - INFO - Vocabulary size: 5000
2025-04-17 12:12:37,863 - root - WARNING - Using legacy_rel_pos and it will be deprecated in the future.
2025-04-17 12:12:37,912 - root - WARNING - Using legacy_rel_selfattn and it will be deprecated in the future.
2025-04-17 12:12:38,384 - root - INFO - Gradient checkpoint layers: []
2025-04-17 12:12:39,205 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/lm_train_lm_transformer2_bpe5000_scheduler_confwarmup_steps25000_batch_bins500000000_accum_grad2_use_amptrue/config.yaml
2025-04-17 12:12:40,029 - root - INFO - Vocabulary size: 5000
2025-04-17 12:12:40,033 - root - INFO - encoder self-attention layer type = self-attention
2025-04-17 12:12:40,443 - espnet2.bin.asr_inference - INFO - Beam_search: BeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (lm): TransformerLM(
      (embed): Embedding(5000, 128)
      (encoder): Encoder(
        (embed): Sequential(
          (0): Linear(in_features=128, out_features=512, bias=True)
          (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): Dropout(p=0.0, inplace=False)
          (3): ReLU()
          (4): Sequential()
        )
        (encoders): MultiSequential(
          (0): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (4): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (5): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (6): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (7): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (8): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (9): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (10): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (11): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (12): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (13): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (14): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (15): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): Linear(in_features=512, out_features=5000, bias=True)
    )
  )
)
2025-04-17 12:12:40,444 - espnet2.bin.asr_inference - INFO - Decoding device=cuda, dtype=float32
2025-04-17 12:12:40,447 - espnet2.bin.asr_inference - INFO - Text tokenizer: SentencepiecesTokenizer(model="/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/data/en_token_list/bpe_unigram5000/bpe.model")
2025-04-17 12:12:40,449 - asr_demo - INFO - ASR model ready. Native SR = 16000 Hz
2025-04-17 12:12:40,571 - asr_demo - INFO - Starting Gradio app...
2025-04-17 12:16:39,125 - asr_demo - INFO - Input audio: sr=48000, length=123840, max_amp=1659583488
2025-04-17 12:16:39,134 - asr_demo - INFO - Saved debug audio sample to debug_samples/input_20250417_121639.wav
2025-04-17 12:16:39,135 - asr_demo - INFO - Resampling from 48000 to 16000
2025-04-17 12:16:41,342 - asr_demo - INFO - Saved debug audio sample to debug_samples/resampled_20250417_121641.wav
2025-04-17 12:16:41,342 - asr_demo - INFO - Audio after preparation: length=41280, max_amp=1644555776.0
2025-04-17 12:16:41,344 - espnet2.bin.asr_inference - INFO - speech length: 41280
2025-04-17 12:16:41,913 - espnet.nets.beam_search - INFO - decoder input length: 39
2025-04-17 12:16:41,914 - espnet.nets.beam_search - INFO - max output length: 39
2025-04-17 12:16:41,914 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 12:16:44,823 - espnet.nets.beam_search - INFO - end detected at 8
2025-04-17 12:16:44,828 - espnet.nets.beam_search - INFO -  -2.68 * 0.7 =  -1.87 for decoder
2025-04-17 12:16:44,829 - espnet.nets.beam_search - INFO - -13.14 * 1.0 = -13.14 for lm
2025-04-17 12:16:44,830 - espnet.nets.beam_search - INFO -  -1.46 * 0.3 =  -0.44 for ctc
2025-04-17 12:16:44,831 - espnet.nets.beam_search - INFO - total log probability: -15.45
2025-04-17 12:16:44,831 - espnet.nets.beam_search - INFO - normalized log probability: -5.15
2025-04-17 12:16:44,832 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 29
2025-04-17 12:16:44,832 - espnet.nets.beam_search - INFO - best hypo: ▁AH

2025-04-17 12:16:44,931 - asr_demo - INFO - ASR result: [('AH', ['▁AH'], [946], Hypothesis(yseq=tensor([4999,  946, 4999], device='cuda:0'), score=tensor(-15.4484, device='cuda:0'), scores={'decoder': tensor(-2.6776, device='cuda:0'), 'lm': tensor(-13.1354, device='cuda:0'), 'ctc': tensor(-1.4623, device='cuda:0')}, states={'decoder': [tensor([[[  9.6461,  15.2141,  -4.8270,  ..., -96.9389,  13.0783, -36.8327],
         [  2.5617,  -8.5428, -44.3771,  ...,  19.8294,   9.2762,  -1.2284]]],
       device='cuda:0'), tensor([[[ 13.7131,  23.5537,  -6.2610,  ..., -91.0175,   8.3533, -39.0991],
         [  9.8368,   4.4816, -55.8679,  ...,  19.2917,  13.0378,  -9.9495]]],
       device='cuda:0'), tensor([[[ 23.2448,  47.7567,  -7.5802,  ..., -67.1553,   3.8164, -31.1579],
         [ 24.9815,  29.8038, -42.3424,  ...,  31.5830, -10.2888, -24.3454]]],
       device='cuda:0'), tensor([[[ 34.0319,  55.8956,  17.3198,  ..., -78.0776,  22.3215,  -9.7785],
         [ 45.4730,  41.8424,   5.9570,  ...,  11.6108,  -3.6608,   1.9814]]],
       device='cuda:0'), tensor([[[ 51.4705,  51.0633,  63.3699,  ..., -35.1589,  -2.4511,  -5.8229],
         [ 74.3130,   6.5334,  41.1204,  ...,  67.8180,  -7.0609,  -4.7876]]],
       device='cuda:0'), tensor([[[ 216.1320, -133.2173,  704.8997,  ...,   37.9378,   -2.9486,
          -202.3909],
         [ 271.8558,  -76.9396,  649.4689,  ...,   92.6346,  -25.9975,
          -199.1173]]], device='cuda:0')], 'lm': [tensor([[[-39.7705, -41.4082, -53.9374,  ..., -60.1550, -85.4351, -42.1970],
         [-86.1684, -57.0228, -56.5182,  ..., -91.0746, -55.5357, -34.1406]]],
       device='cuda:0'), tensor([[[-170.7027, -108.4380,  -57.1364,  ..., -103.9612, -142.6913,
           -86.5089],
         [-165.8552,  -91.6345,  -80.0449,  ..., -141.3147,  -40.4975,
           -50.2145]]], device='cuda:0'), tensor([[[-148.5185, -112.3790,  -50.3978,  ..., -122.3693, -127.5089,
           -68.6950],
         [-171.8034,  -83.1430, -132.9310,  ..., -125.6999, -127.6835,
           -92.9063]]], device='cuda:0'), tensor([[[-136.0194, -112.6451,  -56.3242,  ..., -114.0812, -136.8767,
           -65.6614],
         [-226.0904,  -65.9740, -148.5795,  ..., -144.9113, -134.8580,
           -95.5265]]], device='cuda:0'), tensor([[[-125.1060, -128.5430,  -35.0457,  ..., -111.0084, -165.5469,
           -23.4448],
         [-230.7431,  -94.2846, -188.0107,  ..., -118.9557, -128.1772,
           -92.9540]]], device='cuda:0'), tensor([[[-103.6706, -137.5663,  -43.5697,  ..., -100.1561, -150.1787,
           -17.6993],
         [-245.1389, -112.4736, -248.1613,  ...,  -89.3061, -124.0138,
          -120.1752]]], device='cuda:0'), tensor([[[ -46.9828, -109.7889,  -25.0927,  ...,  -83.3409, -135.3886,
           -44.3387],
         [-158.9613, -114.0883, -288.7806,  ...,  -88.6590, -142.2375,
          -188.7988]]], device='cuda:0'), tensor([[[ -47.5229, -144.3886,  -31.8411,  ...,  -78.2817, -137.4689,
           -80.0558],
         [-162.6447, -137.8321, -265.3320,  ...,  -75.2558, -151.5440,
          -244.6480]]], device='cuda:0'), tensor([[[ -75.9904, -108.4232,  -62.1405,  ...,  -46.0407, -156.8006,
          -126.1571],
         [ -69.8982, -160.0737, -332.9496,  ...,  -15.9478, -164.6024,
          -196.2301]]], device='cuda:0'), tensor([[[ -81.8444, -108.9788,  -81.4366,  ...,  -68.3201,  -90.3554,
          -129.2357],
         [-136.3804, -221.1694, -394.7223,  ...,  -17.7172, -161.7088,
          -189.2299]]], device='cuda:0'), tensor([[[ -72.2617, -121.8983,  -71.8085,  ..., -114.5557,  -86.9422,
          -145.2200],
         [-185.4346, -263.9654, -463.3481,  ...,  -74.9263, -224.0935,
          -269.9138]]], device='cuda:0'), tensor([[[-108.7070, -143.9523,  -82.3740,  ..., -120.4336,  -96.7761,
          -159.7680],
         [-118.1011, -323.7253, -397.1220,  ...,  -18.2295, -190.4052,
          -136.8530]]], device='cuda:0'), tensor([[[-118.2413, -145.9342, -107.4488,  ..., -141.1722,  -89.4754,
          -179.1521],
         [-233.6891, -409.8109, -449.8874,  ...,  106.3896, -114.2220,
          -275.0751]]], device='cuda:0'), tensor([[[-147.0713, -120.3623, -159.5060,  ..., -113.3352, -116.5388,
          -202.0317],
         [ -36.9196, -244.7968, -478.8215,  ...,  -21.4004, -237.4556,
          -502.2864]]], device='cuda:0'), tensor([[[-148.1669, -143.0877, -162.6882,  ..., -120.1432, -150.7227,
          -103.1704],
         [-133.2044, -289.4437, -673.3380,  ..., -356.0060, -310.4261,
          -533.0025]]], device='cuda:0'), tensor([[[-201.2815, -139.5632, -136.4038,  ...,  -74.4828, -147.1849,
          -166.9807],
         [-326.7894, -261.1176, -677.1165,  ..., -295.2989, -651.7605,
          -623.9751]]], device='cuda:0')], 'ctc': (-1.4623413, array([[-1.0000000e+10, -1.0000000e+10],
       [-4.7050911e+01, -1.0000000e+10],
       [-4.6185944e+01, -4.7050934e+01],
       [-4.5081039e+01, -4.5834599e+01],
       [-4.2447182e+01, -4.4695889e+01],
       [-3.9464767e+01, -4.2347595e+01],
       [-3.8172131e+01, -3.9411640e+01],
       [-3.6766758e+01, -3.7920502e+01],
       [-3.5599243e+01, -3.6498608e+01],
       [-3.4391262e+01, -3.5272079e+01],
       [-3.3282082e+01, -3.4068966e+01],
       [-3.2712421e+01, -3.2927235e+01],
       [-3.2686203e+01, -3.2132969e+01],
       [-3.2638138e+01, -3.1687214e+01],
       [-3.2546631e+01, -3.1367336e+01],
       [-3.2474899e+01, -3.1105316e+01],
       [-3.2331501e+01, -3.0885191e+01],
       [-3.2198288e+01, -3.0680906e+01],
       [-3.2187824e+01, -3.0490379e+01],
       [-3.2213989e+01, -3.0330774e+01],
       [-3.2305424e+01, -3.0199375e+01],
       [-3.2204994e+01, -3.0099598e+01],
       [-3.2015472e+01, -3.0006790e+01],
       [-3.1814560e+01, -2.9907713e+01],
       [-3.1554996e+01, -2.9800434e+01],
       [-3.1362841e+01, -2.9671978e+01],
       [-3.1089598e+01, -2.9535307e+01],
       [-3.0774864e+01, -2.9376669e+01],
       [-3.0504065e+01, -2.9192087e+01],
       [-3.0135395e+01, -2.8991173e+01],
       [-2.9791529e+01, -2.8748329e+01],
       [-2.9403934e+01, -2.8479301e+01],
       [-2.8964005e+01, -2.8175428e+01],
       [-2.7887650e+01, -2.7836897e+01],
       [-2.6984619e+01, -2.7189152e+01],
       [-2.7599705e+01, -2.6394663e+01],
       [-2.8448135e+01, -2.6134541e+01],
       [-2.6974466e+01, -2.6040380e+01],
       [-2.7008530e+01, -2.5709099e+01]], dtype=float32))}, hs=[]))]
2025-04-17 12:16:44,932 - asr_demo - INFO - ASR processing took 3.589 seconds
2025-04-17 12:16:44,933 - asr_demo - INFO - Raw recognized text: 'AH'
2025-04-17 12:16:44,933 - asr_demo - INFO - Normalized text: 'AH'
2025-04-17 12:16:44,934 - asr_demo - INFO - Metrics: Latency: 3.59s   |   RTF: 1.39
2025-04-17 12:18:12,450 - asr_demo - INFO - Input audio: sr=48000, length=233280, max_amp=1066021248
2025-04-17 12:18:12,455 - asr_demo - INFO - Saved debug audio sample to debug_samples/input_20250417_121812.wav
2025-04-17 12:18:12,455 - asr_demo - INFO - Resampling from 48000 to 16000
2025-04-17 12:18:12,463 - asr_demo - INFO - Saved debug audio sample to debug_samples/resampled_20250417_121812.wav
2025-04-17 12:18:12,463 - asr_demo - INFO - Audio after preparation: length=77760, max_amp=1047169408.0
2025-04-17 12:18:12,464 - espnet2.bin.asr_inference - INFO - speech length: 77760
2025-04-17 12:18:12,517 - espnet.nets.beam_search - INFO - decoder input length: 75
2025-04-17 12:18:12,517 - espnet.nets.beam_search - INFO - max output length: 75
2025-04-17 12:18:12,518 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 12:18:13,783 - espnet.nets.beam_search - INFO - end detected at 8
2025-04-17 12:18:13,785 - espnet.nets.beam_search - INFO -  -4.07 * 0.7 =  -2.85 for decoder
2025-04-17 12:18:13,786 - espnet.nets.beam_search - INFO - -11.62 * 1.0 = -11.62 for lm
2025-04-17 12:18:13,786 - espnet.nets.beam_search - INFO -  -3.15 * 0.3 =  -0.94 for ctc
2025-04-17 12:18:13,787 - espnet.nets.beam_search - INFO - total log probability: -15.42
2025-04-17 12:18:13,787 - espnet.nets.beam_search - INFO - normalized log probability: -5.14
2025-04-17 12:18:13,787 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 28
2025-04-17 12:18:13,788 - espnet.nets.beam_search - INFO - best hypo: ▁OH

2025-04-17 12:18:13,824 - asr_demo - INFO - ASR result: [('OH', ['▁OH'], [294], Hypothesis(yseq=tensor([4999,  294, 4999], device='cuda:0'), score=tensor(-15.4194, device='cuda:0'), scores={'decoder': tensor(-4.0727, device='cuda:0'), 'lm': tensor(-11.6243, device='cuda:0'), 'ctc': tensor(-3.1473, device='cuda:0')}, states={'decoder': [tensor([[[  9.9334,  14.5865,  -5.6462,  ..., -95.9044,  11.7174, -36.8642],
         [-49.2960,  -0.4416, -23.8570,  ..., -10.2383,  -7.0257, -14.3485]]],
       device='cuda:0'), tensor([[[ 13.5532,  23.5683,  -8.2127,  ..., -89.1239,   6.2427, -38.2311],
         [-50.5874,  14.8038, -28.9638,  ..., -12.2068,  -5.6897, -12.9429]]],
       device='cuda:0'), tensor([[[ 23.6366,  47.9790,  -7.5867,  ..., -64.1336,   0.6472, -30.0056],
         [-39.6718,  36.3925, -10.6580,  ...,  -7.0877, -25.1516, -23.9464]]],
       device='cuda:0'), tensor([[[ 32.4839,  54.3337,  17.1110,  ..., -72.2533,  18.8194,  -6.5816],
         [-47.8877,  42.4977,  52.9043,  ...,  -7.9432, -24.7919,  17.8435]]],
       device='cuda:0'), tensor([[[ 48.3737,  54.7919,  68.3719,  ..., -29.2332,  -9.3404,  -6.7165],
         [-58.4924,  49.0745, 127.3512,  ...,  -6.4988, -73.8316, -10.2666]]],
       device='cuda:0'), tensor([[[ 217.8572, -132.7834,  764.8727,  ...,   53.4706,  -20.2289,
          -194.3657],
         [  69.4205,    8.9364,  715.8985,  ...,   21.1927,  -90.6467,
          -186.1900]]], device='cuda:0')], 'lm': [tensor([[[-39.7705, -41.4082, -53.9374,  ..., -60.1550, -85.4351, -42.1970],
         [-16.5875, -49.0455, -49.6298,  ..., -27.9937,  13.9804,  12.6574]]],
       device='cuda:0'), tensor([[[-170.7027, -108.4380,  -57.1364,  ..., -103.9612, -142.6913,
           -86.5089],
         [ -79.3743,  -86.4681,  -62.7857,  ...,  -59.2040,   -5.6714,
            -7.8986]]], device='cuda:0'), tensor([[[-148.5185, -112.3790,  -50.3978,  ..., -122.3693, -127.5089,
           -68.6950],
         [ -85.2193,  -49.4232, -117.7466,  ...,  -39.1028,    7.5195,
           -17.6074]]], device='cuda:0'), tensor([[[-136.0194, -112.6451,  -56.3242,  ..., -114.0812, -136.8767,
           -65.6614],
         [ -69.8368,  -17.2737, -143.8616,  ...,  -53.9154,   25.2713,
           -27.8963]]], device='cuda:0'), tensor([[[-125.1060, -128.5430,  -35.0457,  ..., -111.0084, -165.5469,
           -23.4448],
         [ -59.4470,   -8.3018, -176.6608,  ...,  -81.6777,   23.5389,
           -28.1165]]], device='cuda:0'), tensor([[[-103.6706, -137.5663,  -43.5697,  ..., -100.1561, -150.1787,
           -17.6993],
         [ -58.6853,   -1.6969, -197.2238,  ...,  -64.9202,   13.1657,
           -43.8850]]], device='cuda:0'), tensor([[[ -46.9828, -109.7889,  -25.0927,  ...,  -83.3409, -135.3886,
           -44.3387],
         [  38.4181,    9.1702, -239.4259,  ...,  -68.6377,   45.4268,
           -93.3414]]], device='cuda:0'), tensor([[[ -47.5229, -144.3886,  -31.8411,  ...,  -78.2817, -137.4689,
           -80.0558],
         [  74.9426,   -8.4308, -236.8700,  ..., -124.3614,   46.2235,
          -204.3613]]], device='cuda:0'), tensor([[[ -75.9904, -108.4232,  -62.1405,  ...,  -46.0407, -156.8006,
          -126.1571],
         [  94.8039,  -37.9277, -242.8985,  ..., -103.5658,   41.3739,
          -218.5011]]], device='cuda:0'), tensor([[[ -81.8444, -108.9788,  -81.4366,  ...,  -68.3201,  -90.3554,
          -129.2357],
         [  60.7835,  -16.0975, -293.4579,  ..., -177.8571,    3.8160,
          -213.4378]]], device='cuda:0'), tensor([[[ -72.2617, -121.8983,  -71.8085,  ..., -114.5557,  -86.9422,
          -145.2200],
         [  51.7570,   27.1435, -306.1979,  ..., -248.3280,  -20.6903,
          -237.6202]]], device='cuda:0'), tensor([[[-108.7070, -143.9523,  -82.3740,  ..., -120.4336,  -96.7761,
          -159.7680],
         [  46.1251,   -6.1455, -313.5939,  ..., -242.3394,  -49.9042,
          -257.4998]]], device='cuda:0'), tensor([[[-118.2413, -145.9342, -107.4488,  ..., -141.1722,  -89.4754,
          -179.1521],
         [ 144.0600,  -40.1198, -347.9975,  ..., -140.7154, -114.3355,
          -312.3122]]], device='cuda:0'), tensor([[[-147.0713, -120.3623, -159.5060,  ..., -113.3352, -116.5388,
          -202.0317],
         [ 489.9173,  -67.9552, -440.5681,  ..., -272.1984, -285.7119,
          -467.8430]]], device='cuda:0'), tensor([[[-148.1669, -143.0877, -162.6882,  ..., -120.1432, -150.7227,
          -103.1704],
         [ 615.6603,  136.7246, -592.2883,  ..., -527.9575, -408.0835,
          -628.4088]]], device='cuda:0'), tensor([[[-201.2815, -139.5632, -136.4038,  ...,  -74.4828, -147.1849,
          -166.9807],
         [ 506.5568,  548.5419, -568.0477,  ..., -310.4229, -358.3789,
          -919.7438]]], device='cuda:0')], 'ctc': (-3.1473272, array([[-1.0000000e+10, -1.0000000e+10],
       [-4.5663372e+01, -1.0000000e+10],
       [-4.4531563e+01, -4.5663403e+01],
       [-4.1789371e+01, -4.4252552e+01],
       [-3.9819374e+01, -4.1713398e+01],
       [-3.8876251e+01, -3.9684830e+01],
       [-3.8466526e+01, -3.8512501e+01],
       [-3.8065388e+01, -3.7800144e+01],
       [-3.7656254e+01, -3.7234436e+01],
       [-3.7279663e+01, -3.6733284e+01],
       [-3.7053196e+01, -3.6279274e+01],
       [-3.7047012e+01, -3.5902287e+01],
       [-3.7044712e+01, -3.5627636e+01],
       [-3.7005241e+01, -3.5411964e+01],
       [-3.6926247e+01, -3.5228127e+01],
       [-3.6807308e+01, -3.5061150e+01],
       [-3.6530067e+01, -3.4901619e+01],
       [-3.6222752e+01, -3.4723892e+01],
       [-3.6052162e+01, -3.4523834e+01],
       [-3.6135399e+01, -3.4329075e+01],
       [-3.6313984e+01, -3.4178455e+01],
       [-3.6414112e+01, -3.4068153e+01],
       [-3.6464600e+01, -3.3978096e+01],
       [-3.6495598e+01, -3.3899502e+01],
       [-3.6422955e+01, -3.3828938e+01],
       [-3.6339909e+01, -3.3758305e+01],
       [-3.6246895e+01, -3.3686798e+01],
       [-3.6249401e+01, -3.3613602e+01],
       [-3.6323708e+01, -3.3545456e+01],
       [-3.6394600e+01, -3.3486053e+01],
       [-3.6395397e+01, -3.3433788e+01],
       [-3.6431961e+01, -3.3384220e+01],
       [-3.6425537e+01, -3.3338795e+01],
       [-3.6396317e+01, -3.3295177e+01],
       [-3.6450489e+01, -3.3252163e+01],
       [-3.6360031e+01, -3.3213295e+01],
       [-3.6096504e+01, -3.3173122e+01],
       [-3.5834606e+01, -3.3123779e+01],
       [-3.5985855e+01, -3.3062172e+01],
       [-3.5993500e+01, -3.3012150e+01],
       [-3.5865120e+01, -3.2964760e+01],
       [-3.5586678e+01, -3.2913643e+01],
       [-3.5508778e+01, -3.2849476e+01],
       [-3.5508244e+01, -3.2784412e+01],
       [-3.5459312e+01, -3.2723469e+01],
       [-3.5297810e+01, -3.2663261e+01],
       [-3.5272224e+01, -3.2596275e+01],
       [-3.5215645e+01, -3.2531788e+01],
       [-3.5132145e+01, -3.2467648e+01],
       [-3.5076141e+01, -3.2402195e+01],
       [-3.5019485e+01, -3.2337318e+01],
       [-3.4985886e+01, -3.2272911e+01],
       [-3.4977928e+01, -3.2210415e+01],
       [-3.5071648e+01, -3.2151169e+01],
       [-3.5114197e+01, -3.2100483e+01],
       [-3.5129337e+01, -3.2054905e+01],
       [-3.5159443e+01, -3.2013302e+01],
       [-3.4995773e+01, -3.1977360e+01],
       [-3.4925087e+01, -3.1938498e+01],
       [-3.4706303e+01, -3.1900583e+01],
       [-3.4507015e+01, -3.1853678e+01],
       [-3.4317825e+01, -3.1797190e+01],
       [-3.4084595e+01, -3.1732096e+01],
       [-3.3779663e+01, -3.1652788e+01],
       [-3.3478447e+01, -3.1552090e+01],
       [-3.2968475e+01, -3.1428089e+01],
       [-3.2396652e+01, -3.1246258e+01],
       [-3.2001770e+01, -3.0983660e+01],
       [-3.1625330e+01, -3.0689587e+01],
       [-3.0662056e+01, -3.0380342e+01],
       [-2.9218283e+01, -2.9837927e+01],
       [-2.9557426e+01, -2.8793940e+01],
       [-3.0400908e+01, -2.8413435e+01],
       [-2.8946005e+01, -2.8285135e+01],
       [-2.8908888e+01, -2.7868914e+01]], dtype=float32))}, hs=[]))]
2025-04-17 12:18:13,824 - asr_demo - INFO - ASR processing took 1.361 seconds
2025-04-17 12:18:13,825 - asr_demo - INFO - Raw recognized text: 'OH'
2025-04-17 12:18:13,825 - asr_demo - INFO - Normalized text: 'OH'
2025-04-17 12:18:13,825 - asr_demo - INFO - Metrics: Latency: 1.36s   |   RTF: 0.28
2025-04-17 12:18:49,240 - asr_demo - INFO - Input audio: sr=48000, length=288000, max_amp=486998400
2025-04-17 12:18:49,245 - asr_demo - INFO - Saved debug audio sample to debug_samples/input_20250417_121849.wav
2025-04-17 12:18:49,245 - asr_demo - INFO - Resampling from 48000 to 16000
2025-04-17 12:18:49,255 - asr_demo - INFO - Saved debug audio sample to debug_samples/resampled_20250417_121849.wav
2025-04-17 12:18:49,256 - asr_demo - INFO - Audio after preparation: length=96000, max_amp=482660000.0
2025-04-17 12:18:49,257 - espnet2.bin.asr_inference - INFO - speech length: 96000
2025-04-17 12:18:49,309 - espnet.nets.beam_search - INFO - decoder input length: 93
2025-04-17 12:18:49,309 - espnet.nets.beam_search - INFO - max output length: 93
2025-04-17 12:18:49,310 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 12:18:50,558 - espnet.nets.beam_search - INFO - end detected at 8
2025-04-17 12:18:50,560 - espnet.nets.beam_search - INFO -  -4.23 * 0.7 =  -2.96 for decoder
2025-04-17 12:18:50,560 - espnet.nets.beam_search - INFO - -11.62 * 1.0 = -11.62 for lm
2025-04-17 12:18:50,560 - espnet.nets.beam_search - INFO -  -3.57 * 0.3 =  -1.07 for ctc
2025-04-17 12:18:50,561 - espnet.nets.beam_search - INFO - total log probability: -15.66
2025-04-17 12:18:50,561 - espnet.nets.beam_search - INFO - normalized log probability: -5.22
2025-04-17 12:18:50,561 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 27
2025-04-17 12:18:50,562 - espnet.nets.beam_search - INFO - best hypo: ▁OH

2025-04-17 12:18:50,588 - asr_demo - INFO - ASR result: [('OH', ['▁OH'], [294], Hypothesis(yseq=tensor([4999,  294, 4999], device='cuda:0'), score=tensor(-15.6553, device='cuda:0'), scores={'decoder': tensor(-4.2286, device='cuda:0'), 'lm': tensor(-11.6243, device='cuda:0'), 'ctc': tensor(-3.5701, device='cuda:0')}, states={'decoder': [tensor([[[ 10.2329,  14.8178,  -6.5045,  ..., -94.8617,  11.3489, -36.1943],
         [-48.8455,  -0.2899, -24.8618,  ...,  -9.2330,  -7.3925, -13.8430]]],
       device='cuda:0'), tensor([[[ 13.7774,  23.7916,  -9.3954,  ..., -87.5693,   5.0321, -36.9720],
         [-49.4969,  15.3825, -29.9964,  ..., -11.0534,  -5.1665, -13.2223]]],
       device='cuda:0'), tensor([[[ 23.1358,  49.0813,  -6.9235,  ..., -60.6349,  -1.3830, -29.6854],
         [-39.6226,  37.8836, -12.0975,  ...,  -3.0667, -24.1658, -23.9003]]],
       device='cuda:0'), tensor([[[ 28.1007,  54.7509,  15.7969,  ..., -68.0244,  17.3534,  -6.8404],
         [-50.5821,  45.6782,  50.8487,  ...,  -0.8914, -23.8278,  18.4151]]],
       device='cuda:0'), tensor([[[ 37.3874,  59.9440,  62.3021,  ..., -27.1835, -13.9923, -11.3152],
         [-67.3177,  48.5151, 127.0224,  ...,   3.1025, -74.8347, -11.0765]]],
       device='cuda:0'), tensor([[[ 198.9637, -138.4827,  814.2045,  ...,   64.3736,  -42.0703,
          -199.9617],
         [  64.3500,   -3.8599,  763.7897,  ...,   42.9764, -107.6738,
          -197.2887]]], device='cuda:0')], 'lm': [tensor([[[-39.7705, -41.4082, -53.9374,  ..., -60.1550, -85.4351, -42.1970],
         [-16.5875, -49.0455, -49.6298,  ..., -27.9937,  13.9804,  12.6574]]],
       device='cuda:0'), tensor([[[-170.7027, -108.4380,  -57.1364,  ..., -103.9612, -142.6913,
           -86.5089],
         [ -79.3743,  -86.4681,  -62.7857,  ...,  -59.2040,   -5.6714,
            -7.8986]]], device='cuda:0'), tensor([[[-148.5185, -112.3790,  -50.3978,  ..., -122.3693, -127.5089,
           -68.6950],
         [ -85.2193,  -49.4232, -117.7466,  ...,  -39.1028,    7.5195,
           -17.6074]]], device='cuda:0'), tensor([[[-136.0194, -112.6451,  -56.3242,  ..., -114.0812, -136.8767,
           -65.6614],
         [ -69.8368,  -17.2737, -143.8616,  ...,  -53.9154,   25.2713,
           -27.8963]]], device='cuda:0'), tensor([[[-125.1060, -128.5430,  -35.0457,  ..., -111.0084, -165.5469,
           -23.4448],
         [ -59.4470,   -8.3018, -176.6608,  ...,  -81.6777,   23.5389,
           -28.1165]]], device='cuda:0'), tensor([[[-103.6706, -137.5663,  -43.5697,  ..., -100.1561, -150.1787,
           -17.6993],
         [ -58.6853,   -1.6969, -197.2238,  ...,  -64.9202,   13.1657,
           -43.8850]]], device='cuda:0'), tensor([[[ -46.9828, -109.7889,  -25.0927,  ...,  -83.3409, -135.3886,
           -44.3387],
         [  38.4181,    9.1702, -239.4259,  ...,  -68.6377,   45.4268,
           -93.3414]]], device='cuda:0'), tensor([[[ -47.5229, -144.3886,  -31.8411,  ...,  -78.2817, -137.4689,
           -80.0558],
         [  74.9426,   -8.4308, -236.8700,  ..., -124.3614,   46.2235,
          -204.3613]]], device='cuda:0'), tensor([[[ -75.9904, -108.4232,  -62.1405,  ...,  -46.0407, -156.8006,
          -126.1571],
         [  94.8039,  -37.9277, -242.8985,  ..., -103.5658,   41.3739,
          -218.5011]]], device='cuda:0'), tensor([[[ -81.8444, -108.9788,  -81.4366,  ...,  -68.3201,  -90.3554,
          -129.2357],
         [  60.7835,  -16.0975, -293.4579,  ..., -177.8571,    3.8160,
          -213.4378]]], device='cuda:0'), tensor([[[ -72.2617, -121.8983,  -71.8085,  ..., -114.5557,  -86.9422,
          -145.2200],
         [  51.7570,   27.1435, -306.1979,  ..., -248.3280,  -20.6903,
          -237.6202]]], device='cuda:0'), tensor([[[-108.7070, -143.9523,  -82.3740,  ..., -120.4336,  -96.7761,
          -159.7680],
         [  46.1251,   -6.1455, -313.5939,  ..., -242.3394,  -49.9042,
          -257.4998]]], device='cuda:0'), tensor([[[-118.2413, -145.9342, -107.4488,  ..., -141.1722,  -89.4754,
          -179.1521],
         [ 144.0600,  -40.1198, -347.9975,  ..., -140.7154, -114.3355,
          -312.3122]]], device='cuda:0'), tensor([[[-147.0713, -120.3623, -159.5060,  ..., -113.3352, -116.5388,
          -202.0317],
         [ 489.9173,  -67.9552, -440.5681,  ..., -272.1984, -285.7119,
          -467.8430]]], device='cuda:0'), tensor([[[-148.1669, -143.0877, -162.6882,  ..., -120.1432, -150.7227,
          -103.1704],
         [ 615.6603,  136.7246, -592.2883,  ..., -527.9575, -408.0835,
          -628.4088]]], device='cuda:0'), tensor([[[-201.2815, -139.5632, -136.4038,  ...,  -74.4828, -147.1849,
          -166.9807],
         [ 506.5568,  548.5419, -568.0477,  ..., -310.4229, -358.3789,
          -919.7438]]], device='cuda:0')], 'ctc': (-3.5701497, array([[-1.0000000e+10, -1.0000000e+10],
       [-4.5899841e+01, -1.0000000e+10],
       [-4.4970001e+01, -4.5899864e+01],
       [-4.3063965e+01, -4.4637581e+01],
       [-4.1658791e+01, -4.2876316e+01],
       [-4.0781231e+01, -4.1400578e+01],
       [-3.9825386e+01, -4.0352222e+01],
       [-3.8990719e+01, -3.9363380e+01],
       [-3.8397930e+01, -3.8468449e+01],
       [-3.8079052e+01, -3.7740944e+01],
       [-3.7923550e+01, -3.7204063e+01],
       [-3.7830032e+01, -3.6808929e+01],
       [-3.7799797e+01, -3.6503101e+01],
       [-3.7892914e+01, -3.6263306e+01],
       [-3.7737194e+01, -3.6086834e+01],
       [-3.7469490e+01, -3.5914215e+01],
       [-3.7135456e+01, -3.5725441e+01],
       [-3.6803604e+01, -3.5509529e+01],
       [-3.6521732e+01, -3.5269760e+01],
       [-3.6553581e+01, -3.5020718e+01],
       [-3.6737236e+01, -3.4827724e+01],
       [-3.6883076e+01, -3.4692207e+01],
       [-3.7103390e+01, -3.4588467e+01],
       [-3.7224937e+01, -3.4512947e+01],
       [-3.7219410e+01, -3.4451218e+01],
       [-3.7205189e+01, -3.4393063e+01],
       [-3.7104267e+01, -3.4337452e+01],
       [-3.6973351e+01, -3.4279213e+01],
       [-3.6935524e+01, -3.4216728e+01],
       [-3.6867008e+01, -3.4156010e+01],
       [-3.6874226e+01, -3.4094139e+01],
       [-3.6924175e+01, -3.4035587e+01],
       [-3.6905590e+01, -3.3982517e+01],
       [-3.6823730e+01, -3.3930973e+01],
       [-3.6758492e+01, -3.3877678e+01],
       [-3.6664188e+01, -3.3823677e+01],
       [-3.6544388e+01, -3.3767517e+01],
       [-3.6542828e+01, -3.3707787e+01],
       [-3.6591942e+01, -3.3651356e+01],
       [-3.6704872e+01, -3.3600475e+01],
       [-3.6689777e+01, -3.3557270e+01],
       [-3.6696831e+01, -3.3515297e+01],
       [-3.6638134e+01, -3.3475361e+01],
       [-3.6529331e+01, -3.3434772e+01],
       [-3.6476227e+01, -3.3391308e+01],
       [-3.6550659e+01, -3.3347301e+01],
       [-3.6670509e+01, -3.3308071e+01],
       [-3.6762379e+01, -3.3274532e+01],
       [-3.6828007e+01, -3.3244957e+01],
       [-3.6903313e+01, -3.3218086e+01],
       [-3.6833313e+01, -3.3193932e+01],
       [-3.6936722e+01, -3.3168720e+01],
       [-3.6896896e+01, -3.3146740e+01],
       [-3.6945919e+01, -3.3124519e+01],
       [-3.6913189e+01, -3.3104431e+01],
       [-3.6805321e+01, -3.3085396e+01],
       [-3.6844284e+01, -3.3065041e+01],
       [-3.6818516e+01, -3.3045921e+01],
       [-3.6596539e+01, -3.3026535e+01],
       [-3.6285492e+01, -3.3002129e+01],
       [-3.6328629e+01, -3.2967579e+01],
       [-3.6214500e+01, -3.2935234e+01],
       [-3.6025253e+01, -3.2899910e+01],
       [-3.5850189e+01, -3.2858349e+01],
       [-3.5819572e+01, -3.2810463e+01],
       [-3.5807213e+01, -3.2763210e+01],
       [-3.5831310e+01, -3.2717518e+01],
       [-3.5764297e+01, -3.2674969e+01],
       [-3.5680279e+01, -3.2631458e+01],
       [-3.5698578e+01, -3.2586262e+01],
       [-3.5732780e+01, -3.2544014e+01],
       [-3.5734070e+01, -3.2505062e+01],
       [-3.5783798e+01, -3.2467743e+01],
       [-3.5845306e+01, -3.2433750e+01],
       [-3.5889549e+01, -3.2403419e+01],
       [-3.5833241e+01, -3.2376266e+01],
       [-3.5761703e+01, -3.2349293e+01],
       [-3.5693691e+01, -3.2322075e+01],
       [-3.5508957e+01, -3.2294510e+01],
       [-3.5384708e+01, -3.2261780e+01],
       [-3.5226265e+01, -3.2226135e+01],
       [-3.4913940e+01, -3.2185707e+01],
       [-3.4592857e+01, -3.2131084e+01],
       [-3.4180893e+01, -3.2057880e+01],
       [-3.3793365e+01, -3.1952757e+01],
       [-3.3308151e+01, -3.1814024e+01],
       [-3.2831867e+01, -3.1622002e+01],
       [-3.1974365e+01, -3.1376156e+01],
       [-3.0402494e+01, -3.0954420e+01],
       [-3.0426046e+01, -2.9953018e+01],
       [-3.1196632e+01, -2.9470436e+01],
       [-2.9433176e+01, -2.9306774e+01],
       [-2.9384281e+01, -2.8674942e+01]], dtype=float32))}, hs=[]))]
2025-04-17 12:18:50,589 - asr_demo - INFO - ASR processing took 1.333 seconds
2025-04-17 12:18:50,589 - asr_demo - INFO - Raw recognized text: 'OH'
2025-04-17 12:18:50,589 - asr_demo - INFO - Normalized text: 'OH'
2025-04-17 12:18:50,589 - asr_demo - INFO - Metrics: Latency: 1.33s   |   RTF: 0.22
2025-04-17 12:22:04,874 - asr_demo - INFO - Using device: cuda
2025-04-17 12:22:04,875 - asr_demo - INFO - Loading ASR model kamo-naoyuki/librispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp_valid.acc.ave (first run can take ~1 min)…
2025-04-17 12:22:31,933 - asr_demo - INFO - Model downloaded successfully to: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo
2025-04-17 12:22:31,934 - asr_demo - INFO - Model dict keys: ['asr_train_config', 'lm_train_config', 'asr_model_file', 'lm_file']
2025-04-17 12:22:31,938 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp/config.yaml
2025-04-17 12:22:33,756 - root - INFO - Vocabulary size: 5000
2025-04-17 12:22:33,885 - root - WARNING - Using legacy_rel_pos and it will be deprecated in the future.
2025-04-17 12:22:33,965 - root - WARNING - Using legacy_rel_selfattn and it will be deprecated in the future.
2025-04-17 12:22:34,707 - root - INFO - Gradient checkpoint layers: []
2025-04-17 12:22:36,756 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/lm_train_lm_transformer2_bpe5000_scheduler_confwarmup_steps25000_batch_bins500000000_accum_grad2_use_amptrue/config.yaml
2025-04-17 12:22:42,230 - root - INFO - Vocabulary size: 5000
2025-04-17 12:22:42,285 - root - INFO - encoder self-attention layer type = self-attention
2025-04-17 12:22:47,393 - espnet2.bin.asr_inference - INFO - BatchBeamSearch implementation is selected.
2025-04-17 12:22:47,845 - espnet2.bin.asr_inference - INFO - Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (lm): TransformerLM(
      (embed): Embedding(5000, 128)
      (encoder): Encoder(
        (embed): Sequential(
          (0): Linear(in_features=128, out_features=512, bias=True)
          (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): Dropout(p=0.0, inplace=False)
          (3): ReLU()
          (4): Sequential()
        )
        (encoders): MultiSequential(
          (0): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (4): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (5): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (6): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (7): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (8): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (9): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (10): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (11): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (12): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (13): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (14): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (15): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): Linear(in_features=512, out_features=5000, bias=True)
    )
  )
)
2025-04-17 12:22:47,848 - espnet2.bin.asr_inference - INFO - Decoding device=cuda, dtype=float32
2025-04-17 12:22:47,867 - espnet2.bin.asr_inference - INFO - Text tokenizer: SentencepiecesTokenizer(model="/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/data/en_token_list/bpe_unigram5000/bpe.model")
2025-04-17 12:22:47,879 - asr_demo - INFO - ASR model ready. Native SR = 16000 Hz
2025-04-17 12:22:49,157 - asr_demo - INFO - Starting Gradio app...
2025-04-17 12:23:08,623 - asr_demo - INFO - Input audio: sr=48000, length=141120, max_amp=1398433408
2025-04-17 12:23:08,627 - asr_demo - INFO - Saved debug audio sample to debug_samples/input_20250417_122308.wav
2025-04-17 12:23:08,628 - asr_demo - INFO - Normalizing audio with max amplitude 1398433408.0
2025-04-17 12:23:08,628 - asr_demo - INFO - Resampling from 48000 to 16000
2025-04-17 12:23:10,021 - asr_demo - INFO - Saved debug audio sample to debug_samples/resampled_20250417_122310.wav
2025-04-17 12:23:10,021 - asr_demo - INFO - Audio after preparation: length=47040, max_amp=0.997830331325531
2025-04-17 12:23:10,022 - espnet2.bin.asr_inference - INFO - speech length: 47040
2025-04-17 12:23:10,352 - espnet.nets.beam_search - INFO - decoder input length: 45
2025-04-17 12:23:10,352 - espnet.nets.beam_search - INFO - max output length: 450
2025-04-17 12:23:10,353 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 12:23:12,074 - asr_demo - ERROR - ASR failed: index 45 is out of bounds for dimension 0 with size 45
2025-04-17 12:26:24,774 - asr_demo - INFO - Logging to /home/ambuja/emo-tts/asr_debug.log
2025-04-17 12:26:24,775 - asr_demo - INFO - Using device: cuda
2025-04-17 12:26:24,776 - asr_demo - INFO - Loading ESPnet model kamo-naoyuki/librispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp_valid.acc.ave (might take ~1 min on first run)…
2025-04-17 12:26:35,825 - asr_demo - INFO - Model cached in /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo
2025-04-17 12:26:35,827 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp/config.yaml
2025-04-17 12:26:36,700 - root - INFO - Vocabulary size: 5000
2025-04-17 12:26:36,774 - root - WARNING - Using legacy_rel_pos and it will be deprecated in the future.
2025-04-17 12:26:36,833 - root - WARNING - Using legacy_rel_selfattn and it will be deprecated in the future.
2025-04-17 12:26:37,300 - root - INFO - Gradient checkpoint layers: []
2025-04-17 12:26:38,640 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/lm_train_lm_transformer2_bpe5000_scheduler_confwarmup_steps25000_batch_bins500000000_accum_grad2_use_amptrue/config.yaml
2025-04-17 12:26:39,458 - root - INFO - Vocabulary size: 5000
2025-04-17 12:26:39,462 - root - INFO - encoder self-attention layer type = self-attention
2025-04-17 12:26:40,091 - espnet2.bin.asr_inference - INFO - BatchBeamSearch implementation is selected.
2025-04-17 12:26:40,121 - espnet2.bin.asr_inference - INFO - Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (lm): TransformerLM(
      (embed): Embedding(5000, 128)
      (encoder): Encoder(
        (embed): Sequential(
          (0): Linear(in_features=128, out_features=512, bias=True)
          (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): Dropout(p=0.0, inplace=False)
          (3): ReLU()
          (4): Sequential()
        )
        (encoders): MultiSequential(
          (0): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (4): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (5): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (6): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (7): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (8): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (9): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (10): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (11): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (12): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (13): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (14): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (15): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): Linear(in_features=512, out_features=5000, bias=True)
    )
  )
)
2025-04-17 12:26:40,122 - espnet2.bin.asr_inference - INFO - Decoding device=cuda, dtype=float32
2025-04-17 12:26:40,126 - espnet2.bin.asr_inference - INFO - Text tokenizer: SentencepiecesTokenizer(model="/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/data/en_token_list/bpe_unigram5000/bpe.model")
2025-04-17 12:26:40,126 - asr_demo - INFO - ASR ready – model SR = 16000 Hz
2025-04-17 12:28:14,282 - asr_demo - INFO - Logging to /home/ambuja/emo-tts/asr_debug.log
2025-04-17 12:28:14,283 - asr_demo - INFO - Using device: cuda
2025-04-17 12:28:14,284 - asr_demo - INFO - Loading ESPnet model kamo-naoyuki/librispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp_valid.acc.ave (might take ~1 min on first run)…
2025-04-17 12:28:25,221 - asr_demo - INFO - Model cached in /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo
2025-04-17 12:28:25,223 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp/config.yaml
2025-04-17 12:28:26,069 - root - INFO - Vocabulary size: 5000
2025-04-17 12:28:26,142 - root - WARNING - Using legacy_rel_pos and it will be deprecated in the future.
2025-04-17 12:28:26,200 - root - WARNING - Using legacy_rel_selfattn and it will be deprecated in the future.
2025-04-17 12:28:26,648 - root - INFO - Gradient checkpoint layers: []
2025-04-17 12:28:27,982 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/lm_train_lm_transformer2_bpe5000_scheduler_confwarmup_steps25000_batch_bins500000000_accum_grad2_use_amptrue/config.yaml
2025-04-17 12:28:28,810 - root - INFO - Vocabulary size: 5000
2025-04-17 12:28:28,814 - root - INFO - encoder self-attention layer type = self-attention
2025-04-17 12:28:29,197 - espnet2.bin.asr_inference - INFO - BatchBeamSearch implementation is selected.
2025-04-17 12:28:29,227 - espnet2.bin.asr_inference - INFO - Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (lm): TransformerLM(
      (embed): Embedding(5000, 128)
      (encoder): Encoder(
        (embed): Sequential(
          (0): Linear(in_features=128, out_features=512, bias=True)
          (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): Dropout(p=0.0, inplace=False)
          (3): ReLU()
          (4): Sequential()
        )
        (encoders): MultiSequential(
          (0): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (4): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (5): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (6): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (7): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (8): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (9): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (10): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (11): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (12): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (13): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (14): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (15): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): Linear(in_features=512, out_features=5000, bias=True)
    )
  )
)
2025-04-17 12:28:29,228 - espnet2.bin.asr_inference - INFO - Decoding device=cuda, dtype=float32
2025-04-17 12:28:29,232 - espnet2.bin.asr_inference - INFO - Text tokenizer: SentencepiecesTokenizer(model="/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/data/en_token_list/bpe_unigram5000/bpe.model")
2025-04-17 12:28:29,233 - asr_demo - INFO - ASR ready – model SR = 16000 Hz
2025-04-17 12:28:29,348 - asr_demo - INFO - Launching Gradio app…
2025-04-17 12:28:56,390 - asr_demo - INFO - Incoming audio: sr=48000, length=3.180 s, peak=2147483648.000
2025-04-17 12:28:56,391 - asr_demo - INFO - Normalised audio (peak was 2147483648.00)
2025-04-17 12:28:56,391 - asr_demo - INFO - Resampling 48000 → 16000 Hz
2025-04-17 12:28:57,780 - asr_demo - INFO - Saved debug_samples/input_20250417_122857.wav (3.2 s)
2025-04-17 12:28:57,781 - espnet2.bin.asr_inference - INFO - speech length: 50880
2025-04-17 12:28:58,100 - espnet.nets.beam_search - INFO - decoder input length: 49
2025-04-17 12:28:58,100 - espnet.nets.beam_search - INFO - max output length: 49
2025-04-17 12:28:58,101 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 12:28:58,545 - espnet.nets.beam_search - INFO - end detected at 11
2025-04-17 12:28:58,546 - espnet.nets.beam_search - INFO - -11.84 * 0.7 =  -8.29 for decoder
2025-04-17 12:28:58,546 - espnet.nets.beam_search - INFO -  -2.02 * 0.3 =  -0.61 for ctc
2025-04-17 12:28:58,547 - espnet.nets.beam_search - INFO - -14.23 * 1.0 = -14.23 for lm
2025-04-17 12:28:58,547 - espnet.nets.beam_search - INFO - total log probability: -23.12
2025-04-17 12:28:58,548 - espnet.nets.beam_search - INFO - normalized log probability: -5.78
2025-04-17 12:28:58,548 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 30
2025-04-17 12:28:58,548 - espnet.nets.beam_search - INFO - best hypo: ▁AND▁LO

2025-04-17 12:28:58,556 - asr_demo - INFO - Recognised: AND LO
2025-04-17 12:29:33,471 - asr_demo - INFO - Incoming audio: sr=48000, length=4.800 s, peak=2147483648.000
2025-04-17 12:29:33,471 - asr_demo - INFO - Normalised audio (peak was 2147483648.00)
2025-04-17 12:29:33,472 - asr_demo - INFO - Resampling 48000 → 16000 Hz
2025-04-17 12:29:33,476 - asr_demo - INFO - Saved debug_samples/input_20250417_122933.wav (4.8 s)
2025-04-17 12:29:33,477 - espnet2.bin.asr_inference - INFO - speech length: 76800
2025-04-17 12:29:33,528 - espnet.nets.beam_search - INFO - decoder input length: 74
2025-04-17 12:29:33,528 - espnet.nets.beam_search - INFO - max output length: 74
2025-04-17 12:29:33,529 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 12:29:34,071 - espnet.nets.beam_search - INFO - end detected at 15
2025-04-17 12:29:34,072 - espnet.nets.beam_search - INFO - -25.97 * 0.7 = -18.18 for decoder
2025-04-17 12:29:34,072 - espnet.nets.beam_search - INFO - -52.44 * 0.3 = -15.73 for ctc
2025-04-17 12:29:34,072 - espnet.nets.beam_search - INFO - -31.67 * 1.0 = -31.67 for lm
2025-04-17 12:29:34,073 - espnet.nets.beam_search - INFO - total log probability: -65.58
2025-04-17 12:29:34,073 - espnet.nets.beam_search - INFO - normalized log probability: -5.96
2025-04-17 12:29:34,073 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 15
2025-04-17 12:29:34,074 - espnet.nets.beam_search - INFO - best hypo: ▁I'D▁LIKE▁TO▁PLAY▁BANNERMAN

2025-04-17 12:29:34,075 - asr_demo - INFO - Recognised: I'D LIKE TO PLAY BANNERMAN
2025-04-17 12:29:58,218 - asr_demo - INFO - Incoming audio: sr=48000, length=4.800 s, peak=2147483648.000
2025-04-17 12:29:58,218 - asr_demo - INFO - Normalised audio (peak was 2147483648.00)
2025-04-17 12:29:58,219 - asr_demo - INFO - Resampling 48000 → 16000 Hz
2025-04-17 12:29:58,223 - asr_demo - INFO - Saved debug_samples/input_20250417_122958.wav (4.8 s)
2025-04-17 12:29:58,224 - espnet2.bin.asr_inference - INFO - speech length: 76800
2025-04-17 12:29:58,270 - espnet.nets.beam_search - INFO - decoder input length: 74
2025-04-17 12:29:58,270 - espnet.nets.beam_search - INFO - max output length: 74
2025-04-17 12:29:58,270 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 12:29:58,817 - espnet.nets.beam_search - INFO - end detected at 15
2025-04-17 12:29:58,818 - espnet.nets.beam_search - INFO - -25.97 * 0.7 = -18.18 for decoder
2025-04-17 12:29:58,818 - espnet.nets.beam_search - INFO - -52.44 * 0.3 = -15.73 for ctc
2025-04-17 12:29:58,819 - espnet.nets.beam_search - INFO - -31.67 * 1.0 = -31.67 for lm
2025-04-17 12:29:58,819 - espnet.nets.beam_search - INFO - total log probability: -65.58
2025-04-17 12:29:58,819 - espnet.nets.beam_search - INFO - normalized log probability: -5.96
2025-04-17 12:29:58,819 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 15
2025-04-17 12:29:58,820 - espnet.nets.beam_search - INFO - best hypo: ▁I'D▁LIKE▁TO▁PLAY▁BANNERMAN

2025-04-17 12:29:58,821 - asr_demo - INFO - Recognised: I'D LIKE TO PLAY BANNERMAN
2025-04-17 12:30:15,896 - asr_demo - INFO - Incoming audio: sr=48000, length=3.660 s, peak=2147483648.000
2025-04-17 12:30:15,897 - asr_demo - INFO - Normalised audio (peak was 2147483648.00)
2025-04-17 12:30:15,897 - asr_demo - INFO - Resampling 48000 → 16000 Hz
2025-04-17 12:30:15,901 - asr_demo - INFO - Saved debug_samples/input_20250417_123015.wav (3.7 s)
2025-04-17 12:30:15,902 - espnet2.bin.asr_inference - INFO - speech length: 58560
2025-04-17 12:30:15,945 - espnet.nets.beam_search - INFO - decoder input length: 56
2025-04-17 12:30:15,945 - espnet.nets.beam_search - INFO - max output length: 56
2025-04-17 12:30:15,945 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 12:30:16,336 - espnet.nets.beam_search - INFO - end detected at 11
2025-04-17 12:30:16,337 - espnet.nets.beam_search - INFO -  -8.86 * 0.7 =  -6.20 for decoder
2025-04-17 12:30:16,337 - espnet.nets.beam_search - INFO -  -0.01 * 0.3 =  -0.00 for ctc
2025-04-17 12:30:16,337 - espnet.nets.beam_search - INFO - -14.07 * 1.0 = -14.07 for lm
2025-04-17 12:30:16,338 - espnet.nets.beam_search - INFO - total log probability: -20.27
2025-04-17 12:30:16,338 - espnet.nets.beam_search - INFO - normalized log probability: -4.05
2025-04-17 12:30:16,338 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 33
2025-04-17 12:30:16,339 - espnet.nets.beam_search - INFO - best hypo: ▁I▁LOVE▁YOU

2025-04-17 12:30:16,340 - asr_demo - INFO - Recognised: I LOVE YOU
2025-04-17 12:33:39,826 - asr_demo - INFO - Incoming audio: sr=48000, length=4.560 s, peak=2147483648.000
2025-04-17 12:33:39,826 - asr_demo - INFO - Normalised audio (peak was 2147483648.00)
2025-04-17 12:33:39,826 - asr_demo - INFO - Resampling 48000 → 16000 Hz
2025-04-17 12:33:39,831 - asr_demo - INFO - Saved debug_samples/input_20250417_123339.wav (4.6 s)
2025-04-17 12:33:39,832 - espnet2.bin.asr_inference - INFO - speech length: 72960
2025-04-17 12:33:39,882 - espnet.nets.beam_search - INFO - decoder input length: 70
2025-04-17 12:33:39,883 - espnet.nets.beam_search - INFO - max output length: 70
2025-04-17 12:33:39,883 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 12:33:40,746 - espnet.nets.beam_search - INFO - end detected at 25
2025-04-17 12:33:40,746 - espnet.nets.beam_search - INFO - -14.89 * 0.7 = -10.42 for decoder
2025-04-17 12:33:40,747 - espnet.nets.beam_search - INFO - -10.60 * 0.3 =  -3.18 for ctc
2025-04-17 12:33:40,747 - espnet.nets.beam_search - INFO - -48.88 * 1.0 = -48.88 for lm
2025-04-17 12:33:40,747 - espnet.nets.beam_search - INFO - total log probability: -62.48
2025-04-17 12:33:40,748 - espnet.nets.beam_search - INFO - normalized log probability: -3.91
2025-04-17 12:33:40,748 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 40
2025-04-17 12:33:40,749 - espnet.nets.beam_search - INFO - best hypo: ▁HELLO▁I▁AM▁ILL▁AND▁I▁WANT▁TO▁GO▁TO▁GOA

2025-04-17 12:33:40,750 - asr_demo - INFO - Recognised: HELLO I AM ILL AND I WANT TO GO TO GOA
2025-04-17 12:38:59,012 - asr_multi - INFO - Starting multi-model ASR/TTS demo
2025-04-17 12:38:59,013 - asr_multi - INFO - Using device: cuda
2025-04-17 12:38:59,020 - asr_multi - INFO - Downloading ASR model 'Conformer6' → kamo-naoyuki/librispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp_valid.acc.ave
2025-04-17 12:39:09,981 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp/config.yaml
2025-04-17 12:39:10,830 - root - INFO - Vocabulary size: 5000
2025-04-17 12:39:10,904 - root - WARNING - Using legacy_rel_pos and it will be deprecated in the future.
2025-04-17 12:39:10,964 - root - WARNING - Using legacy_rel_selfattn and it will be deprecated in the future.
2025-04-17 12:39:11,438 - root - INFO - Gradient checkpoint layers: []
2025-04-17 12:39:12,774 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/lm_train_lm_transformer2_bpe5000_scheduler_confwarmup_steps25000_batch_bins500000000_accum_grad2_use_amptrue/config.yaml
2025-04-17 12:39:13,593 - root - INFO - Vocabulary size: 5000
2025-04-17 12:39:13,597 - root - INFO - encoder self-attention layer type = self-attention
2025-04-17 12:39:13,979 - espnet2.bin.asr_inference - INFO - BatchBeamSearch implementation is selected.
2025-04-17 12:39:14,009 - espnet2.bin.asr_inference - INFO - Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (lm): TransformerLM(
      (embed): Embedding(5000, 128)
      (encoder): Encoder(
        (embed): Sequential(
          (0): Linear(in_features=128, out_features=512, bias=True)
          (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): Dropout(p=0.0, inplace=False)
          (3): ReLU()
          (4): Sequential()
        )
        (encoders): MultiSequential(
          (0): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (4): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (5): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (6): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (7): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (8): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (9): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (10): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (11): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (12): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (13): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (14): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (15): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): Linear(in_features=512, out_features=5000, bias=True)
    )
  )
)
2025-04-17 12:39:14,010 - espnet2.bin.asr_inference - INFO - Decoding device=cuda, dtype=float32
2025-04-17 12:39:14,013 - espnet2.bin.asr_inference - INFO - Text tokenizer: SentencepiecesTokenizer(model="/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/data/en_token_list/bpe_unigram5000/bpe.model")
2025-04-17 12:39:14,017 - asr_multi - INFO - Downloading ASR model 'TransformerE18' → Shinji Watanabe/librispeech_asr_train_asr_transformer_e18_raw_en_bpe5000_sp_valid.acc.ave
2025-04-17 12:42:47,623 - asr_multi - INFO - Starting multi-model ASR/TTS demo
2025-04-17 12:42:47,623 - asr_multi - INFO - Using device: cuda
2025-04-17 12:42:47,631 - asr_multi - INFO - Downloading ASR model 'Conformer6' → kamo-naoyuki/librispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp_valid.acc.ave
2025-04-17 12:42:58,201 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp/config.yaml
2025-04-17 12:42:59,031 - root - INFO - Vocabulary size: 5000
2025-04-17 12:42:59,108 - root - WARNING - Using legacy_rel_pos and it will be deprecated in the future.
2025-04-17 12:42:59,155 - root - WARNING - Using legacy_rel_selfattn and it will be deprecated in the future.
2025-04-17 12:42:59,633 - root - INFO - Gradient checkpoint layers: []
2025-04-17 12:43:01,071 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/lm_train_lm_transformer2_bpe5000_scheduler_confwarmup_steps25000_batch_bins500000000_accum_grad2_use_amptrue/config.yaml
2025-04-17 12:43:01,881 - root - INFO - Vocabulary size: 5000
2025-04-17 12:43:01,885 - root - INFO - encoder self-attention layer type = self-attention
2025-04-17 12:43:02,540 - espnet2.bin.asr_inference - INFO - BatchBeamSearch implementation is selected.
2025-04-17 12:43:02,570 - espnet2.bin.asr_inference - INFO - Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (lm): TransformerLM(
      (embed): Embedding(5000, 128)
      (encoder): Encoder(
        (embed): Sequential(
          (0): Linear(in_features=128, out_features=512, bias=True)
          (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): Dropout(p=0.0, inplace=False)
          (3): ReLU()
          (4): Sequential()
        )
        (encoders): MultiSequential(
          (0): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (4): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (5): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (6): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (7): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (8): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (9): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (10): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (11): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (12): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (13): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (14): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (15): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): Linear(in_features=512, out_features=5000, bias=True)
    )
  )
)
2025-04-17 12:43:02,571 - espnet2.bin.asr_inference - INFO - Decoding device=cuda, dtype=float32
2025-04-17 12:43:02,575 - espnet2.bin.asr_inference - INFO - Text tokenizer: SentencepiecesTokenizer(model="/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/data/en_token_list/bpe_unigram5000/bpe.model")
2025-04-17 12:43:02,576 - asr_multi - INFO - Downloading ASR model 'TransformerE18' → espnet/shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best
2025-04-17 12:43:19,692 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--espnet--shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best/snapshots/bc6bbd771cec698f070640ee677a66719181f0a2/exp/asr_train_asr_transformer_e18_raw_bpe_sp/config.yaml
2025-04-17 12:43:20,719 - root - INFO - Vocabulary size: 5000
2025-04-17 12:43:21,123 - root - INFO - Gradient checkpoint layers: []
2025-04-17 12:43:21,660 - root - INFO - Initialize encoder.embed.conv.0.bias to zeros
2025-04-17 12:43:21,661 - root - INFO - Initialize encoder.embed.conv.2.bias to zeros
2025-04-17 12:43:21,661 - root - INFO - Initialize encoder.embed.out.0.bias to zeros
2025-04-17 12:43:21,662 - root - INFO - Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
2025-04-17 12:43:21,662 - root - INFO - Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
2025-04-17 12:43:21,662 - root - INFO - Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
2025-04-17 12:43:21,663 - root - INFO - Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
2025-04-17 12:43:21,663 - root - INFO - Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
2025-04-17 12:43:21,663 - root - INFO - Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
2025-04-17 12:43:21,663 - root - INFO - Initialize encoder.encoders.0.norm1.bias to zeros
2025-04-17 12:43:21,664 - root - INFO - Initialize encoder.encoders.0.norm2.bias to zeros
2025-04-17 12:43:21,665 - root - INFO - Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
2025-04-17 12:43:21,665 - root - INFO - Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
2025-04-17 12:43:21,665 - root - INFO - Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
2025-04-17 12:43:21,665 - root - INFO - Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
2025-04-17 12:43:21,666 - root - INFO - Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
2025-04-17 12:43:21,666 - root - INFO - Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
2025-04-17 12:43:21,667 - root - INFO - Initialize encoder.encoders.1.norm1.bias to zeros
2025-04-17 12:43:21,667 - root - INFO - Initialize encoder.encoders.1.norm2.bias to zeros
2025-04-17 12:43:21,667 - root - INFO - Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
2025-04-17 12:43:21,668 - root - INFO - Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
2025-04-17 12:43:21,668 - root - INFO - Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
2025-04-17 12:43:21,668 - root - INFO - Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
2025-04-17 12:43:21,669 - root - INFO - Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
2025-04-17 12:43:21,669 - root - INFO - Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
2025-04-17 12:43:21,669 - root - INFO - Initialize encoder.encoders.2.norm1.bias to zeros
2025-04-17 12:43:21,669 - root - INFO - Initialize encoder.encoders.2.norm2.bias to zeros
2025-04-17 12:43:21,670 - root - INFO - Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
2025-04-17 12:43:21,670 - root - INFO - Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
2025-04-17 12:43:21,670 - root - INFO - Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
2025-04-17 12:43:21,671 - root - INFO - Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
2025-04-17 12:43:21,671 - root - INFO - Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
2025-04-17 12:43:21,672 - root - INFO - Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
2025-04-17 12:43:21,672 - root - INFO - Initialize encoder.encoders.3.norm1.bias to zeros
2025-04-17 12:43:21,672 - root - INFO - Initialize encoder.encoders.3.norm2.bias to zeros
2025-04-17 12:43:21,673 - root - INFO - Initialize encoder.encoders.4.self_attn.linear_q.bias to zeros
2025-04-17 12:43:21,673 - root - INFO - Initialize encoder.encoders.4.self_attn.linear_k.bias to zeros
2025-04-17 12:43:21,673 - root - INFO - Initialize encoder.encoders.4.self_attn.linear_v.bias to zeros
2025-04-17 12:43:21,673 - root - INFO - Initialize encoder.encoders.4.self_attn.linear_out.bias to zeros
2025-04-17 12:43:21,674 - root - INFO - Initialize encoder.encoders.4.feed_forward.w_1.bias to zeros
2025-04-17 12:43:21,674 - root - INFO - Initialize encoder.encoders.4.feed_forward.w_2.bias to zeros
2025-04-17 12:43:21,674 - root - INFO - Initialize encoder.encoders.4.norm1.bias to zeros
2025-04-17 12:43:21,675 - root - INFO - Initialize encoder.encoders.4.norm2.bias to zeros
2025-04-17 12:43:21,675 - root - INFO - Initialize encoder.encoders.5.self_attn.linear_q.bias to zeros
2025-04-17 12:43:21,675 - root - INFO - Initialize encoder.encoders.5.self_attn.linear_k.bias to zeros
2025-04-17 12:43:21,676 - root - INFO - Initialize encoder.encoders.5.self_attn.linear_v.bias to zeros
2025-04-17 12:43:21,676 - root - INFO - Initialize encoder.encoders.5.self_attn.linear_out.bias to zeros
2025-04-17 12:43:21,676 - root - INFO - Initialize encoder.encoders.5.feed_forward.w_1.bias to zeros
2025-04-17 12:43:21,677 - root - INFO - Initialize encoder.encoders.5.feed_forward.w_2.bias to zeros
2025-04-17 12:43:21,677 - root - INFO - Initialize encoder.encoders.5.norm1.bias to zeros
2025-04-17 12:43:21,677 - root - INFO - Initialize encoder.encoders.5.norm2.bias to zeros
2025-04-17 12:43:21,678 - root - INFO - Initialize encoder.encoders.6.self_attn.linear_q.bias to zeros
2025-04-17 12:43:21,678 - root - INFO - Initialize encoder.encoders.6.self_attn.linear_k.bias to zeros
2025-04-17 12:43:21,678 - root - INFO - Initialize encoder.encoders.6.self_attn.linear_v.bias to zeros
2025-04-17 12:43:21,679 - root - INFO - Initialize encoder.encoders.6.self_attn.linear_out.bias to zeros
2025-04-17 12:43:21,679 - root - INFO - Initialize encoder.encoders.6.feed_forward.w_1.bias to zeros
2025-04-17 12:43:21,679 - root - INFO - Initialize encoder.encoders.6.feed_forward.w_2.bias to zeros
2025-04-17 12:43:21,680 - root - INFO - Initialize encoder.encoders.6.norm1.bias to zeros
2025-04-17 12:43:21,680 - root - INFO - Initialize encoder.encoders.6.norm2.bias to zeros
2025-04-17 12:43:21,680 - root - INFO - Initialize encoder.encoders.7.self_attn.linear_q.bias to zeros
2025-04-17 12:43:21,681 - root - INFO - Initialize encoder.encoders.7.self_attn.linear_k.bias to zeros
2025-04-17 12:43:21,681 - root - INFO - Initialize encoder.encoders.7.self_attn.linear_v.bias to zeros
2025-04-17 12:43:21,681 - root - INFO - Initialize encoder.encoders.7.self_attn.linear_out.bias to zeros
2025-04-17 12:43:21,682 - root - INFO - Initialize encoder.encoders.7.feed_forward.w_1.bias to zeros
2025-04-17 12:43:21,682 - root - INFO - Initialize encoder.encoders.7.feed_forward.w_2.bias to zeros
2025-04-17 12:43:21,682 - root - INFO - Initialize encoder.encoders.7.norm1.bias to zeros
2025-04-17 12:43:21,683 - root - INFO - Initialize encoder.encoders.7.norm2.bias to zeros
2025-04-17 12:43:21,683 - root - INFO - Initialize encoder.encoders.8.self_attn.linear_q.bias to zeros
2025-04-17 12:43:21,683 - root - INFO - Initialize encoder.encoders.8.self_attn.linear_k.bias to zeros
2025-04-17 12:43:21,683 - root - INFO - Initialize encoder.encoders.8.self_attn.linear_v.bias to zeros
2025-04-17 12:43:21,684 - root - INFO - Initialize encoder.encoders.8.self_attn.linear_out.bias to zeros
2025-04-17 12:43:21,684 - root - INFO - Initialize encoder.encoders.8.feed_forward.w_1.bias to zeros
2025-04-17 12:43:21,684 - root - INFO - Initialize encoder.encoders.8.feed_forward.w_2.bias to zeros
2025-04-17 12:43:21,685 - root - INFO - Initialize encoder.encoders.8.norm1.bias to zeros
2025-04-17 12:43:21,685 - root - INFO - Initialize encoder.encoders.8.norm2.bias to zeros
2025-04-17 12:43:21,685 - root - INFO - Initialize encoder.encoders.9.self_attn.linear_q.bias to zeros
2025-04-17 12:43:21,686 - root - INFO - Initialize encoder.encoders.9.self_attn.linear_k.bias to zeros
2025-04-17 12:43:21,686 - root - INFO - Initialize encoder.encoders.9.self_attn.linear_v.bias to zeros
2025-04-17 12:43:21,686 - root - INFO - Initialize encoder.encoders.9.self_attn.linear_out.bias to zeros
2025-04-17 12:43:21,687 - root - INFO - Initialize encoder.encoders.9.feed_forward.w_1.bias to zeros
2025-04-17 12:43:21,687 - root - INFO - Initialize encoder.encoders.9.feed_forward.w_2.bias to zeros
2025-04-17 12:43:21,687 - root - INFO - Initialize encoder.encoders.9.norm1.bias to zeros
2025-04-17 12:43:21,688 - root - INFO - Initialize encoder.encoders.9.norm2.bias to zeros
2025-04-17 12:43:21,688 - root - INFO - Initialize encoder.encoders.10.self_attn.linear_q.bias to zeros
2025-04-17 12:43:21,688 - root - INFO - Initialize encoder.encoders.10.self_attn.linear_k.bias to zeros
2025-04-17 12:43:21,689 - root - INFO - Initialize encoder.encoders.10.self_attn.linear_v.bias to zeros
2025-04-17 12:43:21,689 - root - INFO - Initialize encoder.encoders.10.self_attn.linear_out.bias to zeros
2025-04-17 12:43:21,689 - root - INFO - Initialize encoder.encoders.10.feed_forward.w_1.bias to zeros
2025-04-17 12:43:21,689 - root - INFO - Initialize encoder.encoders.10.feed_forward.w_2.bias to zeros
2025-04-17 12:43:21,690 - root - INFO - Initialize encoder.encoders.10.norm1.bias to zeros
2025-04-17 12:43:21,690 - root - INFO - Initialize encoder.encoders.10.norm2.bias to zeros
2025-04-17 12:43:21,690 - root - INFO - Initialize encoder.encoders.11.self_attn.linear_q.bias to zeros
2025-04-17 12:43:21,691 - root - INFO - Initialize encoder.encoders.11.self_attn.linear_k.bias to zeros
2025-04-17 12:43:21,691 - root - INFO - Initialize encoder.encoders.11.self_attn.linear_v.bias to zeros
2025-04-17 12:43:21,691 - root - INFO - Initialize encoder.encoders.11.self_attn.linear_out.bias to zeros
2025-04-17 12:43:21,692 - root - INFO - Initialize encoder.encoders.11.feed_forward.w_1.bias to zeros
2025-04-17 12:43:21,692 - root - INFO - Initialize encoder.encoders.11.feed_forward.w_2.bias to zeros
2025-04-17 12:43:21,693 - root - INFO - Initialize encoder.encoders.11.norm1.bias to zeros
2025-04-17 12:43:21,693 - root - INFO - Initialize encoder.encoders.11.norm2.bias to zeros
2025-04-17 12:43:21,693 - root - INFO - Initialize encoder.encoders.12.self_attn.linear_q.bias to zeros
2025-04-17 12:43:21,693 - root - INFO - Initialize encoder.encoders.12.self_attn.linear_k.bias to zeros
2025-04-17 12:43:21,694 - root - INFO - Initialize encoder.encoders.12.self_attn.linear_v.bias to zeros
2025-04-17 12:43:21,694 - root - INFO - Initialize encoder.encoders.12.self_attn.linear_out.bias to zeros
2025-04-17 12:43:21,694 - root - INFO - Initialize encoder.encoders.12.feed_forward.w_1.bias to zeros
2025-04-17 12:43:21,695 - root - INFO - Initialize encoder.encoders.12.feed_forward.w_2.bias to zeros
2025-04-17 12:43:21,695 - root - INFO - Initialize encoder.encoders.12.norm1.bias to zeros
2025-04-17 12:43:21,695 - root - INFO - Initialize encoder.encoders.12.norm2.bias to zeros
2025-04-17 12:43:21,696 - root - INFO - Initialize encoder.encoders.13.self_attn.linear_q.bias to zeros
2025-04-17 12:43:21,696 - root - INFO - Initialize encoder.encoders.13.self_attn.linear_k.bias to zeros
2025-04-17 12:43:21,696 - root - INFO - Initialize encoder.encoders.13.self_attn.linear_v.bias to zeros
2025-04-17 12:43:21,697 - root - INFO - Initialize encoder.encoders.13.self_attn.linear_out.bias to zeros
2025-04-17 12:43:21,697 - root - INFO - Initialize encoder.encoders.13.feed_forward.w_1.bias to zeros
2025-04-17 12:43:21,697 - root - INFO - Initialize encoder.encoders.13.feed_forward.w_2.bias to zeros
2025-04-17 12:43:21,698 - root - INFO - Initialize encoder.encoders.13.norm1.bias to zeros
2025-04-17 12:43:21,699 - root - INFO - Initialize encoder.encoders.13.norm2.bias to zeros
2025-04-17 12:43:21,699 - root - INFO - Initialize encoder.encoders.14.self_attn.linear_q.bias to zeros
2025-04-17 12:43:21,699 - root - INFO - Initialize encoder.encoders.14.self_attn.linear_k.bias to zeros
2025-04-17 12:43:21,699 - root - INFO - Initialize encoder.encoders.14.self_attn.linear_v.bias to zeros
2025-04-17 12:43:21,700 - root - INFO - Initialize encoder.encoders.14.self_attn.linear_out.bias to zeros
2025-04-17 12:43:21,700 - root - INFO - Initialize encoder.encoders.14.feed_forward.w_1.bias to zeros
2025-04-17 12:43:21,700 - root - INFO - Initialize encoder.encoders.14.feed_forward.w_2.bias to zeros
2025-04-17 12:43:21,701 - root - INFO - Initialize encoder.encoders.14.norm1.bias to zeros
2025-04-17 12:43:21,702 - root - INFO - Initialize encoder.encoders.14.norm2.bias to zeros
2025-04-17 12:43:21,702 - root - INFO - Initialize encoder.encoders.15.self_attn.linear_q.bias to zeros
2025-04-17 12:43:21,703 - root - INFO - Initialize encoder.encoders.15.self_attn.linear_k.bias to zeros
2025-04-17 12:43:21,703 - root - INFO - Initialize encoder.encoders.15.self_attn.linear_v.bias to zeros
2025-04-17 12:43:21,703 - root - INFO - Initialize encoder.encoders.15.self_attn.linear_out.bias to zeros
2025-04-17 12:43:21,704 - root - INFO - Initialize encoder.encoders.15.feed_forward.w_1.bias to zeros
2025-04-17 12:43:21,704 - root - INFO - Initialize encoder.encoders.15.feed_forward.w_2.bias to zeros
2025-04-17 12:43:21,704 - root - INFO - Initialize encoder.encoders.15.norm1.bias to zeros
2025-04-17 12:43:21,704 - root - INFO - Initialize encoder.encoders.15.norm2.bias to zeros
2025-04-17 12:43:21,705 - root - INFO - Initialize encoder.encoders.16.self_attn.linear_q.bias to zeros
2025-04-17 12:43:21,705 - root - INFO - Initialize encoder.encoders.16.self_attn.linear_k.bias to zeros
2025-04-17 12:43:21,705 - root - INFO - Initialize encoder.encoders.16.self_attn.linear_v.bias to zeros
2025-04-17 12:43:21,706 - root - INFO - Initialize encoder.encoders.16.self_attn.linear_out.bias to zeros
2025-04-17 12:43:21,706 - root - INFO - Initialize encoder.encoders.16.feed_forward.w_1.bias to zeros
2025-04-17 12:43:21,706 - root - INFO - Initialize encoder.encoders.16.feed_forward.w_2.bias to zeros
2025-04-17 12:43:21,707 - root - INFO - Initialize encoder.encoders.16.norm1.bias to zeros
2025-04-17 12:43:21,707 - root - INFO - Initialize encoder.encoders.16.norm2.bias to zeros
2025-04-17 12:43:21,708 - root - INFO - Initialize encoder.encoders.17.self_attn.linear_q.bias to zeros
2025-04-17 12:43:21,708 - root - INFO - Initialize encoder.encoders.17.self_attn.linear_k.bias to zeros
2025-04-17 12:43:21,708 - root - INFO - Initialize encoder.encoders.17.self_attn.linear_v.bias to zeros
2025-04-17 12:43:21,708 - root - INFO - Initialize encoder.encoders.17.self_attn.linear_out.bias to zeros
2025-04-17 12:43:21,709 - root - INFO - Initialize encoder.encoders.17.feed_forward.w_1.bias to zeros
2025-04-17 12:43:21,709 - root - INFO - Initialize encoder.encoders.17.feed_forward.w_2.bias to zeros
2025-04-17 12:43:21,709 - root - INFO - Initialize encoder.encoders.17.norm1.bias to zeros
2025-04-17 12:43:21,710 - root - INFO - Initialize encoder.encoders.17.norm2.bias to zeros
2025-04-17 12:43:21,710 - root - INFO - Initialize encoder.after_norm.bias to zeros
2025-04-17 12:43:21,710 - root - INFO - Initialize decoder.after_norm.bias to zeros
2025-04-17 12:43:21,711 - root - INFO - Initialize decoder.output_layer.bias to zeros
2025-04-17 12:43:21,711 - root - INFO - Initialize decoder.decoders.0.self_attn.linear_q.bias to zeros
2025-04-17 12:43:21,711 - root - INFO - Initialize decoder.decoders.0.self_attn.linear_k.bias to zeros
2025-04-17 12:43:21,712 - root - INFO - Initialize decoder.decoders.0.self_attn.linear_v.bias to zeros
2025-04-17 12:43:21,712 - root - INFO - Initialize decoder.decoders.0.self_attn.linear_out.bias to zeros
2025-04-17 12:43:21,713 - root - INFO - Initialize decoder.decoders.0.src_attn.linear_q.bias to zeros
2025-04-17 12:43:21,713 - root - INFO - Initialize decoder.decoders.0.src_attn.linear_k.bias to zeros
2025-04-17 12:43:21,713 - root - INFO - Initialize decoder.decoders.0.src_attn.linear_v.bias to zeros
2025-04-17 12:43:21,714 - root - INFO - Initialize decoder.decoders.0.src_attn.linear_out.bias to zeros
2025-04-17 12:43:21,714 - root - INFO - Initialize decoder.decoders.0.feed_forward.w_1.bias to zeros
2025-04-17 12:43:21,714 - root - INFO - Initialize decoder.decoders.0.feed_forward.w_2.bias to zeros
2025-04-17 12:43:21,715 - root - INFO - Initialize decoder.decoders.0.norm1.bias to zeros
2025-04-17 12:43:21,715 - root - INFO - Initialize decoder.decoders.0.norm2.bias to zeros
2025-04-17 12:43:21,715 - root - INFO - Initialize decoder.decoders.0.norm3.bias to zeros
2025-04-17 12:43:21,716 - root - INFO - Initialize decoder.decoders.1.self_attn.linear_q.bias to zeros
2025-04-17 12:43:21,716 - root - INFO - Initialize decoder.decoders.1.self_attn.linear_k.bias to zeros
2025-04-17 12:43:21,716 - root - INFO - Initialize decoder.decoders.1.self_attn.linear_v.bias to zeros
2025-04-17 12:43:21,716 - root - INFO - Initialize decoder.decoders.1.self_attn.linear_out.bias to zeros
2025-04-17 12:43:21,717 - root - INFO - Initialize decoder.decoders.1.src_attn.linear_q.bias to zeros
2025-04-17 12:43:21,717 - root - INFO - Initialize decoder.decoders.1.src_attn.linear_k.bias to zeros
2025-04-17 12:43:21,718 - root - INFO - Initialize decoder.decoders.1.src_attn.linear_v.bias to zeros
2025-04-17 12:43:21,718 - root - INFO - Initialize decoder.decoders.1.src_attn.linear_out.bias to zeros
2025-04-17 12:43:21,718 - root - INFO - Initialize decoder.decoders.1.feed_forward.w_1.bias to zeros
2025-04-17 12:43:21,719 - root - INFO - Initialize decoder.decoders.1.feed_forward.w_2.bias to zeros
2025-04-17 12:43:21,719 - root - INFO - Initialize decoder.decoders.1.norm1.bias to zeros
2025-04-17 12:43:21,719 - root - INFO - Initialize decoder.decoders.1.norm2.bias to zeros
2025-04-17 12:43:21,720 - root - INFO - Initialize decoder.decoders.1.norm3.bias to zeros
2025-04-17 12:43:21,720 - root - INFO - Initialize decoder.decoders.2.self_attn.linear_q.bias to zeros
2025-04-17 12:43:21,720 - root - INFO - Initialize decoder.decoders.2.self_attn.linear_k.bias to zeros
2025-04-17 12:43:21,721 - root - INFO - Initialize decoder.decoders.2.self_attn.linear_v.bias to zeros
2025-04-17 12:43:21,721 - root - INFO - Initialize decoder.decoders.2.self_attn.linear_out.bias to zeros
2025-04-17 12:43:21,721 - root - INFO - Initialize decoder.decoders.2.src_attn.linear_q.bias to zeros
2025-04-17 12:43:21,722 - root - INFO - Initialize decoder.decoders.2.src_attn.linear_k.bias to zeros
2025-04-17 12:43:21,722 - root - INFO - Initialize decoder.decoders.2.src_attn.linear_v.bias to zeros
2025-04-17 12:43:21,722 - root - INFO - Initialize decoder.decoders.2.src_attn.linear_out.bias to zeros
2025-04-17 12:43:21,723 - root - INFO - Initialize decoder.decoders.2.feed_forward.w_1.bias to zeros
2025-04-17 12:43:21,723 - root - INFO - Initialize decoder.decoders.2.feed_forward.w_2.bias to zeros
2025-04-17 12:43:21,723 - root - INFO - Initialize decoder.decoders.2.norm1.bias to zeros
2025-04-17 12:43:21,723 - root - INFO - Initialize decoder.decoders.2.norm2.bias to zeros
2025-04-17 12:43:21,724 - root - INFO - Initialize decoder.decoders.2.norm3.bias to zeros
2025-04-17 12:43:21,724 - root - INFO - Initialize decoder.decoders.3.self_attn.linear_q.bias to zeros
2025-04-17 12:43:21,724 - root - INFO - Initialize decoder.decoders.3.self_attn.linear_k.bias to zeros
2025-04-17 12:43:21,725 - root - INFO - Initialize decoder.decoders.3.self_attn.linear_v.bias to zeros
2025-04-17 12:43:21,725 - root - INFO - Initialize decoder.decoders.3.self_attn.linear_out.bias to zeros
2025-04-17 12:43:21,725 - root - INFO - Initialize decoder.decoders.3.src_attn.linear_q.bias to zeros
2025-04-17 12:43:21,726 - root - INFO - Initialize decoder.decoders.3.src_attn.linear_k.bias to zeros
2025-04-17 12:43:21,726 - root - INFO - Initialize decoder.decoders.3.src_attn.linear_v.bias to zeros
2025-04-17 12:43:21,726 - root - INFO - Initialize decoder.decoders.3.src_attn.linear_out.bias to zeros
2025-04-17 12:43:21,727 - root - INFO - Initialize decoder.decoders.3.feed_forward.w_1.bias to zeros
2025-04-17 12:43:21,727 - root - INFO - Initialize decoder.decoders.3.feed_forward.w_2.bias to zeros
2025-04-17 12:43:21,727 - root - INFO - Initialize decoder.decoders.3.norm1.bias to zeros
2025-04-17 12:43:21,728 - root - INFO - Initialize decoder.decoders.3.norm2.bias to zeros
2025-04-17 12:43:21,728 - root - INFO - Initialize decoder.decoders.3.norm3.bias to zeros
2025-04-17 12:43:21,729 - root - INFO - Initialize decoder.decoders.4.self_attn.linear_q.bias to zeros
2025-04-17 12:43:21,729 - root - INFO - Initialize decoder.decoders.4.self_attn.linear_k.bias to zeros
2025-04-17 12:43:21,729 - root - INFO - Initialize decoder.decoders.4.self_attn.linear_v.bias to zeros
2025-04-17 12:43:21,729 - root - INFO - Initialize decoder.decoders.4.self_attn.linear_out.bias to zeros
2025-04-17 12:43:21,730 - root - INFO - Initialize decoder.decoders.4.src_attn.linear_q.bias to zeros
2025-04-17 12:43:21,730 - root - INFO - Initialize decoder.decoders.4.src_attn.linear_k.bias to zeros
2025-04-17 12:43:21,731 - root - INFO - Initialize decoder.decoders.4.src_attn.linear_v.bias to zeros
2025-04-17 12:43:21,731 - root - INFO - Initialize decoder.decoders.4.src_attn.linear_out.bias to zeros
2025-04-17 12:43:21,731 - root - INFO - Initialize decoder.decoders.4.feed_forward.w_1.bias to zeros
2025-04-17 12:43:21,732 - root - INFO - Initialize decoder.decoders.4.feed_forward.w_2.bias to zeros
2025-04-17 12:43:21,732 - root - INFO - Initialize decoder.decoders.4.norm1.bias to zeros
2025-04-17 12:43:21,732 - root - INFO - Initialize decoder.decoders.4.norm2.bias to zeros
2025-04-17 12:43:21,732 - root - INFO - Initialize decoder.decoders.4.norm3.bias to zeros
2025-04-17 12:43:21,733 - root - INFO - Initialize decoder.decoders.5.self_attn.linear_q.bias to zeros
2025-04-17 12:43:21,733 - root - INFO - Initialize decoder.decoders.5.self_attn.linear_k.bias to zeros
2025-04-17 12:43:21,733 - root - INFO - Initialize decoder.decoders.5.self_attn.linear_v.bias to zeros
2025-04-17 12:43:21,734 - root - INFO - Initialize decoder.decoders.5.self_attn.linear_out.bias to zeros
2025-04-17 12:43:21,734 - root - INFO - Initialize decoder.decoders.5.src_attn.linear_q.bias to zeros
2025-04-17 12:43:21,735 - root - INFO - Initialize decoder.decoders.5.src_attn.linear_k.bias to zeros
2025-04-17 12:43:21,735 - root - INFO - Initialize decoder.decoders.5.src_attn.linear_v.bias to zeros
2025-04-17 12:43:21,735 - root - INFO - Initialize decoder.decoders.5.src_attn.linear_out.bias to zeros
2025-04-17 12:43:21,736 - root - INFO - Initialize decoder.decoders.5.feed_forward.w_1.bias to zeros
2025-04-17 12:43:21,736 - root - INFO - Initialize decoder.decoders.5.feed_forward.w_2.bias to zeros
2025-04-17 12:43:21,736 - root - INFO - Initialize decoder.decoders.5.norm1.bias to zeros
2025-04-17 12:43:21,737 - root - INFO - Initialize decoder.decoders.5.norm2.bias to zeros
2025-04-17 12:43:21,737 - root - INFO - Initialize decoder.decoders.5.norm3.bias to zeros
2025-04-17 12:43:21,737 - root - INFO - Initialize ctc.ctc_lo.bias to zeros
2025-04-17 12:43:22,584 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--espnet--shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best/snapshots/bc6bbd771cec698f070640ee677a66719181f0a2/exp/lm_train_lm_adam_bpe/config.yaml
2025-04-17 12:43:23,388 - root - INFO - Vocabulary size: 5000
2025-04-17 12:43:24,762 - espnet2.bin.asr_inference - INFO - BatchBeamSearch implementation is selected.
2025-04-17 12:43:24,777 - espnet2.bin.asr_inference - INFO - Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (lm): SequentialRNNLM(
      (drop): Dropout(p=0.0, inplace=False)
      (encoder): Embedding(5000, 2048, padding_idx=0)
      (rnn): LSTM(2048, 2048, num_layers=4, batch_first=True)
      (decoder): Linear(in_features=2048, out_features=5000, bias=True)
    )
  )
)
2025-04-17 12:43:24,778 - espnet2.bin.asr_inference - INFO - Decoding device=cuda, dtype=float32
2025-04-17 12:43:24,780 - espnet2.bin.asr_inference - INFO - Text tokenizer: SentencepiecesTokenizer(model="data/token_list/bpe_unigram5000/model.model")
2025-04-17 12:43:24,781 - asr_multi - INFO - Loaded ASR models: ['Conformer6', 'TransformerE18']
2025-04-17 12:43:24,899 - asr_multi - INFO - Launching Gradio app…
2025-04-17 12:47:18,692 - asr_multi - INFO - Audio in: sr=48000, length=4.32 s, peak=2070961024.00
2025-04-17 12:47:18,693 - asr_multi - INFO - Normalized (peak=2070961024.00)
2025-04-17 12:47:19,946 - asr_multi - INFO - Saved debug_samples/input_20250417_124719.wav (4.32 s)
2025-04-17 12:47:19,947 - espnet2.bin.asr_inference - INFO - speech length: 69120
2025-04-17 12:47:20,262 - espnet.nets.beam_search - INFO - decoder input length: 67
2025-04-17 12:47:20,262 - espnet.nets.beam_search - INFO - max output length: 67
2025-04-17 12:47:20,263 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 12:47:20,871 - espnet.nets.beam_search - INFO - end detected at 15
2025-04-17 12:47:20,873 - espnet.nets.beam_search - INFO - -12.07 * 0.7 =  -8.45 for decoder
2025-04-17 12:47:20,874 - espnet.nets.beam_search - INFO - -10.86 * 0.3 =  -3.26 for ctc
2025-04-17 12:47:20,874 - espnet.nets.beam_search - INFO - -30.67 * 1.0 = -30.67 for lm
2025-04-17 12:47:20,875 - espnet.nets.beam_search - INFO - total log probability: -42.37
2025-04-17 12:47:20,875 - espnet.nets.beam_search - INFO - normalized log probability: -4.24
2025-04-17 12:47:20,875 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 18
2025-04-17 12:47:20,876 - espnet.nets.beam_search - INFO - best hypo: ▁HELLO▁MY▁NAME▁IS▁GARRY

2025-04-17 12:47:20,883 - asr_multi - INFO - Recognized: HELLO MY NAME IS GARRY
2025-04-17 12:47:42,474 - asr_multi - INFO - Audio in: sr=48000, length=4.32 s, peak=2070961024.00
2025-04-17 12:47:42,474 - asr_multi - INFO - Normalized (peak=2070961024.00)
2025-04-17 12:47:42,479 - asr_multi - INFO - Saved debug_samples/input_20250417_124742.wav (4.32 s)
2025-04-17 12:47:42,479 - espnet2.bin.asr_inference - INFO - speech length: 69120
2025-04-17 12:47:42,527 - espnet.nets.beam_search - INFO - decoder input length: 89
2025-04-17 12:47:42,527 - espnet.nets.beam_search - INFO - max output length: 89
2025-04-17 12:47:42,528 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 12:47:42,981 - espnet.nets.beam_search - INFO - end detected at 17
2025-04-17 12:47:42,982 - espnet.nets.beam_search - INFO - -10.78 * 0.7 =  -7.54 for decoder
2025-04-17 12:47:42,982 - espnet.nets.beam_search - INFO - -43.43 * 0.3 = -13.03 for ctc
2025-04-17 12:47:42,982 - espnet.nets.beam_search - INFO - -30.96 * 1.0 = -30.96 for lm
2025-04-17 12:47:42,983 - espnet.nets.beam_search - INFO - total log probability: -51.54
2025-04-17 12:47:42,983 - espnet.nets.beam_search - INFO - normalized log probability: -4.29
2025-04-17 12:47:42,983 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 31
2025-04-17 12:47:42,984 - espnet.nets.beam_search - INFO - best hypo: ▁HELLO▁MY▁NAME▁IS▁GABRIEL

2025-04-17 12:47:42,984 - asr_multi - ERROR - ASR error Not found: "data/token_list/bpe_unigram5000/model.model": No such file or directory Error #2
Traceback (most recent call last):
  File "/home/ambuja/emo-tts/src/gradio_app.py", line 167, in transcribe_and_speak
    nbests = model(wav)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet2/bin/asr_inference.py", line 559, in __call__
    results = self._decode_single_sample(enc[0])
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet2/bin/asr_inference.py", line 676, in _decode_single_sample
    text = self.tokenizer.tokens2text(token)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet2/text/sentencepiece_tokenizer.py", line 36, in tokens2text
    self._build_sentence_piece_processor()
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet2/text/sentencepiece_tokenizer.py", line 29, in _build_sentence_piece_processor
    self.sp.load(self.model)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/sentencepiece/__init__.py", line 961, in Load
    return self.LoadFromFile(model_file)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/sentencepiece/__init__.py", line 316, in LoadFromFile
    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)
OSError: Not found: "data/token_list/bpe_unigram5000/model.model": No such file or directory Error #2
2025-04-17 12:50:56,007 - asr_multi - INFO - Starting multi-model ASR/TTS demo
2025-04-17 12:50:56,008 - asr_multi - INFO - Using device: cuda
2025-04-17 12:50:56,015 - asr_multi - INFO - Downloading ASR model 'Conformer6' → kamo-naoyuki/librispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp_valid.acc.ave
2025-04-17 12:51:06,840 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp/config.yaml
2025-04-17 12:51:07,671 - root - INFO - Vocabulary size: 5000
2025-04-17 12:51:07,746 - root - WARNING - Using legacy_rel_pos and it will be deprecated in the future.
2025-04-17 12:51:07,804 - root - WARNING - Using legacy_rel_selfattn and it will be deprecated in the future.
2025-04-17 12:51:08,282 - root - INFO - Gradient checkpoint layers: []
2025-04-17 12:51:09,673 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/lm_train_lm_transformer2_bpe5000_scheduler_confwarmup_steps25000_batch_bins500000000_accum_grad2_use_amptrue/config.yaml
2025-04-17 12:51:10,492 - root - INFO - Vocabulary size: 5000
2025-04-17 12:51:10,496 - root - INFO - encoder self-attention layer type = self-attention
2025-04-17 12:51:11,170 - espnet2.bin.asr_inference - INFO - BatchBeamSearch implementation is selected.
2025-04-17 12:51:11,200 - espnet2.bin.asr_inference - INFO - Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (lm): TransformerLM(
      (embed): Embedding(5000, 128)
      (encoder): Encoder(
        (embed): Sequential(
          (0): Linear(in_features=128, out_features=512, bias=True)
          (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): Dropout(p=0.0, inplace=False)
          (3): ReLU()
          (4): Sequential()
        )
        (encoders): MultiSequential(
          (0): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (4): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (5): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (6): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (7): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (8): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (9): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (10): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (11): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (12): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (13): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (14): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (15): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): Linear(in_features=512, out_features=5000, bias=True)
    )
  )
)
2025-04-17 12:51:11,201 - espnet2.bin.asr_inference - INFO - Decoding device=cuda, dtype=float32
2025-04-17 12:51:11,204 - espnet2.bin.asr_inference - INFO - Text tokenizer: SentencepiecesTokenizer(model="/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/data/en_token_list/bpe_unigram5000/bpe.model")
2025-04-17 12:51:11,207 - asr_multi - INFO - Downloading ASR model 'TransformerE18' → espnet/shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best
2025-04-17 12:51:11,299 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--espnet--shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best/snapshots/bc6bbd771cec698f070640ee677a66719181f0a2/exp/asr_train_asr_transformer_e18_raw_bpe_sp/config.yaml
2025-04-17 12:51:12,124 - root - INFO - Vocabulary size: 5000
2025-04-17 12:51:12,455 - root - INFO - Gradient checkpoint layers: []
2025-04-17 12:51:13,024 - root - INFO - Initialize encoder.embed.conv.0.bias to zeros
2025-04-17 12:51:13,025 - root - INFO - Initialize encoder.embed.conv.2.bias to zeros
2025-04-17 12:51:13,025 - root - INFO - Initialize encoder.embed.out.0.bias to zeros
2025-04-17 12:51:13,026 - root - INFO - Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
2025-04-17 12:51:13,026 - root - INFO - Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
2025-04-17 12:51:13,026 - root - INFO - Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
2025-04-17 12:51:13,027 - root - INFO - Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
2025-04-17 12:51:13,027 - root - INFO - Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
2025-04-17 12:51:13,027 - root - INFO - Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
2025-04-17 12:51:13,028 - root - INFO - Initialize encoder.encoders.0.norm1.bias to zeros
2025-04-17 12:51:13,028 - root - INFO - Initialize encoder.encoders.0.norm2.bias to zeros
2025-04-17 12:51:13,028 - root - INFO - Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
2025-04-17 12:51:13,029 - root - INFO - Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
2025-04-17 12:51:13,029 - root - INFO - Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
2025-04-17 12:51:13,029 - root - INFO - Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
2025-04-17 12:51:13,030 - root - INFO - Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
2025-04-17 12:51:13,030 - root - INFO - Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
2025-04-17 12:51:13,030 - root - INFO - Initialize encoder.encoders.1.norm1.bias to zeros
2025-04-17 12:51:13,031 - root - INFO - Initialize encoder.encoders.1.norm2.bias to zeros
2025-04-17 12:51:13,031 - root - INFO - Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
2025-04-17 12:51:13,032 - root - INFO - Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
2025-04-17 12:51:13,032 - root - INFO - Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
2025-04-17 12:51:13,032 - root - INFO - Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
2025-04-17 12:51:13,033 - root - INFO - Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
2025-04-17 12:51:13,033 - root - INFO - Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
2025-04-17 12:51:13,033 - root - INFO - Initialize encoder.encoders.2.norm1.bias to zeros
2025-04-17 12:51:13,033 - root - INFO - Initialize encoder.encoders.2.norm2.bias to zeros
2025-04-17 12:51:13,034 - root - INFO - Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
2025-04-17 12:51:13,034 - root - INFO - Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
2025-04-17 12:51:13,034 - root - INFO - Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
2025-04-17 12:51:13,035 - root - INFO - Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
2025-04-17 12:51:13,035 - root - INFO - Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
2025-04-17 12:51:13,035 - root - INFO - Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
2025-04-17 12:51:13,036 - root - INFO - Initialize encoder.encoders.3.norm1.bias to zeros
2025-04-17 12:51:13,036 - root - INFO - Initialize encoder.encoders.3.norm2.bias to zeros
2025-04-17 12:51:13,036 - root - INFO - Initialize encoder.encoders.4.self_attn.linear_q.bias to zeros
2025-04-17 12:51:13,037 - root - INFO - Initialize encoder.encoders.4.self_attn.linear_k.bias to zeros
2025-04-17 12:51:13,037 - root - INFO - Initialize encoder.encoders.4.self_attn.linear_v.bias to zeros
2025-04-17 12:51:13,037 - root - INFO - Initialize encoder.encoders.4.self_attn.linear_out.bias to zeros
2025-04-17 12:51:13,038 - root - INFO - Initialize encoder.encoders.4.feed_forward.w_1.bias to zeros
2025-04-17 12:51:13,038 - root - INFO - Initialize encoder.encoders.4.feed_forward.w_2.bias to zeros
2025-04-17 12:51:13,038 - root - INFO - Initialize encoder.encoders.4.norm1.bias to zeros
2025-04-17 12:51:13,039 - root - INFO - Initialize encoder.encoders.4.norm2.bias to zeros
2025-04-17 12:51:13,039 - root - INFO - Initialize encoder.encoders.5.self_attn.linear_q.bias to zeros
2025-04-17 12:51:13,039 - root - INFO - Initialize encoder.encoders.5.self_attn.linear_k.bias to zeros
2025-04-17 12:51:13,040 - root - INFO - Initialize encoder.encoders.5.self_attn.linear_v.bias to zeros
2025-04-17 12:51:13,040 - root - INFO - Initialize encoder.encoders.5.self_attn.linear_out.bias to zeros
2025-04-17 12:51:13,040 - root - INFO - Initialize encoder.encoders.5.feed_forward.w_1.bias to zeros
2025-04-17 12:51:13,041 - root - INFO - Initialize encoder.encoders.5.feed_forward.w_2.bias to zeros
2025-04-17 12:51:13,041 - root - INFO - Initialize encoder.encoders.5.norm1.bias to zeros
2025-04-17 12:51:13,041 - root - INFO - Initialize encoder.encoders.5.norm2.bias to zeros
2025-04-17 12:51:13,042 - root - INFO - Initialize encoder.encoders.6.self_attn.linear_q.bias to zeros
2025-04-17 12:51:13,042 - root - INFO - Initialize encoder.encoders.6.self_attn.linear_k.bias to zeros
2025-04-17 12:51:13,042 - root - INFO - Initialize encoder.encoders.6.self_attn.linear_v.bias to zeros
2025-04-17 12:51:13,042 - root - INFO - Initialize encoder.encoders.6.self_attn.linear_out.bias to zeros
2025-04-17 12:51:13,043 - root - INFO - Initialize encoder.encoders.6.feed_forward.w_1.bias to zeros
2025-04-17 12:51:13,043 - root - INFO - Initialize encoder.encoders.6.feed_forward.w_2.bias to zeros
2025-04-17 12:51:13,043 - root - INFO - Initialize encoder.encoders.6.norm1.bias to zeros
2025-04-17 12:51:13,044 - root - INFO - Initialize encoder.encoders.6.norm2.bias to zeros
2025-04-17 12:51:13,044 - root - INFO - Initialize encoder.encoders.7.self_attn.linear_q.bias to zeros
2025-04-17 12:51:13,044 - root - INFO - Initialize encoder.encoders.7.self_attn.linear_k.bias to zeros
2025-04-17 12:51:13,045 - root - INFO - Initialize encoder.encoders.7.self_attn.linear_v.bias to zeros
2025-04-17 12:51:13,045 - root - INFO - Initialize encoder.encoders.7.self_attn.linear_out.bias to zeros
2025-04-17 12:51:13,045 - root - INFO - Initialize encoder.encoders.7.feed_forward.w_1.bias to zeros
2025-04-17 12:51:13,046 - root - INFO - Initialize encoder.encoders.7.feed_forward.w_2.bias to zeros
2025-04-17 12:51:13,046 - root - INFO - Initialize encoder.encoders.7.norm1.bias to zeros
2025-04-17 12:51:13,046 - root - INFO - Initialize encoder.encoders.7.norm2.bias to zeros
2025-04-17 12:51:13,047 - root - INFO - Initialize encoder.encoders.8.self_attn.linear_q.bias to zeros
2025-04-17 12:51:13,047 - root - INFO - Initialize encoder.encoders.8.self_attn.linear_k.bias to zeros
2025-04-17 12:51:13,047 - root - INFO - Initialize encoder.encoders.8.self_attn.linear_v.bias to zeros
2025-04-17 12:51:13,048 - root - INFO - Initialize encoder.encoders.8.self_attn.linear_out.bias to zeros
2025-04-17 12:51:13,048 - root - INFO - Initialize encoder.encoders.8.feed_forward.w_1.bias to zeros
2025-04-17 12:51:13,048 - root - INFO - Initialize encoder.encoders.8.feed_forward.w_2.bias to zeros
2025-04-17 12:51:13,049 - root - INFO - Initialize encoder.encoders.8.norm1.bias to zeros
2025-04-17 12:51:13,049 - root - INFO - Initialize encoder.encoders.8.norm2.bias to zeros
2025-04-17 12:51:13,049 - root - INFO - Initialize encoder.encoders.9.self_attn.linear_q.bias to zeros
2025-04-17 12:51:13,050 - root - INFO - Initialize encoder.encoders.9.self_attn.linear_k.bias to zeros
2025-04-17 12:51:13,050 - root - INFO - Initialize encoder.encoders.9.self_attn.linear_v.bias to zeros
2025-04-17 12:51:13,050 - root - INFO - Initialize encoder.encoders.9.self_attn.linear_out.bias to zeros
2025-04-17 12:51:13,051 - root - INFO - Initialize encoder.encoders.9.feed_forward.w_1.bias to zeros
2025-04-17 12:51:13,051 - root - INFO - Initialize encoder.encoders.9.feed_forward.w_2.bias to zeros
2025-04-17 12:51:13,051 - root - INFO - Initialize encoder.encoders.9.norm1.bias to zeros
2025-04-17 12:51:13,052 - root - INFO - Initialize encoder.encoders.9.norm2.bias to zeros
2025-04-17 12:51:13,052 - root - INFO - Initialize encoder.encoders.10.self_attn.linear_q.bias to zeros
2025-04-17 12:51:13,052 - root - INFO - Initialize encoder.encoders.10.self_attn.linear_k.bias to zeros
2025-04-17 12:51:13,052 - root - INFO - Initialize encoder.encoders.10.self_attn.linear_v.bias to zeros
2025-04-17 12:51:13,053 - root - INFO - Initialize encoder.encoders.10.self_attn.linear_out.bias to zeros
2025-04-17 12:51:13,053 - root - INFO - Initialize encoder.encoders.10.feed_forward.w_1.bias to zeros
2025-04-17 12:51:13,053 - root - INFO - Initialize encoder.encoders.10.feed_forward.w_2.bias to zeros
2025-04-17 12:51:13,054 - root - INFO - Initialize encoder.encoders.10.norm1.bias to zeros
2025-04-17 12:51:13,054 - root - INFO - Initialize encoder.encoders.10.norm2.bias to zeros
2025-04-17 12:51:13,054 - root - INFO - Initialize encoder.encoders.11.self_attn.linear_q.bias to zeros
2025-04-17 12:51:13,055 - root - INFO - Initialize encoder.encoders.11.self_attn.linear_k.bias to zeros
2025-04-17 12:51:13,055 - root - INFO - Initialize encoder.encoders.11.self_attn.linear_v.bias to zeros
2025-04-17 12:51:13,055 - root - INFO - Initialize encoder.encoders.11.self_attn.linear_out.bias to zeros
2025-04-17 12:51:13,056 - root - INFO - Initialize encoder.encoders.11.feed_forward.w_1.bias to zeros
2025-04-17 12:51:13,056 - root - INFO - Initialize encoder.encoders.11.feed_forward.w_2.bias to zeros
2025-04-17 12:51:13,056 - root - INFO - Initialize encoder.encoders.11.norm1.bias to zeros
2025-04-17 12:51:13,057 - root - INFO - Initialize encoder.encoders.11.norm2.bias to zeros
2025-04-17 12:51:13,057 - root - INFO - Initialize encoder.encoders.12.self_attn.linear_q.bias to zeros
2025-04-17 12:51:13,057 - root - INFO - Initialize encoder.encoders.12.self_attn.linear_k.bias to zeros
2025-04-17 12:51:13,058 - root - INFO - Initialize encoder.encoders.12.self_attn.linear_v.bias to zeros
2025-04-17 12:51:13,058 - root - INFO - Initialize encoder.encoders.12.self_attn.linear_out.bias to zeros
2025-04-17 12:51:13,058 - root - INFO - Initialize encoder.encoders.12.feed_forward.w_1.bias to zeros
2025-04-17 12:51:13,059 - root - INFO - Initialize encoder.encoders.12.feed_forward.w_2.bias to zeros
2025-04-17 12:51:13,059 - root - INFO - Initialize encoder.encoders.12.norm1.bias to zeros
2025-04-17 12:51:13,059 - root - INFO - Initialize encoder.encoders.12.norm2.bias to zeros
2025-04-17 12:51:13,060 - root - INFO - Initialize encoder.encoders.13.self_attn.linear_q.bias to zeros
2025-04-17 12:51:13,060 - root - INFO - Initialize encoder.encoders.13.self_attn.linear_k.bias to zeros
2025-04-17 12:51:13,061 - root - INFO - Initialize encoder.encoders.13.self_attn.linear_v.bias to zeros
2025-04-17 12:51:13,061 - root - INFO - Initialize encoder.encoders.13.self_attn.linear_out.bias to zeros
2025-04-17 12:51:13,061 - root - INFO - Initialize encoder.encoders.13.feed_forward.w_1.bias to zeros
2025-04-17 12:51:13,062 - root - INFO - Initialize encoder.encoders.13.feed_forward.w_2.bias to zeros
2025-04-17 12:51:13,062 - root - INFO - Initialize encoder.encoders.13.norm1.bias to zeros
2025-04-17 12:51:13,062 - root - INFO - Initialize encoder.encoders.13.norm2.bias to zeros
2025-04-17 12:51:13,063 - root - INFO - Initialize encoder.encoders.14.self_attn.linear_q.bias to zeros
2025-04-17 12:51:13,063 - root - INFO - Initialize encoder.encoders.14.self_attn.linear_k.bias to zeros
2025-04-17 12:51:13,063 - root - INFO - Initialize encoder.encoders.14.self_attn.linear_v.bias to zeros
2025-04-17 12:51:13,063 - root - INFO - Initialize encoder.encoders.14.self_attn.linear_out.bias to zeros
2025-04-17 12:51:13,064 - root - INFO - Initialize encoder.encoders.14.feed_forward.w_1.bias to zeros
2025-04-17 12:51:13,064 - root - INFO - Initialize encoder.encoders.14.feed_forward.w_2.bias to zeros
2025-04-17 12:51:13,064 - root - INFO - Initialize encoder.encoders.14.norm1.bias to zeros
2025-04-17 12:51:13,065 - root - INFO - Initialize encoder.encoders.14.norm2.bias to zeros
2025-04-17 12:51:13,065 - root - INFO - Initialize encoder.encoders.15.self_attn.linear_q.bias to zeros
2025-04-17 12:51:13,065 - root - INFO - Initialize encoder.encoders.15.self_attn.linear_k.bias to zeros
2025-04-17 12:51:13,066 - root - INFO - Initialize encoder.encoders.15.self_attn.linear_v.bias to zeros
2025-04-17 12:51:13,066 - root - INFO - Initialize encoder.encoders.15.self_attn.linear_out.bias to zeros
2025-04-17 12:51:13,066 - root - INFO - Initialize encoder.encoders.15.feed_forward.w_1.bias to zeros
2025-04-17 12:51:13,067 - root - INFO - Initialize encoder.encoders.15.feed_forward.w_2.bias to zeros
2025-04-17 12:51:13,067 - root - INFO - Initialize encoder.encoders.15.norm1.bias to zeros
2025-04-17 12:51:13,067 - root - INFO - Initialize encoder.encoders.15.norm2.bias to zeros
2025-04-17 12:51:13,068 - root - INFO - Initialize encoder.encoders.16.self_attn.linear_q.bias to zeros
2025-04-17 12:51:13,068 - root - INFO - Initialize encoder.encoders.16.self_attn.linear_k.bias to zeros
2025-04-17 12:51:13,068 - root - INFO - Initialize encoder.encoders.16.self_attn.linear_v.bias to zeros
2025-04-17 12:51:13,069 - root - INFO - Initialize encoder.encoders.16.self_attn.linear_out.bias to zeros
2025-04-17 12:51:13,069 - root - INFO - Initialize encoder.encoders.16.feed_forward.w_1.bias to zeros
2025-04-17 12:51:13,069 - root - INFO - Initialize encoder.encoders.16.feed_forward.w_2.bias to zeros
2025-04-17 12:51:13,070 - root - INFO - Initialize encoder.encoders.16.norm1.bias to zeros
2025-04-17 12:51:13,070 - root - INFO - Initialize encoder.encoders.16.norm2.bias to zeros
2025-04-17 12:51:13,070 - root - INFO - Initialize encoder.encoders.17.self_attn.linear_q.bias to zeros
2025-04-17 12:51:13,070 - root - INFO - Initialize encoder.encoders.17.self_attn.linear_k.bias to zeros
2025-04-17 12:51:13,071 - root - INFO - Initialize encoder.encoders.17.self_attn.linear_v.bias to zeros
2025-04-17 12:51:13,071 - root - INFO - Initialize encoder.encoders.17.self_attn.linear_out.bias to zeros
2025-04-17 12:51:13,072 - root - INFO - Initialize encoder.encoders.17.feed_forward.w_1.bias to zeros
2025-04-17 12:51:13,072 - root - INFO - Initialize encoder.encoders.17.feed_forward.w_2.bias to zeros
2025-04-17 12:51:13,072 - root - INFO - Initialize encoder.encoders.17.norm1.bias to zeros
2025-04-17 12:51:13,072 - root - INFO - Initialize encoder.encoders.17.norm2.bias to zeros
2025-04-17 12:51:13,073 - root - INFO - Initialize encoder.after_norm.bias to zeros
2025-04-17 12:51:13,073 - root - INFO - Initialize decoder.after_norm.bias to zeros
2025-04-17 12:51:13,073 - root - INFO - Initialize decoder.output_layer.bias to zeros
2025-04-17 12:51:13,074 - root - INFO - Initialize decoder.decoders.0.self_attn.linear_q.bias to zeros
2025-04-17 12:51:13,075 - root - INFO - Initialize decoder.decoders.0.self_attn.linear_k.bias to zeros
2025-04-17 12:51:13,075 - root - INFO - Initialize decoder.decoders.0.self_attn.linear_v.bias to zeros
2025-04-17 12:51:13,076 - root - INFO - Initialize decoder.decoders.0.self_attn.linear_out.bias to zeros
2025-04-17 12:51:13,076 - root - INFO - Initialize decoder.decoders.0.src_attn.linear_q.bias to zeros
2025-04-17 12:51:13,076 - root - INFO - Initialize decoder.decoders.0.src_attn.linear_k.bias to zeros
2025-04-17 12:51:13,077 - root - INFO - Initialize decoder.decoders.0.src_attn.linear_v.bias to zeros
2025-04-17 12:51:13,077 - root - INFO - Initialize decoder.decoders.0.src_attn.linear_out.bias to zeros
2025-04-17 12:51:13,077 - root - INFO - Initialize decoder.decoders.0.feed_forward.w_1.bias to zeros
2025-04-17 12:51:13,078 - root - INFO - Initialize decoder.decoders.0.feed_forward.w_2.bias to zeros
2025-04-17 12:51:13,078 - root - INFO - Initialize decoder.decoders.0.norm1.bias to zeros
2025-04-17 12:51:13,078 - root - INFO - Initialize decoder.decoders.0.norm2.bias to zeros
2025-04-17 12:51:13,078 - root - INFO - Initialize decoder.decoders.0.norm3.bias to zeros
2025-04-17 12:51:13,079 - root - INFO - Initialize decoder.decoders.1.self_attn.linear_q.bias to zeros
2025-04-17 12:51:13,079 - root - INFO - Initialize decoder.decoders.1.self_attn.linear_k.bias to zeros
2025-04-17 12:51:13,079 - root - INFO - Initialize decoder.decoders.1.self_attn.linear_v.bias to zeros
2025-04-17 12:51:13,080 - root - INFO - Initialize decoder.decoders.1.self_attn.linear_out.bias to zeros
2025-04-17 12:51:13,080 - root - INFO - Initialize decoder.decoders.1.src_attn.linear_q.bias to zeros
2025-04-17 12:51:13,080 - root - INFO - Initialize decoder.decoders.1.src_attn.linear_k.bias to zeros
2025-04-17 12:51:13,081 - root - INFO - Initialize decoder.decoders.1.src_attn.linear_v.bias to zeros
2025-04-17 12:51:13,081 - root - INFO - Initialize decoder.decoders.1.src_attn.linear_out.bias to zeros
2025-04-17 12:51:13,082 - root - INFO - Initialize decoder.decoders.1.feed_forward.w_1.bias to zeros
2025-04-17 12:51:13,082 - root - INFO - Initialize decoder.decoders.1.feed_forward.w_2.bias to zeros
2025-04-17 12:51:13,082 - root - INFO - Initialize decoder.decoders.1.norm1.bias to zeros
2025-04-17 12:51:13,082 - root - INFO - Initialize decoder.decoders.1.norm2.bias to zeros
2025-04-17 12:51:13,083 - root - INFO - Initialize decoder.decoders.1.norm3.bias to zeros
2025-04-17 12:51:13,083 - root - INFO - Initialize decoder.decoders.2.self_attn.linear_q.bias to zeros
2025-04-17 12:51:13,083 - root - INFO - Initialize decoder.decoders.2.self_attn.linear_k.bias to zeros
2025-04-17 12:51:13,084 - root - INFO - Initialize decoder.decoders.2.self_attn.linear_v.bias to zeros
2025-04-17 12:51:13,084 - root - INFO - Initialize decoder.decoders.2.self_attn.linear_out.bias to zeros
2025-04-17 12:51:13,084 - root - INFO - Initialize decoder.decoders.2.src_attn.linear_q.bias to zeros
2025-04-17 12:51:13,085 - root - INFO - Initialize decoder.decoders.2.src_attn.linear_k.bias to zeros
2025-04-17 12:51:13,085 - root - INFO - Initialize decoder.decoders.2.src_attn.linear_v.bias to zeros
2025-04-17 12:51:13,085 - root - INFO - Initialize decoder.decoders.2.src_attn.linear_out.bias to zeros
2025-04-17 12:51:13,086 - root - INFO - Initialize decoder.decoders.2.feed_forward.w_1.bias to zeros
2025-04-17 12:51:13,086 - root - INFO - Initialize decoder.decoders.2.feed_forward.w_2.bias to zeros
2025-04-17 12:51:13,087 - root - INFO - Initialize decoder.decoders.2.norm1.bias to zeros
2025-04-17 12:51:13,087 - root - INFO - Initialize decoder.decoders.2.norm2.bias to zeros
2025-04-17 12:51:13,087 - root - INFO - Initialize decoder.decoders.2.norm3.bias to zeros
2025-04-17 12:51:13,087 - root - INFO - Initialize decoder.decoders.3.self_attn.linear_q.bias to zeros
2025-04-17 12:51:13,088 - root - INFO - Initialize decoder.decoders.3.self_attn.linear_k.bias to zeros
2025-04-17 12:51:13,088 - root - INFO - Initialize decoder.decoders.3.self_attn.linear_v.bias to zeros
2025-04-17 12:51:13,088 - root - INFO - Initialize decoder.decoders.3.self_attn.linear_out.bias to zeros
2025-04-17 12:51:13,089 - root - INFO - Initialize decoder.decoders.3.src_attn.linear_q.bias to zeros
2025-04-17 12:51:13,089 - root - INFO - Initialize decoder.decoders.3.src_attn.linear_k.bias to zeros
2025-04-17 12:51:13,089 - root - INFO - Initialize decoder.decoders.3.src_attn.linear_v.bias to zeros
2025-04-17 12:51:13,090 - root - INFO - Initialize decoder.decoders.3.src_attn.linear_out.bias to zeros
2025-04-17 12:51:13,090 - root - INFO - Initialize decoder.decoders.3.feed_forward.w_1.bias to zeros
2025-04-17 12:51:13,090 - root - INFO - Initialize decoder.decoders.3.feed_forward.w_2.bias to zeros
2025-04-17 12:51:13,091 - root - INFO - Initialize decoder.decoders.3.norm1.bias to zeros
2025-04-17 12:51:13,091 - root - INFO - Initialize decoder.decoders.3.norm2.bias to zeros
2025-04-17 12:51:13,091 - root - INFO - Initialize decoder.decoders.3.norm3.bias to zeros
2025-04-17 12:51:13,092 - root - INFO - Initialize decoder.decoders.4.self_attn.linear_q.bias to zeros
2025-04-17 12:51:13,092 - root - INFO - Initialize decoder.decoders.4.self_attn.linear_k.bias to zeros
2025-04-17 12:51:13,092 - root - INFO - Initialize decoder.decoders.4.self_attn.linear_v.bias to zeros
2025-04-17 12:51:13,093 - root - INFO - Initialize decoder.decoders.4.self_attn.linear_out.bias to zeros
2025-04-17 12:51:13,093 - root - INFO - Initialize decoder.decoders.4.src_attn.linear_q.bias to zeros
2025-04-17 12:51:13,093 - root - INFO - Initialize decoder.decoders.4.src_attn.linear_k.bias to zeros
2025-04-17 12:51:13,093 - root - INFO - Initialize decoder.decoders.4.src_attn.linear_v.bias to zeros
2025-04-17 12:51:13,094 - root - INFO - Initialize decoder.decoders.4.src_attn.linear_out.bias to zeros
2025-04-17 12:51:13,094 - root - INFO - Initialize decoder.decoders.4.feed_forward.w_1.bias to zeros
2025-04-17 12:51:13,094 - root - INFO - Initialize decoder.decoders.4.feed_forward.w_2.bias to zeros
2025-04-17 12:51:13,095 - root - INFO - Initialize decoder.decoders.4.norm1.bias to zeros
2025-04-17 12:51:13,095 - root - INFO - Initialize decoder.decoders.4.norm2.bias to zeros
2025-04-17 12:51:13,095 - root - INFO - Initialize decoder.decoders.4.norm3.bias to zeros
2025-04-17 12:51:13,096 - root - INFO - Initialize decoder.decoders.5.self_attn.linear_q.bias to zeros
2025-04-17 12:51:13,096 - root - INFO - Initialize decoder.decoders.5.self_attn.linear_k.bias to zeros
2025-04-17 12:51:13,096 - root - INFO - Initialize decoder.decoders.5.self_attn.linear_v.bias to zeros
2025-04-17 12:51:13,097 - root - INFO - Initialize decoder.decoders.5.self_attn.linear_out.bias to zeros
2025-04-17 12:51:13,097 - root - INFO - Initialize decoder.decoders.5.src_attn.linear_q.bias to zeros
2025-04-17 12:51:13,097 - root - INFO - Initialize decoder.decoders.5.src_attn.linear_k.bias to zeros
2025-04-17 12:51:13,098 - root - INFO - Initialize decoder.decoders.5.src_attn.linear_v.bias to zeros
2025-04-17 12:51:13,098 - root - INFO - Initialize decoder.decoders.5.src_attn.linear_out.bias to zeros
2025-04-17 12:51:13,098 - root - INFO - Initialize decoder.decoders.5.feed_forward.w_1.bias to zeros
2025-04-17 12:51:13,098 - root - INFO - Initialize decoder.decoders.5.feed_forward.w_2.bias to zeros
2025-04-17 12:51:13,099 - root - INFO - Initialize decoder.decoders.5.norm1.bias to zeros
2025-04-17 12:51:13,099 - root - INFO - Initialize decoder.decoders.5.norm2.bias to zeros
2025-04-17 12:51:13,099 - root - INFO - Initialize decoder.decoders.5.norm3.bias to zeros
2025-04-17 12:51:13,100 - root - INFO - Initialize ctc.ctc_lo.bias to zeros
2025-04-17 12:51:13,436 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--espnet--shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best/snapshots/bc6bbd771cec698f070640ee677a66719181f0a2/exp/lm_train_lm_adam_bpe/config.yaml
2025-04-17 12:51:14,248 - root - INFO - Vocabulary size: 5000
2025-04-17 12:51:15,141 - espnet2.bin.asr_inference - INFO - BatchBeamSearch implementation is selected.
2025-04-17 12:51:15,156 - espnet2.bin.asr_inference - INFO - Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (lm): SequentialRNNLM(
      (drop): Dropout(p=0.0, inplace=False)
      (encoder): Embedding(5000, 2048, padding_idx=0)
      (rnn): LSTM(2048, 2048, num_layers=4, batch_first=True)
      (decoder): Linear(in_features=2048, out_features=5000, bias=True)
    )
  )
)
2025-04-17 12:51:15,157 - espnet2.bin.asr_inference - INFO - Decoding device=cuda, dtype=float32
2025-04-17 12:51:15,160 - espnet2.bin.asr_inference - INFO - Text tokenizer: SentencepiecesTokenizer(model="data/token_list/bpe_unigram5000/model.model")
2025-04-17 12:51:15,160 - asr_multi - INFO - Loaded ASR models: ['Conformer6', 'TransformerE18']
2025-04-17 12:51:15,275 - asr_multi - INFO - Launching Gradio app…
2025-04-17 12:51:55,754 - asr_multi - INFO - Audio in: sr=48000, length=4.02 s, peak=1822932480.00
2025-04-17 12:51:55,755 - asr_multi - INFO - Normalized (peak=1822932480.00)
2025-04-17 12:51:57,200 - asr_multi - INFO - Saved debug_samples/input_20250417_125157.wav (4.02 s)
2025-04-17 12:51:57,201 - espnet2.bin.asr_inference - INFO - speech length: 64320
2025-04-17 12:51:57,512 - espnet.nets.beam_search - INFO - decoder input length: 62
2025-04-17 12:51:57,513 - espnet.nets.beam_search - INFO - max output length: 62
2025-04-17 12:51:57,513 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 12:51:58,124 - espnet.nets.beam_search - INFO - end detected at 14
2025-04-17 12:51:58,126 - espnet.nets.beam_search - INFO - -12.12 * 0.7 =  -8.49 for decoder
2025-04-17 12:51:58,126 - espnet.nets.beam_search - INFO - -24.72 * 0.3 =  -7.42 for ctc
2025-04-17 12:51:58,127 - espnet.nets.beam_search - INFO - -23.41 * 1.0 = -23.41 for lm
2025-04-17 12:51:58,127 - espnet.nets.beam_search - INFO - total log probability: -39.31
2025-04-17 12:51:58,127 - espnet.nets.beam_search - INFO - normalized log probability: -3.93
2025-04-17 12:51:58,128 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 22
2025-04-17 12:51:58,128 - espnet.nets.beam_search - INFO - best hypo: ▁HELLO▁I'VE▁GOT▁HIM

2025-04-17 12:51:58,137 - asr_multi - INFO - Recognized: HELLO I'VE GOT HIM
2025-04-17 12:52:17,701 - asr_multi - INFO - Audio in: sr=48000, length=4.02 s, peak=1822932480.00
2025-04-17 12:52:17,702 - asr_multi - INFO - Normalized (peak=1822932480.00)
2025-04-17 12:52:17,706 - asr_multi - INFO - Saved debug_samples/input_20250417_125217.wav (4.02 s)
2025-04-17 12:52:17,707 - espnet2.bin.asr_inference - INFO - speech length: 64320
2025-04-17 12:52:17,760 - espnet.nets.beam_search - INFO - decoder input length: 83
2025-04-17 12:52:17,760 - espnet.nets.beam_search - INFO - max output length: 83
2025-04-17 12:52:17,760 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 12:52:18,156 - espnet.nets.beam_search - INFO - end detected at 15
2025-04-17 12:52:18,157 - espnet.nets.beam_search - INFO - -16.44 * 0.7 = -11.50 for decoder
2025-04-17 12:52:18,158 - espnet.nets.beam_search - INFO - -54.51 * 0.3 = -16.35 for ctc
2025-04-17 12:52:18,159 - espnet.nets.beam_search - INFO - -21.41 * 1.0 = -21.41 for lm
2025-04-17 12:52:18,159 - espnet.nets.beam_search - INFO - total log probability: -49.27
2025-04-17 12:52:18,159 - espnet.nets.beam_search - INFO - normalized log probability: -4.93
2025-04-17 12:52:18,159 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 25
2025-04-17 12:52:18,160 - espnet.nets.beam_search - INFO - best hypo: ▁HELLO▁I▁DON'T▁KNOW

2025-04-17 12:52:18,160 - asr_multi - ERROR - ASR error Not found: "data/token_list/bpe_unigram5000/model.model": No such file or directory Error #2
Traceback (most recent call last):
  File "/home/ambuja/emo-tts/src/gradio_app.py", line 178, in transcribe_and_speak
    nbests = model(wav)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet2/bin/asr_inference.py", line 559, in __call__
    results = self._decode_single_sample(enc[0])
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet2/bin/asr_inference.py", line 676, in _decode_single_sample
    text = self.tokenizer.tokens2text(token)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet2/text/sentencepiece_tokenizer.py", line 36, in tokens2text
    self._build_sentence_piece_processor()
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet2/text/sentencepiece_tokenizer.py", line 29, in _build_sentence_piece_processor
    self.sp.load(self.model)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/sentencepiece/__init__.py", line 961, in Load
    return self.LoadFromFile(model_file)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/sentencepiece/__init__.py", line 316, in LoadFromFile
    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)
OSError: Not found: "data/token_list/bpe_unigram5000/model.model": No such file or directory Error #2
2025-04-17 12:55:54,589 - asr_multi - INFO - Starting multi-model ASR/TTS demo
2025-04-17 12:55:54,590 - asr_multi - INFO - Using device: cuda
2025-04-17 12:55:54,599 - asr_multi - INFO - Downloading ASR model 'Conformer6' → kamo-naoyuki/librispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp_valid.acc.ave
2025-04-17 12:56:05,521 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp/config.yaml
2025-04-17 12:56:06,367 - root - INFO - Vocabulary size: 5000
2025-04-17 12:56:06,441 - root - WARNING - Using legacy_rel_pos and it will be deprecated in the future.
2025-04-17 12:56:06,501 - root - WARNING - Using legacy_rel_selfattn and it will be deprecated in the future.
2025-04-17 12:56:06,980 - root - INFO - Gradient checkpoint layers: []
2025-04-17 12:56:08,411 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/lm_train_lm_transformer2_bpe5000_scheduler_confwarmup_steps25000_batch_bins500000000_accum_grad2_use_amptrue/config.yaml
2025-04-17 12:56:09,239 - root - INFO - Vocabulary size: 5000
2025-04-17 12:56:09,243 - root - INFO - encoder self-attention layer type = self-attention
2025-04-17 12:56:09,899 - espnet2.bin.asr_inference - INFO - BatchBeamSearch implementation is selected.
2025-04-17 12:56:09,930 - espnet2.bin.asr_inference - INFO - Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (lm): TransformerLM(
      (embed): Embedding(5000, 128)
      (encoder): Encoder(
        (embed): Sequential(
          (0): Linear(in_features=128, out_features=512, bias=True)
          (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): Dropout(p=0.0, inplace=False)
          (3): ReLU()
          (4): Sequential()
        )
        (encoders): MultiSequential(
          (0): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (4): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (5): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (6): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (7): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (8): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (9): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (10): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (11): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (12): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (13): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (14): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (15): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): Linear(in_features=512, out_features=5000, bias=True)
    )
  )
)
2025-04-17 12:56:09,931 - espnet2.bin.asr_inference - INFO - Decoding device=cuda, dtype=float32
2025-04-17 12:56:09,958 - espnet2.bin.asr_inference - INFO - Text tokenizer: SentencepiecesTokenizer(model="/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/data/en_token_list/bpe_unigram5000/bpe.model")
2025-04-17 12:56:09,959 - asr_multi - INFO - Downloading ASR model 'TransformerE18' → espnet/shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best
2025-04-17 12:56:10,039 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--espnet--shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best/snapshots/bc6bbd771cec698f070640ee677a66719181f0a2/exp/asr_train_asr_transformer_e18_raw_bpe_sp/config.yaml
2025-04-17 12:56:10,888 - root - INFO - Vocabulary size: 5000
2025-04-17 12:56:11,214 - root - INFO - Gradient checkpoint layers: []
2025-04-17 12:56:11,786 - root - INFO - Initialize encoder.embed.conv.0.bias to zeros
2025-04-17 12:56:11,786 - root - INFO - Initialize encoder.embed.conv.2.bias to zeros
2025-04-17 12:56:11,789 - root - INFO - Initialize encoder.embed.out.0.bias to zeros
2025-04-17 12:56:11,790 - root - INFO - Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
2025-04-17 12:56:11,790 - root - INFO - Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
2025-04-17 12:56:11,791 - root - INFO - Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
2025-04-17 12:56:11,791 - root - INFO - Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
2025-04-17 12:56:11,791 - root - INFO - Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
2025-04-17 12:56:11,792 - root - INFO - Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
2025-04-17 12:56:11,792 - root - INFO - Initialize encoder.encoders.0.norm1.bias to zeros
2025-04-17 12:56:11,792 - root - INFO - Initialize encoder.encoders.0.norm2.bias to zeros
2025-04-17 12:56:11,793 - root - INFO - Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
2025-04-17 12:56:11,793 - root - INFO - Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
2025-04-17 12:56:11,794 - root - INFO - Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
2025-04-17 12:56:11,794 - root - INFO - Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
2025-04-17 12:56:11,794 - root - INFO - Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
2025-04-17 12:56:11,794 - root - INFO - Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
2025-04-17 12:56:11,795 - root - INFO - Initialize encoder.encoders.1.norm1.bias to zeros
2025-04-17 12:56:11,795 - root - INFO - Initialize encoder.encoders.1.norm2.bias to zeros
2025-04-17 12:56:11,795 - root - INFO - Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
2025-04-17 12:56:11,796 - root - INFO - Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
2025-04-17 12:56:11,796 - root - INFO - Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
2025-04-17 12:56:11,796 - root - INFO - Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
2025-04-17 12:56:11,797 - root - INFO - Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
2025-04-17 12:56:11,797 - root - INFO - Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
2025-04-17 12:56:11,798 - root - INFO - Initialize encoder.encoders.2.norm1.bias to zeros
2025-04-17 12:56:11,798 - root - INFO - Initialize encoder.encoders.2.norm2.bias to zeros
2025-04-17 12:56:11,798 - root - INFO - Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
2025-04-17 12:56:11,798 - root - INFO - Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
2025-04-17 12:56:11,799 - root - INFO - Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
2025-04-17 12:56:11,799 - root - INFO - Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
2025-04-17 12:56:11,799 - root - INFO - Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
2025-04-17 12:56:11,800 - root - INFO - Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
2025-04-17 12:56:11,800 - root - INFO - Initialize encoder.encoders.3.norm1.bias to zeros
2025-04-17 12:56:11,800 - root - INFO - Initialize encoder.encoders.3.norm2.bias to zeros
2025-04-17 12:56:11,801 - root - INFO - Initialize encoder.encoders.4.self_attn.linear_q.bias to zeros
2025-04-17 12:56:11,801 - root - INFO - Initialize encoder.encoders.4.self_attn.linear_k.bias to zeros
2025-04-17 12:56:11,801 - root - INFO - Initialize encoder.encoders.4.self_attn.linear_v.bias to zeros
2025-04-17 12:56:11,802 - root - INFO - Initialize encoder.encoders.4.self_attn.linear_out.bias to zeros
2025-04-17 12:56:11,802 - root - INFO - Initialize encoder.encoders.4.feed_forward.w_1.bias to zeros
2025-04-17 12:56:11,802 - root - INFO - Initialize encoder.encoders.4.feed_forward.w_2.bias to zeros
2025-04-17 12:56:11,803 - root - INFO - Initialize encoder.encoders.4.norm1.bias to zeros
2025-04-17 12:56:11,803 - root - INFO - Initialize encoder.encoders.4.norm2.bias to zeros
2025-04-17 12:56:11,803 - root - INFO - Initialize encoder.encoders.5.self_attn.linear_q.bias to zeros
2025-04-17 12:56:11,804 - root - INFO - Initialize encoder.encoders.5.self_attn.linear_k.bias to zeros
2025-04-17 12:56:11,804 - root - INFO - Initialize encoder.encoders.5.self_attn.linear_v.bias to zeros
2025-04-17 12:56:11,804 - root - INFO - Initialize encoder.encoders.5.self_attn.linear_out.bias to zeros
2025-04-17 12:56:11,805 - root - INFO - Initialize encoder.encoders.5.feed_forward.w_1.bias to zeros
2025-04-17 12:56:11,805 - root - INFO - Initialize encoder.encoders.5.feed_forward.w_2.bias to zeros
2025-04-17 12:56:11,805 - root - INFO - Initialize encoder.encoders.5.norm1.bias to zeros
2025-04-17 12:56:11,806 - root - INFO - Initialize encoder.encoders.5.norm2.bias to zeros
2025-04-17 12:56:11,806 - root - INFO - Initialize encoder.encoders.6.self_attn.linear_q.bias to zeros
2025-04-17 12:56:11,806 - root - INFO - Initialize encoder.encoders.6.self_attn.linear_k.bias to zeros
2025-04-17 12:56:11,807 - root - INFO - Initialize encoder.encoders.6.self_attn.linear_v.bias to zeros
2025-04-17 12:56:11,807 - root - INFO - Initialize encoder.encoders.6.self_attn.linear_out.bias to zeros
2025-04-17 12:56:11,807 - root - INFO - Initialize encoder.encoders.6.feed_forward.w_1.bias to zeros
2025-04-17 12:56:11,808 - root - INFO - Initialize encoder.encoders.6.feed_forward.w_2.bias to zeros
2025-04-17 12:56:11,808 - root - INFO - Initialize encoder.encoders.6.norm1.bias to zeros
2025-04-17 12:56:11,808 - root - INFO - Initialize encoder.encoders.6.norm2.bias to zeros
2025-04-17 12:56:11,809 - root - INFO - Initialize encoder.encoders.7.self_attn.linear_q.bias to zeros
2025-04-17 12:56:11,809 - root - INFO - Initialize encoder.encoders.7.self_attn.linear_k.bias to zeros
2025-04-17 12:56:11,809 - root - INFO - Initialize encoder.encoders.7.self_attn.linear_v.bias to zeros
2025-04-17 12:56:11,809 - root - INFO - Initialize encoder.encoders.7.self_attn.linear_out.bias to zeros
2025-04-17 12:56:11,810 - root - INFO - Initialize encoder.encoders.7.feed_forward.w_1.bias to zeros
2025-04-17 12:56:11,810 - root - INFO - Initialize encoder.encoders.7.feed_forward.w_2.bias to zeros
2025-04-17 12:56:11,810 - root - INFO - Initialize encoder.encoders.7.norm1.bias to zeros
2025-04-17 12:56:11,811 - root - INFO - Initialize encoder.encoders.7.norm2.bias to zeros
2025-04-17 12:56:11,811 - root - INFO - Initialize encoder.encoders.8.self_attn.linear_q.bias to zeros
2025-04-17 12:56:11,811 - root - INFO - Initialize encoder.encoders.8.self_attn.linear_k.bias to zeros
2025-04-17 12:56:11,812 - root - INFO - Initialize encoder.encoders.8.self_attn.linear_v.bias to zeros
2025-04-17 12:56:11,812 - root - INFO - Initialize encoder.encoders.8.self_attn.linear_out.bias to zeros
2025-04-17 12:56:11,813 - root - INFO - Initialize encoder.encoders.8.feed_forward.w_1.bias to zeros
2025-04-17 12:56:11,813 - root - INFO - Initialize encoder.encoders.8.feed_forward.w_2.bias to zeros
2025-04-17 12:56:11,813 - root - INFO - Initialize encoder.encoders.8.norm1.bias to zeros
2025-04-17 12:56:11,813 - root - INFO - Initialize encoder.encoders.8.norm2.bias to zeros
2025-04-17 12:56:11,814 - root - INFO - Initialize encoder.encoders.9.self_attn.linear_q.bias to zeros
2025-04-17 12:56:11,814 - root - INFO - Initialize encoder.encoders.9.self_attn.linear_k.bias to zeros
2025-04-17 12:56:11,814 - root - INFO - Initialize encoder.encoders.9.self_attn.linear_v.bias to zeros
2025-04-17 12:56:11,815 - root - INFO - Initialize encoder.encoders.9.self_attn.linear_out.bias to zeros
2025-04-17 12:56:11,815 - root - INFO - Initialize encoder.encoders.9.feed_forward.w_1.bias to zeros
2025-04-17 12:56:11,815 - root - INFO - Initialize encoder.encoders.9.feed_forward.w_2.bias to zeros
2025-04-17 12:56:11,816 - root - INFO - Initialize encoder.encoders.9.norm1.bias to zeros
2025-04-17 12:56:11,816 - root - INFO - Initialize encoder.encoders.9.norm2.bias to zeros
2025-04-17 12:56:11,816 - root - INFO - Initialize encoder.encoders.10.self_attn.linear_q.bias to zeros
2025-04-17 12:56:11,817 - root - INFO - Initialize encoder.encoders.10.self_attn.linear_k.bias to zeros
2025-04-17 12:56:11,817 - root - INFO - Initialize encoder.encoders.10.self_attn.linear_v.bias to zeros
2025-04-17 12:56:11,817 - root - INFO - Initialize encoder.encoders.10.self_attn.linear_out.bias to zeros
2025-04-17 12:56:11,818 - root - INFO - Initialize encoder.encoders.10.feed_forward.w_1.bias to zeros
2025-04-17 12:56:11,818 - root - INFO - Initialize encoder.encoders.10.feed_forward.w_2.bias to zeros
2025-04-17 12:56:11,818 - root - INFO - Initialize encoder.encoders.10.norm1.bias to zeros
2025-04-17 12:56:11,819 - root - INFO - Initialize encoder.encoders.10.norm2.bias to zeros
2025-04-17 12:56:11,819 - root - INFO - Initialize encoder.encoders.11.self_attn.linear_q.bias to zeros
2025-04-17 12:56:11,819 - root - INFO - Initialize encoder.encoders.11.self_attn.linear_k.bias to zeros
2025-04-17 12:56:11,819 - root - INFO - Initialize encoder.encoders.11.self_attn.linear_v.bias to zeros
2025-04-17 12:56:11,820 - root - INFO - Initialize encoder.encoders.11.self_attn.linear_out.bias to zeros
2025-04-17 12:56:11,820 - root - INFO - Initialize encoder.encoders.11.feed_forward.w_1.bias to zeros
2025-04-17 12:56:11,820 - root - INFO - Initialize encoder.encoders.11.feed_forward.w_2.bias to zeros
2025-04-17 12:56:11,821 - root - INFO - Initialize encoder.encoders.11.norm1.bias to zeros
2025-04-17 12:56:11,821 - root - INFO - Initialize encoder.encoders.11.norm2.bias to zeros
2025-04-17 12:56:11,822 - root - INFO - Initialize encoder.encoders.12.self_attn.linear_q.bias to zeros
2025-04-17 12:56:11,822 - root - INFO - Initialize encoder.encoders.12.self_attn.linear_k.bias to zeros
2025-04-17 12:56:11,822 - root - INFO - Initialize encoder.encoders.12.self_attn.linear_v.bias to zeros
2025-04-17 12:56:11,822 - root - INFO - Initialize encoder.encoders.12.self_attn.linear_out.bias to zeros
2025-04-17 12:56:11,823 - root - INFO - Initialize encoder.encoders.12.feed_forward.w_1.bias to zeros
2025-04-17 12:56:11,823 - root - INFO - Initialize encoder.encoders.12.feed_forward.w_2.bias to zeros
2025-04-17 12:56:11,823 - root - INFO - Initialize encoder.encoders.12.norm1.bias to zeros
2025-04-17 12:56:11,824 - root - INFO - Initialize encoder.encoders.12.norm2.bias to zeros
2025-04-17 12:56:11,824 - root - INFO - Initialize encoder.encoders.13.self_attn.linear_q.bias to zeros
2025-04-17 12:56:11,824 - root - INFO - Initialize encoder.encoders.13.self_attn.linear_k.bias to zeros
2025-04-17 12:56:11,825 - root - INFO - Initialize encoder.encoders.13.self_attn.linear_v.bias to zeros
2025-04-17 12:56:11,825 - root - INFO - Initialize encoder.encoders.13.self_attn.linear_out.bias to zeros
2025-04-17 12:56:11,825 - root - INFO - Initialize encoder.encoders.13.feed_forward.w_1.bias to zeros
2025-04-17 12:56:11,826 - root - INFO - Initialize encoder.encoders.13.feed_forward.w_2.bias to zeros
2025-04-17 12:56:11,826 - root - INFO - Initialize encoder.encoders.13.norm1.bias to zeros
2025-04-17 12:56:11,826 - root - INFO - Initialize encoder.encoders.13.norm2.bias to zeros
2025-04-17 12:56:11,827 - root - INFO - Initialize encoder.encoders.14.self_attn.linear_q.bias to zeros
2025-04-17 12:56:11,827 - root - INFO - Initialize encoder.encoders.14.self_attn.linear_k.bias to zeros
2025-04-17 12:56:11,827 - root - INFO - Initialize encoder.encoders.14.self_attn.linear_v.bias to zeros
2025-04-17 12:56:11,828 - root - INFO - Initialize encoder.encoders.14.self_attn.linear_out.bias to zeros
2025-04-17 12:56:11,828 - root - INFO - Initialize encoder.encoders.14.feed_forward.w_1.bias to zeros
2025-04-17 12:56:11,828 - root - INFO - Initialize encoder.encoders.14.feed_forward.w_2.bias to zeros
2025-04-17 12:56:11,829 - root - INFO - Initialize encoder.encoders.14.norm1.bias to zeros
2025-04-17 12:56:11,829 - root - INFO - Initialize encoder.encoders.14.norm2.bias to zeros
2025-04-17 12:56:11,829 - root - INFO - Initialize encoder.encoders.15.self_attn.linear_q.bias to zeros
2025-04-17 12:56:11,829 - root - INFO - Initialize encoder.encoders.15.self_attn.linear_k.bias to zeros
2025-04-17 12:56:11,830 - root - INFO - Initialize encoder.encoders.15.self_attn.linear_v.bias to zeros
2025-04-17 12:56:11,830 - root - INFO - Initialize encoder.encoders.15.self_attn.linear_out.bias to zeros
2025-04-17 12:56:11,830 - root - INFO - Initialize encoder.encoders.15.feed_forward.w_1.bias to zeros
2025-04-17 12:56:11,831 - root - INFO - Initialize encoder.encoders.15.feed_forward.w_2.bias to zeros
2025-04-17 12:56:11,832 - root - INFO - Initialize encoder.encoders.15.norm1.bias to zeros
2025-04-17 12:56:11,832 - root - INFO - Initialize encoder.encoders.15.norm2.bias to zeros
2025-04-17 12:56:11,832 - root - INFO - Initialize encoder.encoders.16.self_attn.linear_q.bias to zeros
2025-04-17 12:56:11,833 - root - INFO - Initialize encoder.encoders.16.self_attn.linear_k.bias to zeros
2025-04-17 12:56:11,833 - root - INFO - Initialize encoder.encoders.16.self_attn.linear_v.bias to zeros
2025-04-17 12:56:11,833 - root - INFO - Initialize encoder.encoders.16.self_attn.linear_out.bias to zeros
2025-04-17 12:56:11,834 - root - INFO - Initialize encoder.encoders.16.feed_forward.w_1.bias to zeros
2025-04-17 12:56:11,834 - root - INFO - Initialize encoder.encoders.16.feed_forward.w_2.bias to zeros
2025-04-17 12:56:11,834 - root - INFO - Initialize encoder.encoders.16.norm1.bias to zeros
2025-04-17 12:56:11,835 - root - INFO - Initialize encoder.encoders.16.norm2.bias to zeros
2025-04-17 12:56:11,835 - root - INFO - Initialize encoder.encoders.17.self_attn.linear_q.bias to zeros
2025-04-17 12:56:11,835 - root - INFO - Initialize encoder.encoders.17.self_attn.linear_k.bias to zeros
2025-04-17 12:56:11,835 - root - INFO - Initialize encoder.encoders.17.self_attn.linear_v.bias to zeros
2025-04-17 12:56:11,836 - root - INFO - Initialize encoder.encoders.17.self_attn.linear_out.bias to zeros
2025-04-17 12:56:11,836 - root - INFO - Initialize encoder.encoders.17.feed_forward.w_1.bias to zeros
2025-04-17 12:56:11,836 - root - INFO - Initialize encoder.encoders.17.feed_forward.w_2.bias to zeros
2025-04-17 12:56:11,837 - root - INFO - Initialize encoder.encoders.17.norm1.bias to zeros
2025-04-17 12:56:11,837 - root - INFO - Initialize encoder.encoders.17.norm2.bias to zeros
2025-04-17 12:56:11,837 - root - INFO - Initialize encoder.after_norm.bias to zeros
2025-04-17 12:56:11,838 - root - INFO - Initialize decoder.after_norm.bias to zeros
2025-04-17 12:56:11,839 - root - INFO - Initialize decoder.output_layer.bias to zeros
2025-04-17 12:56:11,839 - root - INFO - Initialize decoder.decoders.0.self_attn.linear_q.bias to zeros
2025-04-17 12:56:11,839 - root - INFO - Initialize decoder.decoders.0.self_attn.linear_k.bias to zeros
2025-04-17 12:56:11,839 - root - INFO - Initialize decoder.decoders.0.self_attn.linear_v.bias to zeros
2025-04-17 12:56:11,840 - root - INFO - Initialize decoder.decoders.0.self_attn.linear_out.bias to zeros
2025-04-17 12:56:11,840 - root - INFO - Initialize decoder.decoders.0.src_attn.linear_q.bias to zeros
2025-04-17 12:56:11,840 - root - INFO - Initialize decoder.decoders.0.src_attn.linear_k.bias to zeros
2025-04-17 12:56:11,841 - root - INFO - Initialize decoder.decoders.0.src_attn.linear_v.bias to zeros
2025-04-17 12:56:11,841 - root - INFO - Initialize decoder.decoders.0.src_attn.linear_out.bias to zeros
2025-04-17 12:56:11,842 - root - INFO - Initialize decoder.decoders.0.feed_forward.w_1.bias to zeros
2025-04-17 12:56:11,842 - root - INFO - Initialize decoder.decoders.0.feed_forward.w_2.bias to zeros
2025-04-17 12:56:11,842 - root - INFO - Initialize decoder.decoders.0.norm1.bias to zeros
2025-04-17 12:56:11,843 - root - INFO - Initialize decoder.decoders.0.norm2.bias to zeros
2025-04-17 12:56:11,843 - root - INFO - Initialize decoder.decoders.0.norm3.bias to zeros
2025-04-17 12:56:11,843 - root - INFO - Initialize decoder.decoders.1.self_attn.linear_q.bias to zeros
2025-04-17 12:56:11,844 - root - INFO - Initialize decoder.decoders.1.self_attn.linear_k.bias to zeros
2025-04-17 12:56:11,844 - root - INFO - Initialize decoder.decoders.1.self_attn.linear_v.bias to zeros
2025-04-17 12:56:11,844 - root - INFO - Initialize decoder.decoders.1.self_attn.linear_out.bias to zeros
2025-04-17 12:56:11,845 - root - INFO - Initialize decoder.decoders.1.src_attn.linear_q.bias to zeros
2025-04-17 12:56:11,845 - root - INFO - Initialize decoder.decoders.1.src_attn.linear_k.bias to zeros
2025-04-17 12:56:11,845 - root - INFO - Initialize decoder.decoders.1.src_attn.linear_v.bias to zeros
2025-04-17 12:56:11,845 - root - INFO - Initialize decoder.decoders.1.src_attn.linear_out.bias to zeros
2025-04-17 12:56:11,846 - root - INFO - Initialize decoder.decoders.1.feed_forward.w_1.bias to zeros
2025-04-17 12:56:11,846 - root - INFO - Initialize decoder.decoders.1.feed_forward.w_2.bias to zeros
2025-04-17 12:56:11,846 - root - INFO - Initialize decoder.decoders.1.norm1.bias to zeros
2025-04-17 12:56:11,847 - root - INFO - Initialize decoder.decoders.1.norm2.bias to zeros
2025-04-17 12:56:11,847 - root - INFO - Initialize decoder.decoders.1.norm3.bias to zeros
2025-04-17 12:56:11,847 - root - INFO - Initialize decoder.decoders.2.self_attn.linear_q.bias to zeros
2025-04-17 12:56:11,848 - root - INFO - Initialize decoder.decoders.2.self_attn.linear_k.bias to zeros
2025-04-17 12:56:11,848 - root - INFO - Initialize decoder.decoders.2.self_attn.linear_v.bias to zeros
2025-04-17 12:56:11,848 - root - INFO - Initialize decoder.decoders.2.self_attn.linear_out.bias to zeros
2025-04-17 12:56:11,849 - root - INFO - Initialize decoder.decoders.2.src_attn.linear_q.bias to zeros
2025-04-17 12:56:11,849 - root - INFO - Initialize decoder.decoders.2.src_attn.linear_k.bias to zeros
2025-04-17 12:56:11,849 - root - INFO - Initialize decoder.decoders.2.src_attn.linear_v.bias to zeros
2025-04-17 12:56:11,850 - root - INFO - Initialize decoder.decoders.2.src_attn.linear_out.bias to zeros
2025-04-17 12:56:11,850 - root - INFO - Initialize decoder.decoders.2.feed_forward.w_1.bias to zeros
2025-04-17 12:56:11,850 - root - INFO - Initialize decoder.decoders.2.feed_forward.w_2.bias to zeros
2025-04-17 12:56:11,850 - root - INFO - Initialize decoder.decoders.2.norm1.bias to zeros
2025-04-17 12:56:11,851 - root - INFO - Initialize decoder.decoders.2.norm2.bias to zeros
2025-04-17 12:56:11,851 - root - INFO - Initialize decoder.decoders.2.norm3.bias to zeros
2025-04-17 12:56:11,851 - root - INFO - Initialize decoder.decoders.3.self_attn.linear_q.bias to zeros
2025-04-17 12:56:11,852 - root - INFO - Initialize decoder.decoders.3.self_attn.linear_k.bias to zeros
2025-04-17 12:56:11,852 - root - INFO - Initialize decoder.decoders.3.self_attn.linear_v.bias to zeros
2025-04-17 12:56:11,852 - root - INFO - Initialize decoder.decoders.3.self_attn.linear_out.bias to zeros
2025-04-17 12:56:11,853 - root - INFO - Initialize decoder.decoders.3.src_attn.linear_q.bias to zeros
2025-04-17 12:56:11,853 - root - INFO - Initialize decoder.decoders.3.src_attn.linear_k.bias to zeros
2025-04-17 12:56:11,853 - root - INFO - Initialize decoder.decoders.3.src_attn.linear_v.bias to zeros
2025-04-17 12:56:11,854 - root - INFO - Initialize decoder.decoders.3.src_attn.linear_out.bias to zeros
2025-04-17 12:56:11,854 - root - INFO - Initialize decoder.decoders.3.feed_forward.w_1.bias to zeros
2025-04-17 12:56:11,854 - root - INFO - Initialize decoder.decoders.3.feed_forward.w_2.bias to zeros
2025-04-17 12:56:11,855 - root - INFO - Initialize decoder.decoders.3.norm1.bias to zeros
2025-04-17 12:56:11,855 - root - INFO - Initialize decoder.decoders.3.norm2.bias to zeros
2025-04-17 12:56:11,855 - root - INFO - Initialize decoder.decoders.3.norm3.bias to zeros
2025-04-17 12:56:11,856 - root - INFO - Initialize decoder.decoders.4.self_attn.linear_q.bias to zeros
2025-04-17 12:56:11,856 - root - INFO - Initialize decoder.decoders.4.self_attn.linear_k.bias to zeros
2025-04-17 12:56:11,856 - root - INFO - Initialize decoder.decoders.4.self_attn.linear_v.bias to zeros
2025-04-17 12:56:11,857 - root - INFO - Initialize decoder.decoders.4.self_attn.linear_out.bias to zeros
2025-04-17 12:56:11,857 - root - INFO - Initialize decoder.decoders.4.src_attn.linear_q.bias to zeros
2025-04-17 12:56:11,857 - root - INFO - Initialize decoder.decoders.4.src_attn.linear_k.bias to zeros
2025-04-17 12:56:11,857 - root - INFO - Initialize decoder.decoders.4.src_attn.linear_v.bias to zeros
2025-04-17 12:56:11,858 - root - INFO - Initialize decoder.decoders.4.src_attn.linear_out.bias to zeros
2025-04-17 12:56:11,858 - root - INFO - Initialize decoder.decoders.4.feed_forward.w_1.bias to zeros
2025-04-17 12:56:11,859 - root - INFO - Initialize decoder.decoders.4.feed_forward.w_2.bias to zeros
2025-04-17 12:56:11,859 - root - INFO - Initialize decoder.decoders.4.norm1.bias to zeros
2025-04-17 12:56:11,859 - root - INFO - Initialize decoder.decoders.4.norm2.bias to zeros
2025-04-17 12:56:11,859 - root - INFO - Initialize decoder.decoders.4.norm3.bias to zeros
2025-04-17 12:56:11,860 - root - INFO - Initialize decoder.decoders.5.self_attn.linear_q.bias to zeros
2025-04-17 12:56:11,860 - root - INFO - Initialize decoder.decoders.5.self_attn.linear_k.bias to zeros
2025-04-17 12:56:11,860 - root - INFO - Initialize decoder.decoders.5.self_attn.linear_v.bias to zeros
2025-04-17 12:56:11,861 - root - INFO - Initialize decoder.decoders.5.self_attn.linear_out.bias to zeros
2025-04-17 12:56:11,861 - root - INFO - Initialize decoder.decoders.5.src_attn.linear_q.bias to zeros
2025-04-17 12:56:11,861 - root - INFO - Initialize decoder.decoders.5.src_attn.linear_k.bias to zeros
2025-04-17 12:56:11,862 - root - INFO - Initialize decoder.decoders.5.src_attn.linear_v.bias to zeros
2025-04-17 12:56:11,862 - root - INFO - Initialize decoder.decoders.5.src_attn.linear_out.bias to zeros
2025-04-17 12:56:11,862 - root - INFO - Initialize decoder.decoders.5.feed_forward.w_1.bias to zeros
2025-04-17 12:56:11,863 - root - INFO - Initialize decoder.decoders.5.feed_forward.w_2.bias to zeros
2025-04-17 12:56:11,863 - root - INFO - Initialize decoder.decoders.5.norm1.bias to zeros
2025-04-17 12:56:11,863 - root - INFO - Initialize decoder.decoders.5.norm2.bias to zeros
2025-04-17 12:56:11,864 - root - INFO - Initialize decoder.decoders.5.norm3.bias to zeros
2025-04-17 12:56:11,864 - root - INFO - Initialize ctc.ctc_lo.bias to zeros
2025-04-17 12:56:12,272 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--espnet--shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best/snapshots/bc6bbd771cec698f070640ee677a66719181f0a2/exp/lm_train_lm_adam_bpe/config.yaml
2025-04-17 12:56:13,113 - root - INFO - Vocabulary size: 5000
2025-04-17 12:56:14,003 - espnet2.bin.asr_inference - INFO - BatchBeamSearch implementation is selected.
2025-04-17 12:56:14,019 - espnet2.bin.asr_inference - INFO - Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (lm): SequentialRNNLM(
      (drop): Dropout(p=0.0, inplace=False)
      (encoder): Embedding(5000, 2048, padding_idx=0)
      (rnn): LSTM(2048, 2048, num_layers=4, batch_first=True)
      (decoder): Linear(in_features=2048, out_features=5000, bias=True)
    )
  )
)
2025-04-17 12:56:14,019 - espnet2.bin.asr_inference - INFO - Decoding device=cuda, dtype=float32
2025-04-17 12:56:14,022 - espnet2.bin.asr_inference - INFO - Text tokenizer: SentencepiecesTokenizer(model="data/token_list/bpe_unigram5000/model.model")
2025-04-17 12:56:14,023 - asr_multi - INFO - Loaded ASR models: ['Conformer6', 'TransformerE18']
2025-04-17 12:56:14,148 - asr_multi - INFO - Launching Gradio app…
2025-04-17 12:57:43,554 - asr_multi - INFO - Audio in: sr=48000, length=2.88 s, peak=1966634624.00
2025-04-17 12:57:43,555 - asr_multi - INFO - Normalized (peak=1966634624.00)
2025-04-17 12:57:45,026 - asr_multi - INFO - Saved debug_samples/input_20250417_125745.wav (2.88 s)
2025-04-17 12:57:45,027 - espnet2.bin.asr_inference - INFO - speech length: 46080
2025-04-17 12:57:45,341 - espnet.nets.beam_search - INFO - decoder input length: 44
2025-04-17 12:57:45,341 - espnet.nets.beam_search - INFO - max output length: 44
2025-04-17 12:57:45,342 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 12:57:45,785 - espnet.nets.beam_search - INFO - end detected at 11
2025-04-17 12:57:45,785 - espnet.nets.beam_search - INFO -  -9.50 * 0.7 =  -6.65 for decoder
2025-04-17 12:57:45,786 - espnet.nets.beam_search - INFO - -20.73 * 0.3 =  -6.22 for ctc
2025-04-17 12:57:45,786 - espnet.nets.beam_search - INFO - -27.37 * 1.0 = -27.37 for lm
2025-04-17 12:57:45,786 - espnet.nets.beam_search - INFO - total log probability: -40.24
2025-04-17 12:57:45,787 - espnet.nets.beam_search - INFO - normalized log probability: -5.75
2025-04-17 12:57:45,787 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 26
2025-04-17 12:57:45,787 - espnet.nets.beam_search - INFO - best hypo: ▁ANY▁KIND▁BETTY

2025-04-17 12:57:45,795 - asr_multi - INFO - Recognized: ANY KIND BETTY
2025-04-17 12:57:51,859 - asr_multi - INFO - Audio in: sr=48000, length=2.88 s, peak=1966634624.00
2025-04-17 12:57:51,859 - asr_multi - INFO - Normalized (peak=1966634624.00)
2025-04-17 12:57:51,864 - asr_multi - INFO - Saved debug_samples/input_20250417_125751.wav (2.88 s)
2025-04-17 12:57:51,864 - espnet2.bin.asr_inference - INFO - speech length: 46080
2025-04-17 12:57:51,892 - espnet.nets.beam_search - INFO - decoder input length: 59
2025-04-17 12:57:51,892 - espnet.nets.beam_search - INFO - max output length: 59
2025-04-17 12:57:51,893 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 12:57:52,163 - espnet.nets.beam_search - INFO - end detected at 11
2025-04-17 12:57:52,164 - espnet.nets.beam_search - INFO -  -7.93 * 0.7 =  -5.55 for decoder
2025-04-17 12:57:52,164 - espnet.nets.beam_search - INFO - -11.77 * 0.3 =  -3.53 for ctc
2025-04-17 12:57:52,165 - espnet.nets.beam_search - INFO - -15.30 * 1.0 = -15.30 for lm
2025-04-17 12:57:52,165 - espnet.nets.beam_search - INFO - total log probability: -24.39
2025-04-17 12:57:52,165 - espnet.nets.beam_search - INFO - normalized log probability: -4.06
2025-04-17 12:57:52,166 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 25
2025-04-17 12:57:52,166 - espnet.nets.beam_search - INFO - best hypo: ▁YOU▁ARE▁VERY▁GOOD

2025-04-17 12:57:52,167 - asr_multi - ERROR - ASR error Not found: "data/token_list/bpe_unigram5000/model.model": No such file or directory Error #2
Traceback (most recent call last):
  File "/home/ambuja/emo-tts/src/gradio_app.py", line 194, in transcribe_and_speak
    nbests = model(wav)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet2/bin/asr_inference.py", line 559, in __call__
    results = self._decode_single_sample(enc[0])
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet2/bin/asr_inference.py", line 676, in _decode_single_sample
    text = self.tokenizer.tokens2text(token)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet2/text/sentencepiece_tokenizer.py", line 36, in tokens2text
    self._build_sentence_piece_processor()
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet2/text/sentencepiece_tokenizer.py", line 29, in _build_sentence_piece_processor
    self.sp.load(self.model)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/sentencepiece/__init__.py", line 961, in Load
    return self.LoadFromFile(model_file)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/sentencepiece/__init__.py", line 316, in LoadFromFile
    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)
OSError: Not found: "data/token_list/bpe_unigram5000/model.model": No such file or directory Error #2
2025-04-17 12:59:09,462 - asr_multi - INFO - Starting multi-model ASR/TTS demo
2025-04-17 12:59:09,463 - asr_multi - INFO - Using device: cuda
2025-04-17 12:59:09,470 - asr_multi - INFO - Downloading ASR model 'Conformer6' → kamo-naoyuki/librispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp_valid.acc.ave
2025-04-17 12:59:20,337 - asr_multi - INFO - Model directory for Conformer6: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp
2025-04-17 12:59:20,338 - asr_multi - INFO -   asr_train_config: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp/config.yaml
2025-04-17 12:59:20,338 - asr_multi - INFO -   lm_train_config: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/lm_train_lm_transformer2_bpe5000_scheduler_confwarmup_steps25000_batch_bins500000000_accum_grad2_use_amptrue/config.yaml
2025-04-17 12:59:20,338 - asr_multi - INFO -   asr_model_file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp/valid.acc.ave_10best.pth
2025-04-17 12:59:20,338 - asr_multi - INFO -   lm_file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/lm_train_lm_transformer2_bpe5000_scheduler_confwarmup_steps25000_batch_bins500000000_accum_grad2_use_amptrue/valid.loss.ave_10best.pth
2025-04-17 12:59:20,341 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp/config.yaml
2025-04-17 12:59:21,198 - root - INFO - Vocabulary size: 5000
2025-04-17 12:59:21,273 - root - WARNING - Using legacy_rel_pos and it will be deprecated in the future.
2025-04-17 12:59:21,333 - root - WARNING - Using legacy_rel_selfattn and it will be deprecated in the future.
2025-04-17 12:59:21,811 - root - INFO - Gradient checkpoint layers: []
2025-04-17 12:59:23,234 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/lm_train_lm_transformer2_bpe5000_scheduler_confwarmup_steps25000_batch_bins500000000_accum_grad2_use_amptrue/config.yaml
2025-04-17 12:59:24,066 - root - INFO - Vocabulary size: 5000
2025-04-17 12:59:24,070 - root - INFO - encoder self-attention layer type = self-attention
2025-04-17 12:59:24,460 - espnet2.bin.asr_inference - INFO - BatchBeamSearch implementation is selected.
2025-04-17 12:59:24,490 - espnet2.bin.asr_inference - INFO - Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (lm): TransformerLM(
      (embed): Embedding(5000, 128)
      (encoder): Encoder(
        (embed): Sequential(
          (0): Linear(in_features=128, out_features=512, bias=True)
          (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): Dropout(p=0.0, inplace=False)
          (3): ReLU()
          (4): Sequential()
        )
        (encoders): MultiSequential(
          (0): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (4): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (5): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (6): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (7): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (8): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (9): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (10): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (11): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (12): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (13): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (14): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (15): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): Linear(in_features=512, out_features=5000, bias=True)
    )
  )
)
2025-04-17 12:59:24,492 - espnet2.bin.asr_inference - INFO - Decoding device=cuda, dtype=float32
2025-04-17 12:59:24,495 - espnet2.bin.asr_inference - INFO - Text tokenizer: SentencepiecesTokenizer(model="/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/data/en_token_list/bpe_unigram5000/bpe.model")
2025-04-17 12:59:24,497 - asr_multi - INFO - Successfully loaded model: Conformer6
2025-04-17 12:59:24,499 - asr_multi - INFO - Downloading ASR model 'TransformerE18' → espnet/shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best
2025-04-17 12:59:24,588 - asr_multi - INFO - Model directory for TransformerE18: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--espnet--shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best/snapshots/bc6bbd771cec698f070640ee677a66719181f0a2/exp/asr_train_asr_transformer_e18_raw_bpe_sp
2025-04-17 12:59:24,589 - asr_multi - INFO -   asr_train_config: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--espnet--shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best/snapshots/bc6bbd771cec698f070640ee677a66719181f0a2/exp/asr_train_asr_transformer_e18_raw_bpe_sp/config.yaml
2025-04-17 12:59:24,589 - asr_multi - INFO -   lm_train_config: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--espnet--shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best/snapshots/bc6bbd771cec698f070640ee677a66719181f0a2/exp/lm_train_lm_adam_bpe/config.yaml
2025-04-17 12:59:24,589 - asr_multi - INFO -   asr_model_file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--espnet--shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best/snapshots/bc6bbd771cec698f070640ee677a66719181f0a2/exp/asr_train_asr_transformer_e18_raw_bpe_sp/54epoch.pth
2025-04-17 12:59:24,590 - asr_multi - INFO -   lm_file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--espnet--shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best/snapshots/bc6bbd771cec698f070640ee677a66719181f0a2/exp/lm_train_lm_adam_bpe/17epoch.pth
2025-04-17 12:59:24,591 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--espnet--shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best/snapshots/bc6bbd771cec698f070640ee677a66719181f0a2/exp/asr_train_asr_transformer_e18_raw_bpe_sp/config.yaml
2025-04-17 12:59:25,435 - root - INFO - Vocabulary size: 5000
2025-04-17 12:59:25,759 - root - INFO - Gradient checkpoint layers: []
2025-04-17 12:59:26,290 - root - INFO - Initialize encoder.embed.conv.0.bias to zeros
2025-04-17 12:59:26,291 - root - INFO - Initialize encoder.embed.conv.2.bias to zeros
2025-04-17 12:59:26,291 - root - INFO - Initialize encoder.embed.out.0.bias to zeros
2025-04-17 12:59:26,292 - root - INFO - Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
2025-04-17 12:59:26,292 - root - INFO - Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
2025-04-17 12:59:26,292 - root - INFO - Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
2025-04-17 12:59:26,293 - root - INFO - Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
2025-04-17 12:59:26,293 - root - INFO - Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
2025-04-17 12:59:26,293 - root - INFO - Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
2025-04-17 12:59:26,294 - root - INFO - Initialize encoder.encoders.0.norm1.bias to zeros
2025-04-17 12:59:26,294 - root - INFO - Initialize encoder.encoders.0.norm2.bias to zeros
2025-04-17 12:59:26,294 - root - INFO - Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
2025-04-17 12:59:26,294 - root - INFO - Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
2025-04-17 12:59:26,295 - root - INFO - Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
2025-04-17 12:59:26,295 - root - INFO - Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
2025-04-17 12:59:26,295 - root - INFO - Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
2025-04-17 12:59:26,296 - root - INFO - Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
2025-04-17 12:59:26,296 - root - INFO - Initialize encoder.encoders.1.norm1.bias to zeros
2025-04-17 12:59:26,296 - root - INFO - Initialize encoder.encoders.1.norm2.bias to zeros
2025-04-17 12:59:26,297 - root - INFO - Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
2025-04-17 12:59:26,297 - root - INFO - Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
2025-04-17 12:59:26,298 - root - INFO - Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
2025-04-17 12:59:26,298 - root - INFO - Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
2025-04-17 12:59:26,298 - root - INFO - Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
2025-04-17 12:59:26,298 - root - INFO - Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
2025-04-17 12:59:26,299 - root - INFO - Initialize encoder.encoders.2.norm1.bias to zeros
2025-04-17 12:59:26,299 - root - INFO - Initialize encoder.encoders.2.norm2.bias to zeros
2025-04-17 12:59:26,299 - root - INFO - Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
2025-04-17 12:59:26,300 - root - INFO - Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
2025-04-17 12:59:26,300 - root - INFO - Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
2025-04-17 12:59:26,300 - root - INFO - Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
2025-04-17 12:59:26,301 - root - INFO - Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
2025-04-17 12:59:26,301 - root - INFO - Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
2025-04-17 12:59:26,301 - root - INFO - Initialize encoder.encoders.3.norm1.bias to zeros
2025-04-17 12:59:26,302 - root - INFO - Initialize encoder.encoders.3.norm2.bias to zeros
2025-04-17 12:59:26,302 - root - INFO - Initialize encoder.encoders.4.self_attn.linear_q.bias to zeros
2025-04-17 12:59:26,302 - root - INFO - Initialize encoder.encoders.4.self_attn.linear_k.bias to zeros
2025-04-17 12:59:26,303 - root - INFO - Initialize encoder.encoders.4.self_attn.linear_v.bias to zeros
2025-04-17 12:59:26,303 - root - INFO - Initialize encoder.encoders.4.self_attn.linear_out.bias to zeros
2025-04-17 12:59:26,303 - root - INFO - Initialize encoder.encoders.4.feed_forward.w_1.bias to zeros
2025-04-17 12:59:26,304 - root - INFO - Initialize encoder.encoders.4.feed_forward.w_2.bias to zeros
2025-04-17 12:59:26,304 - root - INFO - Initialize encoder.encoders.4.norm1.bias to zeros
2025-04-17 12:59:26,304 - root - INFO - Initialize encoder.encoders.4.norm2.bias to zeros
2025-04-17 12:59:26,305 - root - INFO - Initialize encoder.encoders.5.self_attn.linear_q.bias to zeros
2025-04-17 12:59:26,305 - root - INFO - Initialize encoder.encoders.5.self_attn.linear_k.bias to zeros
2025-04-17 12:59:26,305 - root - INFO - Initialize encoder.encoders.5.self_attn.linear_v.bias to zeros
2025-04-17 12:59:26,306 - root - INFO - Initialize encoder.encoders.5.self_attn.linear_out.bias to zeros
2025-04-17 12:59:26,306 - root - INFO - Initialize encoder.encoders.5.feed_forward.w_1.bias to zeros
2025-04-17 12:59:26,306 - root - INFO - Initialize encoder.encoders.5.feed_forward.w_2.bias to zeros
2025-04-17 12:59:26,307 - root - INFO - Initialize encoder.encoders.5.norm1.bias to zeros
2025-04-17 12:59:26,307 - root - INFO - Initialize encoder.encoders.5.norm2.bias to zeros
2025-04-17 12:59:26,307 - root - INFO - Initialize encoder.encoders.6.self_attn.linear_q.bias to zeros
2025-04-17 12:59:26,308 - root - INFO - Initialize encoder.encoders.6.self_attn.linear_k.bias to zeros
2025-04-17 12:59:26,308 - root - INFO - Initialize encoder.encoders.6.self_attn.linear_v.bias to zeros
2025-04-17 12:59:26,308 - root - INFO - Initialize encoder.encoders.6.self_attn.linear_out.bias to zeros
2025-04-17 12:59:26,308 - root - INFO - Initialize encoder.encoders.6.feed_forward.w_1.bias to zeros
2025-04-17 12:59:26,309 - root - INFO - Initialize encoder.encoders.6.feed_forward.w_2.bias to zeros
2025-04-17 12:59:26,309 - root - INFO - Initialize encoder.encoders.6.norm1.bias to zeros
2025-04-17 12:59:26,309 - root - INFO - Initialize encoder.encoders.6.norm2.bias to zeros
2025-04-17 12:59:26,310 - root - INFO - Initialize encoder.encoders.7.self_attn.linear_q.bias to zeros
2025-04-17 12:59:26,310 - root - INFO - Initialize encoder.encoders.7.self_attn.linear_k.bias to zeros
2025-04-17 12:59:26,310 - root - INFO - Initialize encoder.encoders.7.self_attn.linear_v.bias to zeros
2025-04-17 12:59:26,311 - root - INFO - Initialize encoder.encoders.7.self_attn.linear_out.bias to zeros
2025-04-17 12:59:26,311 - root - INFO - Initialize encoder.encoders.7.feed_forward.w_1.bias to zeros
2025-04-17 12:59:26,311 - root - INFO - Initialize encoder.encoders.7.feed_forward.w_2.bias to zeros
2025-04-17 12:59:26,312 - root - INFO - Initialize encoder.encoders.7.norm1.bias to zeros
2025-04-17 12:59:26,312 - root - INFO - Initialize encoder.encoders.7.norm2.bias to zeros
2025-04-17 12:59:26,312 - root - INFO - Initialize encoder.encoders.8.self_attn.linear_q.bias to zeros
2025-04-17 12:59:26,313 - root - INFO - Initialize encoder.encoders.8.self_attn.linear_k.bias to zeros
2025-04-17 12:59:26,313 - root - INFO - Initialize encoder.encoders.8.self_attn.linear_v.bias to zeros
2025-04-17 12:59:26,313 - root - INFO - Initialize encoder.encoders.8.self_attn.linear_out.bias to zeros
2025-04-17 12:59:26,314 - root - INFO - Initialize encoder.encoders.8.feed_forward.w_1.bias to zeros
2025-04-17 12:59:26,316 - root - INFO - Initialize encoder.encoders.8.feed_forward.w_2.bias to zeros
2025-04-17 12:59:26,316 - root - INFO - Initialize encoder.encoders.8.norm1.bias to zeros
2025-04-17 12:59:26,316 - root - INFO - Initialize encoder.encoders.8.norm2.bias to zeros
2025-04-17 12:59:26,317 - root - INFO - Initialize encoder.encoders.9.self_attn.linear_q.bias to zeros
2025-04-17 12:59:26,317 - root - INFO - Initialize encoder.encoders.9.self_attn.linear_k.bias to zeros
2025-04-17 12:59:26,317 - root - INFO - Initialize encoder.encoders.9.self_attn.linear_v.bias to zeros
2025-04-17 12:59:26,318 - root - INFO - Initialize encoder.encoders.9.self_attn.linear_out.bias to zeros
2025-04-17 12:59:26,318 - root - INFO - Initialize encoder.encoders.9.feed_forward.w_1.bias to zeros
2025-04-17 12:59:26,318 - root - INFO - Initialize encoder.encoders.9.feed_forward.w_2.bias to zeros
2025-04-17 12:59:26,321 - root - INFO - Initialize encoder.encoders.9.norm1.bias to zeros
2025-04-17 12:59:26,321 - root - INFO - Initialize encoder.encoders.9.norm2.bias to zeros
2025-04-17 12:59:26,322 - root - INFO - Initialize encoder.encoders.10.self_attn.linear_q.bias to zeros
2025-04-17 12:59:26,322 - root - INFO - Initialize encoder.encoders.10.self_attn.linear_k.bias to zeros
2025-04-17 12:59:26,322 - root - INFO - Initialize encoder.encoders.10.self_attn.linear_v.bias to zeros
2025-04-17 12:59:26,323 - root - INFO - Initialize encoder.encoders.10.self_attn.linear_out.bias to zeros
2025-04-17 12:59:26,323 - root - INFO - Initialize encoder.encoders.10.feed_forward.w_1.bias to zeros
2025-04-17 12:59:26,323 - root - INFO - Initialize encoder.encoders.10.feed_forward.w_2.bias to zeros
2025-04-17 12:59:26,324 - root - INFO - Initialize encoder.encoders.10.norm1.bias to zeros
2025-04-17 12:59:26,324 - root - INFO - Initialize encoder.encoders.10.norm2.bias to zeros
2025-04-17 12:59:26,324 - root - INFO - Initialize encoder.encoders.11.self_attn.linear_q.bias to zeros
2025-04-17 12:59:26,324 - root - INFO - Initialize encoder.encoders.11.self_attn.linear_k.bias to zeros
2025-04-17 12:59:26,325 - root - INFO - Initialize encoder.encoders.11.self_attn.linear_v.bias to zeros
2025-04-17 12:59:26,325 - root - INFO - Initialize encoder.encoders.11.self_attn.linear_out.bias to zeros
2025-04-17 12:59:26,325 - root - INFO - Initialize encoder.encoders.11.feed_forward.w_1.bias to zeros
2025-04-17 12:59:26,326 - root - INFO - Initialize encoder.encoders.11.feed_forward.w_2.bias to zeros
2025-04-17 12:59:26,326 - root - INFO - Initialize encoder.encoders.11.norm1.bias to zeros
2025-04-17 12:59:26,327 - root - INFO - Initialize encoder.encoders.11.norm2.bias to zeros
2025-04-17 12:59:26,327 - root - INFO - Initialize encoder.encoders.12.self_attn.linear_q.bias to zeros
2025-04-17 12:59:26,327 - root - INFO - Initialize encoder.encoders.12.self_attn.linear_k.bias to zeros
2025-04-17 12:59:26,328 - root - INFO - Initialize encoder.encoders.12.self_attn.linear_v.bias to zeros
2025-04-17 12:59:26,328 - root - INFO - Initialize encoder.encoders.12.self_attn.linear_out.bias to zeros
2025-04-17 12:59:26,328 - root - INFO - Initialize encoder.encoders.12.feed_forward.w_1.bias to zeros
2025-04-17 12:59:26,329 - root - INFO - Initialize encoder.encoders.12.feed_forward.w_2.bias to zeros
2025-04-17 12:59:26,329 - root - INFO - Initialize encoder.encoders.12.norm1.bias to zeros
2025-04-17 12:59:26,329 - root - INFO - Initialize encoder.encoders.12.norm2.bias to zeros
2025-04-17 12:59:26,330 - root - INFO - Initialize encoder.encoders.13.self_attn.linear_q.bias to zeros
2025-04-17 12:59:26,330 - root - INFO - Initialize encoder.encoders.13.self_attn.linear_k.bias to zeros
2025-04-17 12:59:26,330 - root - INFO - Initialize encoder.encoders.13.self_attn.linear_v.bias to zeros
2025-04-17 12:59:26,330 - root - INFO - Initialize encoder.encoders.13.self_attn.linear_out.bias to zeros
2025-04-17 12:59:26,331 - root - INFO - Initialize encoder.encoders.13.feed_forward.w_1.bias to zeros
2025-04-17 12:59:26,331 - root - INFO - Initialize encoder.encoders.13.feed_forward.w_2.bias to zeros
2025-04-17 12:59:26,331 - root - INFO - Initialize encoder.encoders.13.norm1.bias to zeros
2025-04-17 12:59:26,332 - root - INFO - Initialize encoder.encoders.13.norm2.bias to zeros
2025-04-17 12:59:26,332 - root - INFO - Initialize encoder.encoders.14.self_attn.linear_q.bias to zeros
2025-04-17 12:59:26,332 - root - INFO - Initialize encoder.encoders.14.self_attn.linear_k.bias to zeros
2025-04-17 12:59:26,333 - root - INFO - Initialize encoder.encoders.14.self_attn.linear_v.bias to zeros
2025-04-17 12:59:26,333 - root - INFO - Initialize encoder.encoders.14.self_attn.linear_out.bias to zeros
2025-04-17 12:59:26,333 - root - INFO - Initialize encoder.encoders.14.feed_forward.w_1.bias to zeros
2025-04-17 12:59:26,334 - root - INFO - Initialize encoder.encoders.14.feed_forward.w_2.bias to zeros
2025-04-17 12:59:26,334 - root - INFO - Initialize encoder.encoders.14.norm1.bias to zeros
2025-04-17 12:59:26,334 - root - INFO - Initialize encoder.encoders.14.norm2.bias to zeros
2025-04-17 12:59:26,334 - root - INFO - Initialize encoder.encoders.15.self_attn.linear_q.bias to zeros
2025-04-17 12:59:26,335 - root - INFO - Initialize encoder.encoders.15.self_attn.linear_k.bias to zeros
2025-04-17 12:59:26,335 - root - INFO - Initialize encoder.encoders.15.self_attn.linear_v.bias to zeros
2025-04-17 12:59:26,335 - root - INFO - Initialize encoder.encoders.15.self_attn.linear_out.bias to zeros
2025-04-17 12:59:26,336 - root - INFO - Initialize encoder.encoders.15.feed_forward.w_1.bias to zeros
2025-04-17 12:59:26,336 - root - INFO - Initialize encoder.encoders.15.feed_forward.w_2.bias to zeros
2025-04-17 12:59:26,336 - root - INFO - Initialize encoder.encoders.15.norm1.bias to zeros
2025-04-17 12:59:26,338 - root - INFO - Initialize encoder.encoders.15.norm2.bias to zeros
2025-04-17 12:59:26,338 - root - INFO - Initialize encoder.encoders.16.self_attn.linear_q.bias to zeros
2025-04-17 12:59:26,338 - root - INFO - Initialize encoder.encoders.16.self_attn.linear_k.bias to zeros
2025-04-17 12:59:26,339 - root - INFO - Initialize encoder.encoders.16.self_attn.linear_v.bias to zeros
2025-04-17 12:59:26,339 - root - INFO - Initialize encoder.encoders.16.self_attn.linear_out.bias to zeros
2025-04-17 12:59:26,339 - root - INFO - Initialize encoder.encoders.16.feed_forward.w_1.bias to zeros
2025-04-17 12:59:26,339 - root - INFO - Initialize encoder.encoders.16.feed_forward.w_2.bias to zeros
2025-04-17 12:59:26,340 - root - INFO - Initialize encoder.encoders.16.norm1.bias to zeros
2025-04-17 12:59:26,340 - root - INFO - Initialize encoder.encoders.16.norm2.bias to zeros
2025-04-17 12:59:26,340 - root - INFO - Initialize encoder.encoders.17.self_attn.linear_q.bias to zeros
2025-04-17 12:59:26,340 - root - INFO - Initialize encoder.encoders.17.self_attn.linear_k.bias to zeros
2025-04-17 12:59:26,341 - root - INFO - Initialize encoder.encoders.17.self_attn.linear_v.bias to zeros
2025-04-17 12:59:26,341 - root - INFO - Initialize encoder.encoders.17.self_attn.linear_out.bias to zeros
2025-04-17 12:59:26,341 - root - INFO - Initialize encoder.encoders.17.feed_forward.w_1.bias to zeros
2025-04-17 12:59:26,342 - root - INFO - Initialize encoder.encoders.17.feed_forward.w_2.bias to zeros
2025-04-17 12:59:26,342 - root - INFO - Initialize encoder.encoders.17.norm1.bias to zeros
2025-04-17 12:59:26,342 - root - INFO - Initialize encoder.encoders.17.norm2.bias to zeros
2025-04-17 12:59:26,342 - root - INFO - Initialize encoder.after_norm.bias to zeros
2025-04-17 12:59:26,343 - root - INFO - Initialize decoder.after_norm.bias to zeros
2025-04-17 12:59:26,343 - root - INFO - Initialize decoder.output_layer.bias to zeros
2025-04-17 12:59:26,343 - root - INFO - Initialize decoder.decoders.0.self_attn.linear_q.bias to zeros
2025-04-17 12:59:26,343 - root - INFO - Initialize decoder.decoders.0.self_attn.linear_k.bias to zeros
2025-04-17 12:59:26,344 - root - INFO - Initialize decoder.decoders.0.self_attn.linear_v.bias to zeros
2025-04-17 12:59:26,344 - root - INFO - Initialize decoder.decoders.0.self_attn.linear_out.bias to zeros
2025-04-17 12:59:26,344 - root - INFO - Initialize decoder.decoders.0.src_attn.linear_q.bias to zeros
2025-04-17 12:59:26,345 - root - INFO - Initialize decoder.decoders.0.src_attn.linear_k.bias to zeros
2025-04-17 12:59:26,345 - root - INFO - Initialize decoder.decoders.0.src_attn.linear_v.bias to zeros
2025-04-17 12:59:26,345 - root - INFO - Initialize decoder.decoders.0.src_attn.linear_out.bias to zeros
2025-04-17 12:59:26,345 - root - INFO - Initialize decoder.decoders.0.feed_forward.w_1.bias to zeros
2025-04-17 12:59:26,346 - root - INFO - Initialize decoder.decoders.0.feed_forward.w_2.bias to zeros
2025-04-17 12:59:26,346 - root - INFO - Initialize decoder.decoders.0.norm1.bias to zeros
2025-04-17 12:59:26,347 - root - INFO - Initialize decoder.decoders.0.norm2.bias to zeros
2025-04-17 12:59:26,347 - root - INFO - Initialize decoder.decoders.0.norm3.bias to zeros
2025-04-17 12:59:26,347 - root - INFO - Initialize decoder.decoders.1.self_attn.linear_q.bias to zeros
2025-04-17 12:59:26,348 - root - INFO - Initialize decoder.decoders.1.self_attn.linear_k.bias to zeros
2025-04-17 12:59:26,348 - root - INFO - Initialize decoder.decoders.1.self_attn.linear_v.bias to zeros
2025-04-17 12:59:26,348 - root - INFO - Initialize decoder.decoders.1.self_attn.linear_out.bias to zeros
2025-04-17 12:59:26,351 - root - INFO - Initialize decoder.decoders.1.src_attn.linear_q.bias to zeros
2025-04-17 12:59:26,351 - root - INFO - Initialize decoder.decoders.1.src_attn.linear_k.bias to zeros
2025-04-17 12:59:26,351 - root - INFO - Initialize decoder.decoders.1.src_attn.linear_v.bias to zeros
2025-04-17 12:59:26,352 - root - INFO - Initialize decoder.decoders.1.src_attn.linear_out.bias to zeros
2025-04-17 12:59:26,352 - root - INFO - Initialize decoder.decoders.1.feed_forward.w_1.bias to zeros
2025-04-17 12:59:26,352 - root - INFO - Initialize decoder.decoders.1.feed_forward.w_2.bias to zeros
2025-04-17 12:59:26,354 - root - INFO - Initialize decoder.decoders.1.norm1.bias to zeros
2025-04-17 12:59:26,355 - root - INFO - Initialize decoder.decoders.1.norm2.bias to zeros
2025-04-17 12:59:26,355 - root - INFO - Initialize decoder.decoders.1.norm3.bias to zeros
2025-04-17 12:59:26,355 - root - INFO - Initialize decoder.decoders.2.self_attn.linear_q.bias to zeros
2025-04-17 12:59:26,356 - root - INFO - Initialize decoder.decoders.2.self_attn.linear_k.bias to zeros
2025-04-17 12:59:26,356 - root - INFO - Initialize decoder.decoders.2.self_attn.linear_v.bias to zeros
2025-04-17 12:59:26,356 - root - INFO - Initialize decoder.decoders.2.self_attn.linear_out.bias to zeros
2025-04-17 12:59:26,357 - root - INFO - Initialize decoder.decoders.2.src_attn.linear_q.bias to zeros
2025-04-17 12:59:26,357 - root - INFO - Initialize decoder.decoders.2.src_attn.linear_k.bias to zeros
2025-04-17 12:59:26,358 - root - INFO - Initialize decoder.decoders.2.src_attn.linear_v.bias to zeros
2025-04-17 12:59:26,358 - root - INFO - Initialize decoder.decoders.2.src_attn.linear_out.bias to zeros
2025-04-17 12:59:26,358 - root - INFO - Initialize decoder.decoders.2.feed_forward.w_1.bias to zeros
2025-04-17 12:59:26,359 - root - INFO - Initialize decoder.decoders.2.feed_forward.w_2.bias to zeros
2025-04-17 12:59:26,359 - root - INFO - Initialize decoder.decoders.2.norm1.bias to zeros
2025-04-17 12:59:26,359 - root - INFO - Initialize decoder.decoders.2.norm2.bias to zeros
2025-04-17 12:59:26,359 - root - INFO - Initialize decoder.decoders.2.norm3.bias to zeros
2025-04-17 12:59:26,360 - root - INFO - Initialize decoder.decoders.3.self_attn.linear_q.bias to zeros
2025-04-17 12:59:26,360 - root - INFO - Initialize decoder.decoders.3.self_attn.linear_k.bias to zeros
2025-04-17 12:59:26,361 - root - INFO - Initialize decoder.decoders.3.self_attn.linear_v.bias to zeros
2025-04-17 12:59:26,362 - root - INFO - Initialize decoder.decoders.3.self_attn.linear_out.bias to zeros
2025-04-17 12:59:26,362 - root - INFO - Initialize decoder.decoders.3.src_attn.linear_q.bias to zeros
2025-04-17 12:59:26,362 - root - INFO - Initialize decoder.decoders.3.src_attn.linear_k.bias to zeros
2025-04-17 12:59:26,363 - root - INFO - Initialize decoder.decoders.3.src_attn.linear_v.bias to zeros
2025-04-17 12:59:26,363 - root - INFO - Initialize decoder.decoders.3.src_attn.linear_out.bias to zeros
2025-04-17 12:59:26,363 - root - INFO - Initialize decoder.decoders.3.feed_forward.w_1.bias to zeros
2025-04-17 12:59:26,363 - root - INFO - Initialize decoder.decoders.3.feed_forward.w_2.bias to zeros
2025-04-17 12:59:26,364 - root - INFO - Initialize decoder.decoders.3.norm1.bias to zeros
2025-04-17 12:59:26,364 - root - INFO - Initialize decoder.decoders.3.norm2.bias to zeros
2025-04-17 12:59:26,365 - root - INFO - Initialize decoder.decoders.3.norm3.bias to zeros
2025-04-17 12:59:26,365 - root - INFO - Initialize decoder.decoders.4.self_attn.linear_q.bias to zeros
2025-04-17 12:59:26,365 - root - INFO - Initialize decoder.decoders.4.self_attn.linear_k.bias to zeros
2025-04-17 12:59:26,365 - root - INFO - Initialize decoder.decoders.4.self_attn.linear_v.bias to zeros
2025-04-17 12:59:26,366 - root - INFO - Initialize decoder.decoders.4.self_attn.linear_out.bias to zeros
2025-04-17 12:59:26,366 - root - INFO - Initialize decoder.decoders.4.src_attn.linear_q.bias to zeros
2025-04-17 12:59:26,367 - root - INFO - Initialize decoder.decoders.4.src_attn.linear_k.bias to zeros
2025-04-17 12:59:26,367 - root - INFO - Initialize decoder.decoders.4.src_attn.linear_v.bias to zeros
2025-04-17 12:59:26,368 - root - INFO - Initialize decoder.decoders.4.src_attn.linear_out.bias to zeros
2025-04-17 12:59:26,368 - root - INFO - Initialize decoder.decoders.4.feed_forward.w_1.bias to zeros
2025-04-17 12:59:26,368 - root - INFO - Initialize decoder.decoders.4.feed_forward.w_2.bias to zeros
2025-04-17 12:59:26,369 - root - INFO - Initialize decoder.decoders.4.norm1.bias to zeros
2025-04-17 12:59:26,369 - root - INFO - Initialize decoder.decoders.4.norm2.bias to zeros
2025-04-17 12:59:26,369 - root - INFO - Initialize decoder.decoders.4.norm3.bias to zeros
2025-04-17 12:59:26,369 - root - INFO - Initialize decoder.decoders.5.self_attn.linear_q.bias to zeros
2025-04-17 12:59:26,370 - root - INFO - Initialize decoder.decoders.5.self_attn.linear_k.bias to zeros
2025-04-17 12:59:26,370 - root - INFO - Initialize decoder.decoders.5.self_attn.linear_v.bias to zeros
2025-04-17 12:59:26,371 - root - INFO - Initialize decoder.decoders.5.self_attn.linear_out.bias to zeros
2025-04-17 12:59:26,371 - root - INFO - Initialize decoder.decoders.5.src_attn.linear_q.bias to zeros
2025-04-17 12:59:26,372 - root - INFO - Initialize decoder.decoders.5.src_attn.linear_k.bias to zeros
2025-04-17 12:59:26,372 - root - INFO - Initialize decoder.decoders.5.src_attn.linear_v.bias to zeros
2025-04-17 12:59:26,374 - root - INFO - Initialize decoder.decoders.5.src_attn.linear_out.bias to zeros
2025-04-17 12:59:26,374 - root - INFO - Initialize decoder.decoders.5.feed_forward.w_1.bias to zeros
2025-04-17 12:59:26,374 - root - INFO - Initialize decoder.decoders.5.feed_forward.w_2.bias to zeros
2025-04-17 12:59:26,375 - root - INFO - Initialize decoder.decoders.5.norm1.bias to zeros
2025-04-17 12:59:26,375 - root - INFO - Initialize decoder.decoders.5.norm2.bias to zeros
2025-04-17 12:59:26,375 - root - INFO - Initialize decoder.decoders.5.norm3.bias to zeros
2025-04-17 12:59:26,375 - root - INFO - Initialize ctc.ctc_lo.bias to zeros
2025-04-17 12:59:26,828 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--espnet--shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best/snapshots/bc6bbd771cec698f070640ee677a66719181f0a2/exp/lm_train_lm_adam_bpe/config.yaml
2025-04-17 12:59:27,666 - root - INFO - Vocabulary size: 5000
2025-04-17 12:59:28,565 - espnet2.bin.asr_inference - INFO - BatchBeamSearch implementation is selected.
2025-04-17 12:59:28,580 - espnet2.bin.asr_inference - INFO - Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (lm): SequentialRNNLM(
      (drop): Dropout(p=0.0, inplace=False)
      (encoder): Embedding(5000, 2048, padding_idx=0)
      (rnn): LSTM(2048, 2048, num_layers=4, batch_first=True)
      (decoder): Linear(in_features=2048, out_features=5000, bias=True)
    )
  )
)
2025-04-17 12:59:28,581 - espnet2.bin.asr_inference - INFO - Decoding device=cuda, dtype=float32
2025-04-17 12:59:28,583 - espnet2.bin.asr_inference - INFO - Text tokenizer: SentencepiecesTokenizer(model="data/token_list/bpe_unigram5000/model.model")
2025-04-17 12:59:28,584 - asr_multi - INFO - Successfully loaded model: TransformerE18
2025-04-17 12:59:28,584 - asr_multi - INFO - Loaded ASR models: ['Conformer6', 'TransformerE18']
2025-04-17 12:59:28,699 - asr_multi - INFO - Launching Gradio app…
2025-04-17 12:59:57,122 - asr_multi - INFO - Audio in: sr=48000, length=3.06 s, peak=1693035520.00
2025-04-17 12:59:57,122 - asr_multi - INFO - Normalized (peak=1693035520.00)
2025-04-17 12:59:58,493 - asr_multi - INFO - Saved debug_samples/input_20250417_125958.wav (3.06 s)
2025-04-17 12:59:58,494 - espnet2.bin.asr_inference - INFO - speech length: 48960
2025-04-17 12:59:58,807 - espnet.nets.beam_search - INFO - decoder input length: 47
2025-04-17 12:59:58,807 - espnet.nets.beam_search - INFO - max output length: 47
2025-04-17 12:59:58,808 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 12:59:59,415 - espnet.nets.beam_search - INFO - end detected at 16
2025-04-17 12:59:59,416 - espnet.nets.beam_search - INFO - -15.67 * 0.7 = -10.97 for decoder
2025-04-17 12:59:59,416 - espnet.nets.beam_search - INFO -  -9.97 * 0.3 =  -2.99 for ctc
2025-04-17 12:59:59,417 - espnet.nets.beam_search - INFO - -33.22 * 1.0 = -33.22 for lm
2025-04-17 12:59:59,417 - espnet.nets.beam_search - INFO - total log probability: -47.18
2025-04-17 12:59:59,417 - espnet.nets.beam_search - INFO - normalized log probability: -3.93
2025-04-17 12:59:59,417 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 20
2025-04-17 12:59:59,418 - espnet.nets.beam_search - INFO - best hypo: ▁HELLO▁MY▁NAME▁IS▁UMBOO

2025-04-17 12:59:59,425 - asr_multi - INFO - Recognized: HELLO MY NAME IS UMBOO
2025-04-17 13:00:09,327 - asr_multi - INFO - Audio in: sr=48000, length=3.06 s, peak=1693035520.00
2025-04-17 13:00:09,327 - asr_multi - INFO - Normalized (peak=1693035520.00)
2025-04-17 13:00:09,332 - asr_multi - INFO - Saved debug_samples/input_20250417_130009.wav (3.06 s)
2025-04-17 13:00:09,332 - espnet2.bin.asr_inference - INFO - speech length: 48960
2025-04-17 13:00:09,359 - espnet.nets.beam_search - INFO - decoder input length: 63
2025-04-17 13:00:09,360 - espnet.nets.beam_search - INFO - max output length: 63
2025-04-17 13:00:09,360 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 13:00:09,736 - espnet.nets.beam_search - INFO - end detected at 16
2025-04-17 13:00:09,737 - espnet.nets.beam_search - INFO - -18.41 * 0.7 = -12.88 for decoder
2025-04-17 13:00:09,737 - espnet.nets.beam_search - INFO - -22.25 * 0.3 =  -6.68 for ctc
2025-04-17 13:00:09,737 - espnet.nets.beam_search - INFO - -29.55 * 1.0 = -29.55 for lm
2025-04-17 13:00:09,737 - espnet.nets.beam_search - INFO - total log probability: -49.11
2025-04-17 13:00:09,738 - espnet.nets.beam_search - INFO - normalized log probability: -4.46
2025-04-17 13:00:09,738 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 16
2025-04-17 13:00:09,739 - espnet.nets.beam_search - INFO - best hypo: ▁HELLO▁MY▁NAME▁ISN'T

2025-04-17 13:00:09,739 - asr_multi - ERROR - ASR error Not found: "data/token_list/bpe_unigram5000/model.model": No such file or directory Error #2
Traceback (most recent call last):
  File "/home/ambuja/emo-tts/src/gradio_app.py", line 225, in transcribe_and_speak
    nbests = model(wav)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet2/bin/asr_inference.py", line 559, in __call__
    results = self._decode_single_sample(enc[0])
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet2/bin/asr_inference.py", line 676, in _decode_single_sample
    text = self.tokenizer.tokens2text(token)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet2/text/sentencepiece_tokenizer.py", line 36, in tokens2text
    self._build_sentence_piece_processor()
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet2/text/sentencepiece_tokenizer.py", line 29, in _build_sentence_piece_processor
    self.sp.load(self.model)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/sentencepiece/__init__.py", line 961, in Load
    return self.LoadFromFile(model_file)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/sentencepiece/__init__.py", line 316, in LoadFromFile
    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)
OSError: Not found: "data/token_list/bpe_unigram5000/model.model": No such file or directory Error #2
2025-04-17 13:00:20,134 - asr_multi - INFO - Audio in: sr=48000, length=4.14 s, peak=2147483648.00
2025-04-17 13:00:20,134 - asr_multi - INFO - Normalized (peak=2147483648.00)
2025-04-17 13:00:20,138 - asr_multi - INFO - Saved debug_samples/input_20250417_130020.wav (4.14 s)
2025-04-17 13:00:20,139 - espnet2.bin.asr_inference - INFO - speech length: 66240
2025-04-17 13:00:20,159 - espnet.nets.beam_search - INFO - decoder input length: 85
2025-04-17 13:00:20,160 - espnet.nets.beam_search - INFO - max output length: 85
2025-04-17 13:00:20,160 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 13:00:20,548 - espnet.nets.beam_search - INFO - end detected at 15
2025-04-17 13:00:20,548 - espnet.nets.beam_search - INFO -  -6.39 * 0.7 =  -4.48 for decoder
2025-04-17 13:00:20,549 - espnet.nets.beam_search - INFO - -14.34 * 0.3 =  -4.30 for ctc
2025-04-17 13:00:20,549 - espnet.nets.beam_search - INFO - -33.71 * 1.0 = -33.71 for lm
2025-04-17 13:00:20,549 - espnet.nets.beam_search - INFO - total log probability: -42.49
2025-04-17 13:00:20,550 - espnet.nets.beam_search - INFO - normalized log probability: -4.25
2025-04-17 13:00:20,550 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 26
2025-04-17 13:00:20,550 - espnet.nets.beam_search - INFO - best hypo: ▁HELLO▁MY▁NAME▁IS▁ON▁BOARD

2025-04-17 13:00:20,553 - asr_multi - INFO - Recognized: 
2025-04-17 13:00:31,131 - asr_multi - INFO - Audio in: sr=48000, length=4.14 s, peak=2147483648.00
2025-04-17 13:00:31,131 - asr_multi - INFO - Normalized (peak=2147483648.00)
2025-04-17 13:00:31,135 - asr_multi - INFO - Saved debug_samples/input_20250417_130031.wav (4.14 s)
2025-04-17 13:00:31,136 - espnet2.bin.asr_inference - INFO - speech length: 66240
2025-04-17 13:00:31,154 - espnet.nets.beam_search - INFO - decoder input length: 85
2025-04-17 13:00:31,154 - espnet.nets.beam_search - INFO - max output length: 85
2025-04-17 13:00:31,154 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 13:00:31,548 - espnet.nets.beam_search - INFO - end detected at 15
2025-04-17 13:00:31,548 - espnet.nets.beam_search - INFO -  -6.39 * 0.7 =  -4.48 for decoder
2025-04-17 13:00:31,549 - espnet.nets.beam_search - INFO - -14.34 * 0.3 =  -4.30 for ctc
2025-04-17 13:00:31,549 - espnet.nets.beam_search - INFO - -33.71 * 1.0 = -33.71 for lm
2025-04-17 13:00:31,549 - espnet.nets.beam_search - INFO - total log probability: -42.49
2025-04-17 13:00:31,550 - espnet.nets.beam_search - INFO - normalized log probability: -4.25
2025-04-17 13:00:31,550 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 26
2025-04-17 13:00:31,550 - espnet.nets.beam_search - INFO - best hypo: ▁HELLO▁MY▁NAME▁IS▁ON▁BOARD

2025-04-17 13:00:31,552 - asr_multi - INFO - Recognized: 
2025-04-17 13:01:08,835 - asr_multi - INFO - Audio in: sr=48000, length=6.24 s, peak=1163816960.00
2025-04-17 13:01:08,836 - asr_multi - INFO - Normalized (peak=1163816960.00)
2025-04-17 13:01:08,841 - asr_multi - INFO - Saved debug_samples/input_20250417_130108.wav (6.24 s)
2025-04-17 13:01:08,841 - espnet2.bin.asr_inference - INFO - speech length: 99840
2025-04-17 13:01:08,885 - espnet.nets.beam_search - INFO - decoder input length: 129
2025-04-17 13:01:08,885 - espnet.nets.beam_search - INFO - max output length: 129
2025-04-17 13:01:08,886 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 13:01:09,664 - espnet.nets.beam_search - INFO - end detected at 26
2025-04-17 13:01:09,665 - espnet.nets.beam_search - INFO - -21.12 * 0.7 = -14.79 for decoder
2025-04-17 13:01:09,665 - espnet.nets.beam_search - INFO - -32.01 * 0.3 =  -9.60 for ctc
2025-04-17 13:01:09,665 - espnet.nets.beam_search - INFO - -72.58 * 1.0 = -72.58 for lm
2025-04-17 13:01:09,666 - espnet.nets.beam_search - INFO - total log probability: -96.96
2025-04-17 13:01:09,666 - espnet.nets.beam_search - INFO - normalized log probability: -4.41
2025-04-17 13:01:09,666 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 19
2025-04-17 13:01:09,667 - espnet.nets.beam_search - INFO - best hypo: ▁HELLO▁MY▁NAME▁IS▁UMBUTTON▁AND▁I▁LIKE▁TO▁PLAY▁BANDAGES

2025-04-17 13:01:09,668 - asr_multi - INFO - Recognized: 
2025-04-17 13:01:25,047 - asr_multi - INFO - Audio in: sr=48000, length=6.24 s, peak=1163816960.00
2025-04-17 13:01:25,047 - asr_multi - INFO - Normalized (peak=1163816960.00)
2025-04-17 13:01:25,052 - asr_multi - INFO - Saved debug_samples/input_20250417_130125.wav (6.24 s)
2025-04-17 13:01:25,053 - espnet2.bin.asr_inference - INFO - speech length: 99840
2025-04-17 13:01:25,085 - espnet.nets.beam_search - INFO - decoder input length: 97
2025-04-17 13:01:25,085 - espnet.nets.beam_search - INFO - max output length: 97
2025-04-17 13:01:25,085 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 13:01:25,930 - espnet.nets.beam_search - INFO - end detected at 22
2025-04-17 13:01:25,931 - espnet.nets.beam_search - INFO - -23.27 * 0.7 = -16.29 for decoder
2025-04-17 13:01:25,931 - espnet.nets.beam_search - INFO - -40.91 * 0.3 = -12.27 for ctc
2025-04-17 13:01:25,931 - espnet.nets.beam_search - INFO - -49.82 * 1.0 = -49.82 for lm
2025-04-17 13:01:25,932 - espnet.nets.beam_search - INFO - total log probability: -78.38
2025-04-17 13:01:25,932 - espnet.nets.beam_search - INFO - normalized log probability: -4.35
2025-04-17 13:01:25,932 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 19
2025-04-17 13:01:25,933 - espnet.nets.beam_search - INFO - best hypo: ▁HELLO▁MY▁NAME▁IS▁UMPIRE▁AND▁I▁LIKE▁TO▁PLAY

2025-04-17 13:01:25,934 - asr_multi - INFO - Recognized: HELLO MY NAME IS UMPIRE AND I LIKE TO PLAY
2025-04-17 13:03:31,525 - asr_multi - INFO - Starting multi-model ASR/TTS demo
2025-04-17 13:03:31,525 - asr_multi - INFO - Using device: cuda
2025-04-17 13:03:31,533 - asr_multi - INFO - Downloading ASR model 'Conformer6' → kamo-naoyuki/librispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp_valid.acc.ave
2025-04-17 13:03:42,498 - asr_multi - INFO - Model directory for Conformer6: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp
2025-04-17 13:03:42,498 - asr_multi - INFO -   asr_train_config: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp/config.yaml
2025-04-17 13:03:42,498 - asr_multi - INFO -   lm_train_config: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/lm_train_lm_transformer2_bpe5000_scheduler_confwarmup_steps25000_batch_bins500000000_accum_grad2_use_amptrue/config.yaml
2025-04-17 13:03:42,499 - asr_multi - INFO -   asr_model_file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp/valid.acc.ave_10best.pth
2025-04-17 13:03:42,499 - asr_multi - INFO -   lm_file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/lm_train_lm_transformer2_bpe5000_scheduler_confwarmup_steps25000_batch_bins500000000_accum_grad2_use_amptrue/valid.loss.ave_10best.pth
2025-04-17 13:03:42,501 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp/config.yaml
2025-04-17 13:03:43,340 - root - INFO - Vocabulary size: 5000
2025-04-17 13:03:43,414 - root - WARNING - Using legacy_rel_pos and it will be deprecated in the future.
2025-04-17 13:03:43,475 - root - WARNING - Using legacy_rel_selfattn and it will be deprecated in the future.
2025-04-17 13:03:43,954 - root - INFO - Gradient checkpoint layers: []
2025-04-17 13:03:45,451 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/lm_train_lm_transformer2_bpe5000_scheduler_confwarmup_steps25000_batch_bins500000000_accum_grad2_use_amptrue/config.yaml
2025-04-17 13:03:46,271 - root - INFO - Vocabulary size: 5000
2025-04-17 13:03:46,275 - root - INFO - encoder self-attention layer type = self-attention
2025-04-17 13:03:46,669 - espnet2.bin.asr_inference - INFO - BatchBeamSearch implementation is selected.
2025-04-17 13:03:46,699 - espnet2.bin.asr_inference - INFO - Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (lm): TransformerLM(
      (embed): Embedding(5000, 128)
      (encoder): Encoder(
        (embed): Sequential(
          (0): Linear(in_features=128, out_features=512, bias=True)
          (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): Dropout(p=0.0, inplace=False)
          (3): ReLU()
          (4): Sequential()
        )
        (encoders): MultiSequential(
          (0): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (4): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (5): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (6): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (7): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (8): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (9): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (10): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (11): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (12): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (13): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (14): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (15): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): Linear(in_features=512, out_features=5000, bias=True)
    )
  )
)
2025-04-17 13:03:46,700 - espnet2.bin.asr_inference - INFO - Decoding device=cuda, dtype=float32
2025-04-17 13:03:46,703 - espnet2.bin.asr_inference - INFO - Text tokenizer: SentencepiecesTokenizer(model="/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/data/en_token_list/bpe_unigram5000/bpe.model")
2025-04-17 13:03:46,704 - asr_multi - INFO - Successfully loaded model: Conformer6
2025-04-17 13:03:46,706 - asr_multi - INFO - Downloading ASR model 'TransformerE18' → espnet/shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best
2025-04-17 13:03:46,810 - asr_multi - INFO - Model directory for TransformerE18: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--espnet--shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best/snapshots/bc6bbd771cec698f070640ee677a66719181f0a2/exp/asr_train_asr_transformer_e18_raw_bpe_sp
2025-04-17 13:03:46,810 - asr_multi - INFO -   asr_train_config: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--espnet--shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best/snapshots/bc6bbd771cec698f070640ee677a66719181f0a2/exp/asr_train_asr_transformer_e18_raw_bpe_sp/config.yaml
2025-04-17 13:03:46,811 - asr_multi - INFO -   lm_train_config: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--espnet--shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best/snapshots/bc6bbd771cec698f070640ee677a66719181f0a2/exp/lm_train_lm_adam_bpe/config.yaml
2025-04-17 13:03:46,811 - asr_multi - INFO -   asr_model_file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--espnet--shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best/snapshots/bc6bbd771cec698f070640ee677a66719181f0a2/exp/asr_train_asr_transformer_e18_raw_bpe_sp/54epoch.pth
2025-04-17 13:03:46,811 - asr_multi - INFO -   lm_file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--espnet--shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best/snapshots/bc6bbd771cec698f070640ee677a66719181f0a2/exp/lm_train_lm_adam_bpe/17epoch.pth
2025-04-17 13:03:46,813 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--espnet--shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best/snapshots/bc6bbd771cec698f070640ee677a66719181f0a2/exp/asr_train_asr_transformer_e18_raw_bpe_sp/config.yaml
2025-04-17 13:03:47,638 - root - INFO - Vocabulary size: 5000
2025-04-17 13:03:47,974 - root - INFO - Gradient checkpoint layers: []
2025-04-17 13:03:48,505 - root - INFO - Initialize encoder.embed.conv.0.bias to zeros
2025-04-17 13:03:48,505 - root - INFO - Initialize encoder.embed.conv.2.bias to zeros
2025-04-17 13:03:48,506 - root - INFO - Initialize encoder.embed.out.0.bias to zeros
2025-04-17 13:03:48,506 - root - INFO - Initialize encoder.encoders.0.self_attn.linear_q.bias to zeros
2025-04-17 13:03:48,506 - root - INFO - Initialize encoder.encoders.0.self_attn.linear_k.bias to zeros
2025-04-17 13:03:48,507 - root - INFO - Initialize encoder.encoders.0.self_attn.linear_v.bias to zeros
2025-04-17 13:03:48,507 - root - INFO - Initialize encoder.encoders.0.self_attn.linear_out.bias to zeros
2025-04-17 13:03:48,507 - root - INFO - Initialize encoder.encoders.0.feed_forward.w_1.bias to zeros
2025-04-17 13:03:48,508 - root - INFO - Initialize encoder.encoders.0.feed_forward.w_2.bias to zeros
2025-04-17 13:03:48,508 - root - INFO - Initialize encoder.encoders.0.norm1.bias to zeros
2025-04-17 13:03:48,508 - root - INFO - Initialize encoder.encoders.0.norm2.bias to zeros
2025-04-17 13:03:48,509 - root - INFO - Initialize encoder.encoders.1.self_attn.linear_q.bias to zeros
2025-04-17 13:03:48,509 - root - INFO - Initialize encoder.encoders.1.self_attn.linear_k.bias to zeros
2025-04-17 13:03:48,509 - root - INFO - Initialize encoder.encoders.1.self_attn.linear_v.bias to zeros
2025-04-17 13:03:48,509 - root - INFO - Initialize encoder.encoders.1.self_attn.linear_out.bias to zeros
2025-04-17 13:03:48,510 - root - INFO - Initialize encoder.encoders.1.feed_forward.w_1.bias to zeros
2025-04-17 13:03:48,510 - root - INFO - Initialize encoder.encoders.1.feed_forward.w_2.bias to zeros
2025-04-17 13:03:48,510 - root - INFO - Initialize encoder.encoders.1.norm1.bias to zeros
2025-04-17 13:03:48,510 - root - INFO - Initialize encoder.encoders.1.norm2.bias to zeros
2025-04-17 13:03:48,511 - root - INFO - Initialize encoder.encoders.2.self_attn.linear_q.bias to zeros
2025-04-17 13:03:48,511 - root - INFO - Initialize encoder.encoders.2.self_attn.linear_k.bias to zeros
2025-04-17 13:03:48,511 - root - INFO - Initialize encoder.encoders.2.self_attn.linear_v.bias to zeros
2025-04-17 13:03:48,511 - root - INFO - Initialize encoder.encoders.2.self_attn.linear_out.bias to zeros
2025-04-17 13:03:48,512 - root - INFO - Initialize encoder.encoders.2.feed_forward.w_1.bias to zeros
2025-04-17 13:03:48,512 - root - INFO - Initialize encoder.encoders.2.feed_forward.w_2.bias to zeros
2025-04-17 13:03:48,512 - root - INFO - Initialize encoder.encoders.2.norm1.bias to zeros
2025-04-17 13:03:48,512 - root - INFO - Initialize encoder.encoders.2.norm2.bias to zeros
2025-04-17 13:03:48,513 - root - INFO - Initialize encoder.encoders.3.self_attn.linear_q.bias to zeros
2025-04-17 13:03:48,513 - root - INFO - Initialize encoder.encoders.3.self_attn.linear_k.bias to zeros
2025-04-17 13:03:48,514 - root - INFO - Initialize encoder.encoders.3.self_attn.linear_v.bias to zeros
2025-04-17 13:03:48,515 - root - INFO - Initialize encoder.encoders.3.self_attn.linear_out.bias to zeros
2025-04-17 13:03:48,515 - root - INFO - Initialize encoder.encoders.3.feed_forward.w_1.bias to zeros
2025-04-17 13:03:48,515 - root - INFO - Initialize encoder.encoders.3.feed_forward.w_2.bias to zeros
2025-04-17 13:03:48,516 - root - INFO - Initialize encoder.encoders.3.norm1.bias to zeros
2025-04-17 13:03:48,518 - root - INFO - Initialize encoder.encoders.3.norm2.bias to zeros
2025-04-17 13:03:48,518 - root - INFO - Initialize encoder.encoders.4.self_attn.linear_q.bias to zeros
2025-04-17 13:03:48,518 - root - INFO - Initialize encoder.encoders.4.self_attn.linear_k.bias to zeros
2025-04-17 13:03:48,519 - root - INFO - Initialize encoder.encoders.4.self_attn.linear_v.bias to zeros
2025-04-17 13:03:48,519 - root - INFO - Initialize encoder.encoders.4.self_attn.linear_out.bias to zeros
2025-04-17 13:03:48,519 - root - INFO - Initialize encoder.encoders.4.feed_forward.w_1.bias to zeros
2025-04-17 13:03:48,520 - root - INFO - Initialize encoder.encoders.4.feed_forward.w_2.bias to zeros
2025-04-17 13:03:48,520 - root - INFO - Initialize encoder.encoders.4.norm1.bias to zeros
2025-04-17 13:03:48,520 - root - INFO - Initialize encoder.encoders.4.norm2.bias to zeros
2025-04-17 13:03:48,521 - root - INFO - Initialize encoder.encoders.5.self_attn.linear_q.bias to zeros
2025-04-17 13:03:48,521 - root - INFO - Initialize encoder.encoders.5.self_attn.linear_k.bias to zeros
2025-04-17 13:03:48,521 - root - INFO - Initialize encoder.encoders.5.self_attn.linear_v.bias to zeros
2025-04-17 13:03:48,522 - root - INFO - Initialize encoder.encoders.5.self_attn.linear_out.bias to zeros
2025-04-17 13:03:48,522 - root - INFO - Initialize encoder.encoders.5.feed_forward.w_1.bias to zeros
2025-04-17 13:03:48,522 - root - INFO - Initialize encoder.encoders.5.feed_forward.w_2.bias to zeros
2025-04-17 13:03:48,523 - root - INFO - Initialize encoder.encoders.5.norm1.bias to zeros
2025-04-17 13:03:48,523 - root - INFO - Initialize encoder.encoders.5.norm2.bias to zeros
2025-04-17 13:03:48,523 - root - INFO - Initialize encoder.encoders.6.self_attn.linear_q.bias to zeros
2025-04-17 13:03:48,524 - root - INFO - Initialize encoder.encoders.6.self_attn.linear_k.bias to zeros
2025-04-17 13:03:48,524 - root - INFO - Initialize encoder.encoders.6.self_attn.linear_v.bias to zeros
2025-04-17 13:03:48,524 - root - INFO - Initialize encoder.encoders.6.self_attn.linear_out.bias to zeros
2025-04-17 13:03:48,525 - root - INFO - Initialize encoder.encoders.6.feed_forward.w_1.bias to zeros
2025-04-17 13:03:48,525 - root - INFO - Initialize encoder.encoders.6.feed_forward.w_2.bias to zeros
2025-04-17 13:03:48,525 - root - INFO - Initialize encoder.encoders.6.norm1.bias to zeros
2025-04-17 13:03:48,526 - root - INFO - Initialize encoder.encoders.6.norm2.bias to zeros
2025-04-17 13:03:48,526 - root - INFO - Initialize encoder.encoders.7.self_attn.linear_q.bias to zeros
2025-04-17 13:03:48,526 - root - INFO - Initialize encoder.encoders.7.self_attn.linear_k.bias to zeros
2025-04-17 13:03:48,527 - root - INFO - Initialize encoder.encoders.7.self_attn.linear_v.bias to zeros
2025-04-17 13:03:48,527 - root - INFO - Initialize encoder.encoders.7.self_attn.linear_out.bias to zeros
2025-04-17 13:03:48,527 - root - INFO - Initialize encoder.encoders.7.feed_forward.w_1.bias to zeros
2025-04-17 13:03:48,527 - root - INFO - Initialize encoder.encoders.7.feed_forward.w_2.bias to zeros
2025-04-17 13:03:48,528 - root - INFO - Initialize encoder.encoders.7.norm1.bias to zeros
2025-04-17 13:03:48,528 - root - INFO - Initialize encoder.encoders.7.norm2.bias to zeros
2025-04-17 13:03:48,528 - root - INFO - Initialize encoder.encoders.8.self_attn.linear_q.bias to zeros
2025-04-17 13:03:48,529 - root - INFO - Initialize encoder.encoders.8.self_attn.linear_k.bias to zeros
2025-04-17 13:03:48,530 - root - INFO - Initialize encoder.encoders.8.self_attn.linear_v.bias to zeros
2025-04-17 13:03:48,530 - root - INFO - Initialize encoder.encoders.8.self_attn.linear_out.bias to zeros
2025-04-17 13:03:48,531 - root - INFO - Initialize encoder.encoders.8.feed_forward.w_1.bias to zeros
2025-04-17 13:03:48,531 - root - INFO - Initialize encoder.encoders.8.feed_forward.w_2.bias to zeros
2025-04-17 13:03:48,531 - root - INFO - Initialize encoder.encoders.8.norm1.bias to zeros
2025-04-17 13:03:48,532 - root - INFO - Initialize encoder.encoders.8.norm2.bias to zeros
2025-04-17 13:03:48,532 - root - INFO - Initialize encoder.encoders.9.self_attn.linear_q.bias to zeros
2025-04-17 13:03:48,532 - root - INFO - Initialize encoder.encoders.9.self_attn.linear_k.bias to zeros
2025-04-17 13:03:48,532 - root - INFO - Initialize encoder.encoders.9.self_attn.linear_v.bias to zeros
2025-04-17 13:03:48,533 - root - INFO - Initialize encoder.encoders.9.self_attn.linear_out.bias to zeros
2025-04-17 13:03:48,533 - root - INFO - Initialize encoder.encoders.9.feed_forward.w_1.bias to zeros
2025-04-17 13:03:48,533 - root - INFO - Initialize encoder.encoders.9.feed_forward.w_2.bias to zeros
2025-04-17 13:03:48,535 - root - INFO - Initialize encoder.encoders.9.norm1.bias to zeros
2025-04-17 13:03:48,536 - root - INFO - Initialize encoder.encoders.9.norm2.bias to zeros
2025-04-17 13:03:48,536 - root - INFO - Initialize encoder.encoders.10.self_attn.linear_q.bias to zeros
2025-04-17 13:03:48,536 - root - INFO - Initialize encoder.encoders.10.self_attn.linear_k.bias to zeros
2025-04-17 13:03:48,536 - root - INFO - Initialize encoder.encoders.10.self_attn.linear_v.bias to zeros
2025-04-17 13:03:48,537 - root - INFO - Initialize encoder.encoders.10.self_attn.linear_out.bias to zeros
2025-04-17 13:03:48,537 - root - INFO - Initialize encoder.encoders.10.feed_forward.w_1.bias to zeros
2025-04-17 13:03:48,537 - root - INFO - Initialize encoder.encoders.10.feed_forward.w_2.bias to zeros
2025-04-17 13:03:48,537 - root - INFO - Initialize encoder.encoders.10.norm1.bias to zeros
2025-04-17 13:03:48,538 - root - INFO - Initialize encoder.encoders.10.norm2.bias to zeros
2025-04-17 13:03:48,538 - root - INFO - Initialize encoder.encoders.11.self_attn.linear_q.bias to zeros
2025-04-17 13:03:48,538 - root - INFO - Initialize encoder.encoders.11.self_attn.linear_k.bias to zeros
2025-04-17 13:03:48,538 - root - INFO - Initialize encoder.encoders.11.self_attn.linear_v.bias to zeros
2025-04-17 13:03:48,539 - root - INFO - Initialize encoder.encoders.11.self_attn.linear_out.bias to zeros
2025-04-17 13:03:48,539 - root - INFO - Initialize encoder.encoders.11.feed_forward.w_1.bias to zeros
2025-04-17 13:03:48,539 - root - INFO - Initialize encoder.encoders.11.feed_forward.w_2.bias to zeros
2025-04-17 13:03:48,540 - root - INFO - Initialize encoder.encoders.11.norm1.bias to zeros
2025-04-17 13:03:48,540 - root - INFO - Initialize encoder.encoders.11.norm2.bias to zeros
2025-04-17 13:03:48,540 - root - INFO - Initialize encoder.encoders.12.self_attn.linear_q.bias to zeros
2025-04-17 13:03:48,540 - root - INFO - Initialize encoder.encoders.12.self_attn.linear_k.bias to zeros
2025-04-17 13:03:48,541 - root - INFO - Initialize encoder.encoders.12.self_attn.linear_v.bias to zeros
2025-04-17 13:03:48,541 - root - INFO - Initialize encoder.encoders.12.self_attn.linear_out.bias to zeros
2025-04-17 13:03:48,541 - root - INFO - Initialize encoder.encoders.12.feed_forward.w_1.bias to zeros
2025-04-17 13:03:48,541 - root - INFO - Initialize encoder.encoders.12.feed_forward.w_2.bias to zeros
2025-04-17 13:03:48,542 - root - INFO - Initialize encoder.encoders.12.norm1.bias to zeros
2025-04-17 13:03:48,542 - root - INFO - Initialize encoder.encoders.12.norm2.bias to zeros
2025-04-17 13:03:48,542 - root - INFO - Initialize encoder.encoders.13.self_attn.linear_q.bias to zeros
2025-04-17 13:03:48,545 - root - INFO - Initialize encoder.encoders.13.self_attn.linear_k.bias to zeros
2025-04-17 13:03:48,545 - root - INFO - Initialize encoder.encoders.13.self_attn.linear_v.bias to zeros
2025-04-17 13:03:48,545 - root - INFO - Initialize encoder.encoders.13.self_attn.linear_out.bias to zeros
2025-04-17 13:03:48,546 - root - INFO - Initialize encoder.encoders.13.feed_forward.w_1.bias to zeros
2025-04-17 13:03:48,546 - root - INFO - Initialize encoder.encoders.13.feed_forward.w_2.bias to zeros
2025-04-17 13:03:48,546 - root - INFO - Initialize encoder.encoders.13.norm1.bias to zeros
2025-04-17 13:03:48,546 - root - INFO - Initialize encoder.encoders.13.norm2.bias to zeros
2025-04-17 13:03:48,547 - root - INFO - Initialize encoder.encoders.14.self_attn.linear_q.bias to zeros
2025-04-17 13:03:48,547 - root - INFO - Initialize encoder.encoders.14.self_attn.linear_k.bias to zeros
2025-04-17 13:03:48,547 - root - INFO - Initialize encoder.encoders.14.self_attn.linear_v.bias to zeros
2025-04-17 13:03:48,548 - root - INFO - Initialize encoder.encoders.14.self_attn.linear_out.bias to zeros
2025-04-17 13:03:48,548 - root - INFO - Initialize encoder.encoders.14.feed_forward.w_1.bias to zeros
2025-04-17 13:03:48,548 - root - INFO - Initialize encoder.encoders.14.feed_forward.w_2.bias to zeros
2025-04-17 13:03:48,548 - root - INFO - Initialize encoder.encoders.14.norm1.bias to zeros
2025-04-17 13:03:48,549 - root - INFO - Initialize encoder.encoders.14.norm2.bias to zeros
2025-04-17 13:03:48,549 - root - INFO - Initialize encoder.encoders.15.self_attn.linear_q.bias to zeros
2025-04-17 13:03:48,549 - root - INFO - Initialize encoder.encoders.15.self_attn.linear_k.bias to zeros
2025-04-17 13:03:48,550 - root - INFO - Initialize encoder.encoders.15.self_attn.linear_v.bias to zeros
2025-04-17 13:03:48,550 - root - INFO - Initialize encoder.encoders.15.self_attn.linear_out.bias to zeros
2025-04-17 13:03:48,550 - root - INFO - Initialize encoder.encoders.15.feed_forward.w_1.bias to zeros
2025-04-17 13:03:48,550 - root - INFO - Initialize encoder.encoders.15.feed_forward.w_2.bias to zeros
2025-04-17 13:03:48,551 - root - INFO - Initialize encoder.encoders.15.norm1.bias to zeros
2025-04-17 13:03:48,551 - root - INFO - Initialize encoder.encoders.15.norm2.bias to zeros
2025-04-17 13:03:48,551 - root - INFO - Initialize encoder.encoders.16.self_attn.linear_q.bias to zeros
2025-04-17 13:03:48,552 - root - INFO - Initialize encoder.encoders.16.self_attn.linear_k.bias to zeros
2025-04-17 13:03:48,552 - root - INFO - Initialize encoder.encoders.16.self_attn.linear_v.bias to zeros
2025-04-17 13:03:48,552 - root - INFO - Initialize encoder.encoders.16.self_attn.linear_out.bias to zeros
2025-04-17 13:03:48,553 - root - INFO - Initialize encoder.encoders.16.feed_forward.w_1.bias to zeros
2025-04-17 13:03:48,555 - root - INFO - Initialize encoder.encoders.16.feed_forward.w_2.bias to zeros
2025-04-17 13:03:48,555 - root - INFO - Initialize encoder.encoders.16.norm1.bias to zeros
2025-04-17 13:03:48,555 - root - INFO - Initialize encoder.encoders.16.norm2.bias to zeros
2025-04-17 13:03:48,559 - root - INFO - Initialize encoder.encoders.17.self_attn.linear_q.bias to zeros
2025-04-17 13:03:48,559 - root - INFO - Initialize encoder.encoders.17.self_attn.linear_k.bias to zeros
2025-04-17 13:03:48,559 - root - INFO - Initialize encoder.encoders.17.self_attn.linear_v.bias to zeros
2025-04-17 13:03:48,560 - root - INFO - Initialize encoder.encoders.17.self_attn.linear_out.bias to zeros
2025-04-17 13:03:48,560 - root - INFO - Initialize encoder.encoders.17.feed_forward.w_1.bias to zeros
2025-04-17 13:03:48,561 - root - INFO - Initialize encoder.encoders.17.feed_forward.w_2.bias to zeros
2025-04-17 13:03:48,561 - root - INFO - Initialize encoder.encoders.17.norm1.bias to zeros
2025-04-17 13:03:48,561 - root - INFO - Initialize encoder.encoders.17.norm2.bias to zeros
2025-04-17 13:03:48,561 - root - INFO - Initialize encoder.after_norm.bias to zeros
2025-04-17 13:03:48,562 - root - INFO - Initialize decoder.after_norm.bias to zeros
2025-04-17 13:03:48,562 - root - INFO - Initialize decoder.output_layer.bias to zeros
2025-04-17 13:03:48,562 - root - INFO - Initialize decoder.decoders.0.self_attn.linear_q.bias to zeros
2025-04-17 13:03:48,563 - root - INFO - Initialize decoder.decoders.0.self_attn.linear_k.bias to zeros
2025-04-17 13:03:48,563 - root - INFO - Initialize decoder.decoders.0.self_attn.linear_v.bias to zeros
2025-04-17 13:03:48,564 - root - INFO - Initialize decoder.decoders.0.self_attn.linear_out.bias to zeros
2025-04-17 13:03:48,564 - root - INFO - Initialize decoder.decoders.0.src_attn.linear_q.bias to zeros
2025-04-17 13:03:48,564 - root - INFO - Initialize decoder.decoders.0.src_attn.linear_k.bias to zeros
2025-04-17 13:03:48,566 - root - INFO - Initialize decoder.decoders.0.src_attn.linear_v.bias to zeros
2025-04-17 13:03:48,566 - root - INFO - Initialize decoder.decoders.0.src_attn.linear_out.bias to zeros
2025-04-17 13:03:48,566 - root - INFO - Initialize decoder.decoders.0.feed_forward.w_1.bias to zeros
2025-04-17 13:03:48,569 - root - INFO - Initialize decoder.decoders.0.feed_forward.w_2.bias to zeros
2025-04-17 13:03:48,569 - root - INFO - Initialize decoder.decoders.0.norm1.bias to zeros
2025-04-17 13:03:48,570 - root - INFO - Initialize decoder.decoders.0.norm2.bias to zeros
2025-04-17 13:03:48,570 - root - INFO - Initialize decoder.decoders.0.norm3.bias to zeros
2025-04-17 13:03:48,570 - root - INFO - Initialize decoder.decoders.1.self_attn.linear_q.bias to zeros
2025-04-17 13:03:48,571 - root - INFO - Initialize decoder.decoders.1.self_attn.linear_k.bias to zeros
2025-04-17 13:03:48,571 - root - INFO - Initialize decoder.decoders.1.self_attn.linear_v.bias to zeros
2025-04-17 13:03:48,571 - root - INFO - Initialize decoder.decoders.1.self_attn.linear_out.bias to zeros
2025-04-17 13:03:48,572 - root - INFO - Initialize decoder.decoders.1.src_attn.linear_q.bias to zeros
2025-04-17 13:03:48,573 - root - INFO - Initialize decoder.decoders.1.src_attn.linear_k.bias to zeros
2025-04-17 13:03:48,573 - root - INFO - Initialize decoder.decoders.1.src_attn.linear_v.bias to zeros
2025-04-17 13:03:48,573 - root - INFO - Initialize decoder.decoders.1.src_attn.linear_out.bias to zeros
2025-04-17 13:03:48,574 - root - INFO - Initialize decoder.decoders.1.feed_forward.w_1.bias to zeros
2025-04-17 13:03:48,574 - root - INFO - Initialize decoder.decoders.1.feed_forward.w_2.bias to zeros
2025-04-17 13:03:48,574 - root - INFO - Initialize decoder.decoders.1.norm1.bias to zeros
2025-04-17 13:03:48,575 - root - INFO - Initialize decoder.decoders.1.norm2.bias to zeros
2025-04-17 13:03:48,578 - root - INFO - Initialize decoder.decoders.1.norm3.bias to zeros
2025-04-17 13:03:48,578 - root - INFO - Initialize decoder.decoders.2.self_attn.linear_q.bias to zeros
2025-04-17 13:03:48,579 - root - INFO - Initialize decoder.decoders.2.self_attn.linear_k.bias to zeros
2025-04-17 13:03:48,579 - root - INFO - Initialize decoder.decoders.2.self_attn.linear_v.bias to zeros
2025-04-17 13:03:48,579 - root - INFO - Initialize decoder.decoders.2.self_attn.linear_out.bias to zeros
2025-04-17 13:03:48,580 - root - INFO - Initialize decoder.decoders.2.src_attn.linear_q.bias to zeros
2025-04-17 13:03:48,580 - root - INFO - Initialize decoder.decoders.2.src_attn.linear_k.bias to zeros
2025-04-17 13:03:48,580 - root - INFO - Initialize decoder.decoders.2.src_attn.linear_v.bias to zeros
2025-04-17 13:03:48,581 - root - INFO - Initialize decoder.decoders.2.src_attn.linear_out.bias to zeros
2025-04-17 13:03:48,581 - root - INFO - Initialize decoder.decoders.2.feed_forward.w_1.bias to zeros
2025-04-17 13:03:48,581 - root - INFO - Initialize decoder.decoders.2.feed_forward.w_2.bias to zeros
2025-04-17 13:03:48,582 - root - INFO - Initialize decoder.decoders.2.norm1.bias to zeros
2025-04-17 13:03:48,582 - root - INFO - Initialize decoder.decoders.2.norm2.bias to zeros
2025-04-17 13:03:48,582 - root - INFO - Initialize decoder.decoders.2.norm3.bias to zeros
2025-04-17 13:03:48,583 - root - INFO - Initialize decoder.decoders.3.self_attn.linear_q.bias to zeros
2025-04-17 13:03:48,583 - root - INFO - Initialize decoder.decoders.3.self_attn.linear_k.bias to zeros
2025-04-17 13:03:48,584 - root - INFO - Initialize decoder.decoders.3.self_attn.linear_v.bias to zeros
2025-04-17 13:03:48,585 - root - INFO - Initialize decoder.decoders.3.self_attn.linear_out.bias to zeros
2025-04-17 13:03:48,585 - root - INFO - Initialize decoder.decoders.3.src_attn.linear_q.bias to zeros
2025-04-17 13:03:48,586 - root - INFO - Initialize decoder.decoders.3.src_attn.linear_k.bias to zeros
2025-04-17 13:03:48,586 - root - INFO - Initialize decoder.decoders.3.src_attn.linear_v.bias to zeros
2025-04-17 13:03:48,587 - root - INFO - Initialize decoder.decoders.3.src_attn.linear_out.bias to zeros
2025-04-17 13:03:48,587 - root - INFO - Initialize decoder.decoders.3.feed_forward.w_1.bias to zeros
2025-04-17 13:03:48,587 - root - INFO - Initialize decoder.decoders.3.feed_forward.w_2.bias to zeros
2025-04-17 13:03:48,589 - root - INFO - Initialize decoder.decoders.3.norm1.bias to zeros
2025-04-17 13:03:48,590 - root - INFO - Initialize decoder.decoders.3.norm2.bias to zeros
2025-04-17 13:03:48,590 - root - INFO - Initialize decoder.decoders.3.norm3.bias to zeros
2025-04-17 13:03:48,591 - root - INFO - Initialize decoder.decoders.4.self_attn.linear_q.bias to zeros
2025-04-17 13:03:48,591 - root - INFO - Initialize decoder.decoders.4.self_attn.linear_k.bias to zeros
2025-04-17 13:03:48,591 - root - INFO - Initialize decoder.decoders.4.self_attn.linear_v.bias to zeros
2025-04-17 13:03:48,592 - root - INFO - Initialize decoder.decoders.4.self_attn.linear_out.bias to zeros
2025-04-17 13:03:48,592 - root - INFO - Initialize decoder.decoders.4.src_attn.linear_q.bias to zeros
2025-04-17 13:03:48,595 - root - INFO - Initialize decoder.decoders.4.src_attn.linear_k.bias to zeros
2025-04-17 13:03:48,596 - root - INFO - Initialize decoder.decoders.4.src_attn.linear_v.bias to zeros
2025-04-17 13:03:48,596 - root - INFO - Initialize decoder.decoders.4.src_attn.linear_out.bias to zeros
2025-04-17 13:03:48,596 - root - INFO - Initialize decoder.decoders.4.feed_forward.w_1.bias to zeros
2025-04-17 13:03:48,597 - root - INFO - Initialize decoder.decoders.4.feed_forward.w_2.bias to zeros
2025-04-17 13:03:48,597 - root - INFO - Initialize decoder.decoders.4.norm1.bias to zeros
2025-04-17 13:03:48,598 - root - INFO - Initialize decoder.decoders.4.norm2.bias to zeros
2025-04-17 13:03:48,598 - root - INFO - Initialize decoder.decoders.4.norm3.bias to zeros
2025-04-17 13:03:48,598 - root - INFO - Initialize decoder.decoders.5.self_attn.linear_q.bias to zeros
2025-04-17 13:03:48,598 - root - INFO - Initialize decoder.decoders.5.self_attn.linear_k.bias to zeros
2025-04-17 13:03:48,599 - root - INFO - Initialize decoder.decoders.5.self_attn.linear_v.bias to zeros
2025-04-17 13:03:48,599 - root - INFO - Initialize decoder.decoders.5.self_attn.linear_out.bias to zeros
2025-04-17 13:03:48,599 - root - INFO - Initialize decoder.decoders.5.src_attn.linear_q.bias to zeros
2025-04-17 13:03:48,600 - root - INFO - Initialize decoder.decoders.5.src_attn.linear_k.bias to zeros
2025-04-17 13:03:48,600 - root - INFO - Initialize decoder.decoders.5.src_attn.linear_v.bias to zeros
2025-04-17 13:03:48,600 - root - INFO - Initialize decoder.decoders.5.src_attn.linear_out.bias to zeros
2025-04-17 13:03:48,601 - root - INFO - Initialize decoder.decoders.5.feed_forward.w_1.bias to zeros
2025-04-17 13:03:48,602 - root - INFO - Initialize decoder.decoders.5.feed_forward.w_2.bias to zeros
2025-04-17 13:03:48,602 - root - INFO - Initialize decoder.decoders.5.norm1.bias to zeros
2025-04-17 13:03:48,604 - root - INFO - Initialize decoder.decoders.5.norm2.bias to zeros
2025-04-17 13:03:48,604 - root - INFO - Initialize decoder.decoders.5.norm3.bias to zeros
2025-04-17 13:03:48,605 - root - INFO - Initialize ctc.ctc_lo.bias to zeros
2025-04-17 13:03:49,048 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--espnet--shinji-watanabe-librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best/snapshots/bc6bbd771cec698f070640ee677a66719181f0a2/exp/lm_train_lm_adam_bpe/config.yaml
2025-04-17 13:03:49,864 - root - INFO - Vocabulary size: 5000
2025-04-17 13:03:50,760 - espnet2.bin.asr_inference - INFO - BatchBeamSearch implementation is selected.
2025-04-17 13:03:50,775 - espnet2.bin.asr_inference - INFO - Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (lm): SequentialRNNLM(
      (drop): Dropout(p=0.0, inplace=False)
      (encoder): Embedding(5000, 2048, padding_idx=0)
      (rnn): LSTM(2048, 2048, num_layers=4, batch_first=True)
      (decoder): Linear(in_features=2048, out_features=5000, bias=True)
    )
  )
)
2025-04-17 13:03:50,775 - espnet2.bin.asr_inference - INFO - Decoding device=cuda, dtype=float32
2025-04-17 13:03:50,778 - espnet2.bin.asr_inference - INFO - Text tokenizer: SentencepiecesTokenizer(model="data/token_list/bpe_unigram5000/model.model")
2025-04-17 13:03:50,779 - asr_multi - INFO - Successfully loaded model: TransformerE18
2025-04-17 13:03:50,779 - asr_multi - INFO - Loaded ASR models: ['Conformer6', 'TransformerE18']
2025-04-17 13:03:50,899 - asr_multi - INFO - Launching Gradio app…
2025-04-17 13:04:09,737 - asr_multi - INFO - Audio in: sr=48000, length=2.88 s, peak=2085806976.00
2025-04-17 13:04:09,738 - asr_multi - INFO - Normalized (peak=2085806976.00)
2025-04-17 13:04:11,098 - asr_multi - INFO - Saved debug_samples/input_20250417_130411.wav (2.88 s)
2025-04-17 13:04:11,099 - espnet2.bin.asr_inference - INFO - speech length: 46080
2025-04-17 13:04:11,412 - espnet.nets.beam_search - INFO - decoder input length: 44
2025-04-17 13:04:11,413 - espnet.nets.beam_search - INFO - max output length: 44
2025-04-17 13:04:11,413 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 13:04:12,014 - espnet.nets.beam_search - INFO - end detected at 16
2025-04-17 13:04:12,014 - espnet.nets.beam_search - INFO -  -7.17 * 0.7 =  -5.02 for decoder
2025-04-17 13:04:12,015 - espnet.nets.beam_search - INFO -  -8.96 * 0.3 =  -2.69 for ctc
2025-04-17 13:04:12,015 - espnet.nets.beam_search - INFO - -37.61 * 1.0 = -37.61 for lm
2025-04-17 13:04:12,015 - espnet.nets.beam_search - INFO - total log probability: -45.31
2025-04-17 13:04:12,016 - espnet.nets.beam_search - INFO - normalized log probability: -3.78
2025-04-17 13:04:12,016 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 26
2025-04-17 13:04:12,017 - espnet.nets.beam_search - INFO - best hypo: ▁HELLO▁MY▁NAME▁IS▁UMBUG

2025-04-17 13:04:12,024 - asr_multi - INFO - Recognized: HELLO MY NAME IS UMBUG
2025-04-17 13:04:26,390 - asr_multi - INFO - Audio in: sr=48000, length=2.88 s, peak=2085806976.00
2025-04-17 13:04:26,390 - asr_multi - INFO - Normalized (peak=2085806976.00)
2025-04-17 13:04:26,394 - asr_multi - INFO - Saved debug_samples/input_20250417_130426.wav (2.88 s)
2025-04-17 13:04:26,395 - espnet2.bin.asr_inference - INFO - speech length: 46080
2025-04-17 13:04:26,422 - espnet.nets.beam_search - INFO - decoder input length: 59
2025-04-17 13:04:26,422 - espnet.nets.beam_search - INFO - max output length: 59
2025-04-17 13:04:26,422 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 13:04:26,793 - espnet.nets.beam_search - INFO - end detected at 16
2025-04-17 13:04:26,794 - espnet.nets.beam_search - INFO -  -9.80 * 0.7 =  -6.86 for decoder
2025-04-17 13:04:26,794 - espnet.nets.beam_search - INFO - -21.36 * 0.3 =  -6.41 for ctc
2025-04-17 13:04:26,794 - espnet.nets.beam_search - INFO - -34.16 * 1.0 = -34.16 for lm
2025-04-17 13:04:26,795 - espnet.nets.beam_search - INFO - total log probability: -47.42
2025-04-17 13:04:26,795 - espnet.nets.beam_search - INFO - normalized log probability: -3.95
2025-04-17 13:04:26,795 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 29
2025-04-17 13:04:26,796 - espnet.nets.beam_search - INFO - best hypo: ▁HELLO▁MY▁NAME▁IS▁AMBUSH

2025-04-17 13:04:26,796 - asr_multi - ERROR - ASR error Not found: "data/token_list/bpe_unigram5000/model.model": No such file or directory Error #2
Traceback (most recent call last):
  File "/home/ambuja/emo-tts/src/gradio_app.py", line 246, in transcribe_and_speak
    nbests = model(wav)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet2/bin/asr_inference.py", line 559, in __call__
    results = self._decode_single_sample(enc[0])
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet2/bin/asr_inference.py", line 676, in _decode_single_sample
    text = self.tokenizer.tokens2text(token)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet2/text/sentencepiece_tokenizer.py", line 36, in tokens2text
    self._build_sentence_piece_processor()
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet2/text/sentencepiece_tokenizer.py", line 29, in _build_sentence_piece_processor
    self.sp.load(self.model)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/sentencepiece/__init__.py", line 961, in Load
    return self.LoadFromFile(model_file)
  File "/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/sentencepiece/__init__.py", line 316, in LoadFromFile
    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)
OSError: Not found: "data/token_list/bpe_unigram5000/model.model": No such file or directory Error #2
2025-04-17 13:19:31,588 - asr_multi - INFO - Starting multi-model ASR/TTS demo
2025-04-17 13:19:31,588 - asr_multi - INFO - Using device: cuda
2025-04-17 13:19:31,596 - asr_multi - INFO - Downloading ASR model 'Conformer6' → kamo-naoyuki/librispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp_valid.acc.ave
2025-04-17 13:19:42,767 - asr_multi - INFO - Model directory for Conformer6: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp
2025-04-17 13:19:42,767 - asr_multi - INFO -   asr_train_config: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp/config.yaml
2025-04-17 13:19:42,768 - asr_multi - INFO -   lm_train_config: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/lm_train_lm_transformer2_bpe5000_scheduler_confwarmup_steps25000_batch_bins500000000_accum_grad2_use_amptrue/config.yaml
2025-04-17 13:19:42,768 - asr_multi - INFO -   asr_model_file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp/valid.acc.ave_10best.pth
2025-04-17 13:19:42,768 - asr_multi - INFO -   lm_file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/lm_train_lm_transformer2_bpe5000_scheduler_confwarmup_steps25000_batch_bins500000000_accum_grad2_use_amptrue/valid.loss.ave_10best.pth
2025-04-17 13:19:42,770 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp/config.yaml
2025-04-17 13:19:43,642 - root - INFO - Vocabulary size: 5000
2025-04-17 13:19:43,718 - root - WARNING - Using legacy_rel_pos and it will be deprecated in the future.
2025-04-17 13:19:43,778 - root - WARNING - Using legacy_rel_selfattn and it will be deprecated in the future.
2025-04-17 13:19:44,255 - root - INFO - Gradient checkpoint layers: []
2025-04-17 13:19:45,645 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/lm_train_lm_transformer2_bpe5000_scheduler_confwarmup_steps25000_batch_bins500000000_accum_grad2_use_amptrue/config.yaml
2025-04-17 13:19:46,472 - root - INFO - Vocabulary size: 5000
2025-04-17 13:19:46,475 - root - INFO - encoder self-attention layer type = self-attention
2025-04-17 13:19:47,131 - espnet2.bin.asr_inference - INFO - BatchBeamSearch implementation is selected.
2025-04-17 13:19:47,161 - espnet2.bin.asr_inference - INFO - Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (lm): TransformerLM(
      (embed): Embedding(5000, 128)
      (encoder): Encoder(
        (embed): Sequential(
          (0): Linear(in_features=128, out_features=512, bias=True)
          (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): Dropout(p=0.0, inplace=False)
          (3): ReLU()
          (4): Sequential()
        )
        (encoders): MultiSequential(
          (0): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (4): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (5): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (6): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (7): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (8): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (9): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (10): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (11): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (12): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (13): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (14): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (15): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): Linear(in_features=512, out_features=5000, bias=True)
    )
  )
)
2025-04-17 13:19:47,163 - espnet2.bin.asr_inference - INFO - Decoding device=cuda, dtype=float32
2025-04-17 13:19:47,166 - espnet2.bin.asr_inference - INFO - Text tokenizer: SentencepiecesTokenizer(model="/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/data/en_token_list/bpe_unigram5000/bpe.model")
2025-04-17 13:19:47,166 - asr_multi - INFO - Successfully loaded model: Conformer6
2025-04-17 13:19:47,191 - asr_multi - INFO - Downloading ASR model 'Conformer8' → pyf98/librispeech_conformer
2025-04-17 13:20:00,008 - asr_multi - INFO - Model directory for Conformer8: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--pyf98--librispeech_conformer/snapshots/b74448c3b089eceea913735bd59a76d5252d8eec/exp/asr_train_asr_conformer8_raw_en_bpe5000_sp
2025-04-17 13:20:00,009 - asr_multi - INFO -   asr_train_config: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--pyf98--librispeech_conformer/snapshots/b74448c3b089eceea913735bd59a76d5252d8eec/exp/asr_train_asr_conformer8_raw_en_bpe5000_sp/config.yaml
2025-04-17 13:20:00,009 - asr_multi - INFO -   asr_model_file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--pyf98--librispeech_conformer/snapshots/b74448c3b089eceea913735bd59a76d5252d8eec/exp/asr_train_asr_conformer8_raw_en_bpe5000_sp/valid.acc.ave.pth
2025-04-17 13:20:00,011 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--pyf98--librispeech_conformer/snapshots/b74448c3b089eceea913735bd59a76d5252d8eec/exp/asr_train_asr_conformer8_raw_en_bpe5000_sp/config.yaml
2025-04-17 13:20:00,854 - root - INFO - Vocabulary size: 5000
2025-04-17 13:20:01,287 - root - INFO - Gradient checkpoint layers: []
2025-04-17 13:20:02,565 - espnet2.bin.asr_inference - INFO - BatchBeamSearch implementation is selected.
2025-04-17 13:20:02,579 - espnet2.bin.asr_inference - INFO - Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2025-04-17 13:20:02,583 - espnet2.bin.asr_inference - INFO - Decoding device=cuda, dtype=float32
2025-04-17 13:20:02,586 - espnet2.bin.asr_inference - INFO - Text tokenizer: SentencepiecesTokenizer(model="/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--pyf98--librispeech_conformer/snapshots/b74448c3b089eceea913735bd59a76d5252d8eec/data/en_token_list/bpe_unigram5000/bpe.model")
2025-04-17 13:20:02,586 - asr_multi - INFO - Successfully loaded model: Conformer8
2025-04-17 13:20:02,586 - asr_multi - INFO - Loaded ASR models: ['Conformer6', 'Conformer8']
2025-04-17 13:20:02,713 - asr_multi - INFO - Launching Gradio app…
2025-04-17 13:21:27,236 - asr_multi - INFO - Audio in: sr=48000, length=3.66 s, peak=1125288832.00
2025-04-17 13:21:27,237 - asr_multi - INFO - Normalized (peak=1125288832.00)
2025-04-17 13:21:28,512 - asr_multi - INFO - Saved debug_samples/input_20250417_132128.wav (3.66 s)
2025-04-17 13:21:28,513 - espnet2.bin.asr_inference - INFO - speech length: 58560
2025-04-17 13:21:28,847 - espnet.nets.beam_search - INFO - decoder input length: 56
2025-04-17 13:21:28,847 - espnet.nets.beam_search - INFO - max output length: 56
2025-04-17 13:21:28,848 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 13:21:29,399 - espnet.nets.beam_search - INFO - end detected at 14
2025-04-17 13:21:29,400 - espnet.nets.beam_search - INFO - -11.73 * 0.7 =  -8.21 for decoder
2025-04-17 13:21:29,400 - espnet.nets.beam_search - INFO -  -7.92 * 0.3 =  -2.38 for ctc
2025-04-17 13:21:29,401 - espnet.nets.beam_search - INFO - -30.67 * 1.0 = -30.67 for lm
2025-04-17 13:21:29,401 - espnet.nets.beam_search - INFO - total log probability: -41.26
2025-04-17 13:21:29,401 - espnet.nets.beam_search - INFO - normalized log probability: -4.13
2025-04-17 13:21:29,402 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 19
2025-04-17 13:21:29,402 - espnet.nets.beam_search - INFO - best hypo: ▁HELLO▁MY▁NAME▁IS▁GARRY

2025-04-17 13:21:29,409 - asr_multi - INFO - Recognized: HELLO MY NAME IS GARRY
2025-04-17 13:21:37,350 - asr_multi - INFO - Audio in: sr=48000, length=3.66 s, peak=1125288832.00
2025-04-17 13:21:37,350 - asr_multi - INFO - Normalized (peak=1125288832.00)
2025-04-17 13:21:37,355 - asr_multi - INFO - Saved debug_samples/input_20250417_132137.wav (3.66 s)
2025-04-17 13:21:37,356 - espnet2.bin.asr_inference - INFO - speech length: 58560
2025-04-17 13:21:37,388 - espnet.nets.beam_search - INFO - decoder input length: 56
2025-04-17 13:21:37,388 - espnet.nets.beam_search - INFO - max output length: 56
2025-04-17 13:21:37,388 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 13:21:37,716 - espnet.nets.beam_search - INFO - end detected at 16
2025-04-17 13:21:37,717 - espnet.nets.beam_search - INFO -  -3.71 * 0.7 =  -2.60 for decoder
2025-04-17 13:21:37,717 - espnet.nets.beam_search - INFO -  -4.29 * 0.3 =  -1.29 for ctc
2025-04-17 13:21:37,717 - espnet.nets.beam_search - INFO - total log probability: -3.88
2025-04-17 13:21:37,718 - espnet.nets.beam_search - INFO - normalized log probability: -0.39
2025-04-17 13:21:37,718 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 43
2025-04-17 13:21:37,718 - espnet.nets.beam_search - INFO - best hypo: ▁HALLO▁MY▁NAME▁IS▁GARRIMA

2025-04-17 13:21:37,726 - asr_multi - INFO - Recognized: HALLO MY NAME IS GARRIMA
2025-04-17 13:30:43,375 - asr_multi - INFO - Starting multi-model ASR/TTS demo
2025-04-17 13:30:43,375 - asr_multi - INFO - Using device: cuda
2025-04-17 13:30:43,383 - asr_multi - INFO - Downloading ASR model 'Conformer6' → kamo-naoyuki/librispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp_valid.acc.ave
2025-04-17 13:30:54,173 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp/config.yaml
2025-04-17 13:30:55,016 - root - INFO - Vocabulary size: 5000
2025-04-17 13:30:55,083 - root - WARNING - Using legacy_rel_pos and it will be deprecated in the future.
2025-04-17 13:30:55,142 - root - WARNING - Using legacy_rel_selfattn and it will be deprecated in the future.
2025-04-17 13:30:55,641 - root - INFO - Gradient checkpoint layers: []
2025-04-17 13:30:56,533 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/lm_train_lm_transformer2_bpe5000_scheduler_confwarmup_steps25000_batch_bins500000000_accum_grad2_use_amptrue/config.yaml
2025-04-17 13:30:57,350 - root - INFO - Vocabulary size: 5000
2025-04-17 13:30:57,354 - root - INFO - encoder self-attention layer type = self-attention
2025-04-17 13:30:58,001 - espnet2.bin.asr_inference - INFO - BatchBeamSearch implementation is selected.
2025-04-17 13:30:58,031 - espnet2.bin.asr_inference - INFO - Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (lm): TransformerLM(
      (embed): Embedding(5000, 128)
      (encoder): Encoder(
        (embed): Sequential(
          (0): Linear(in_features=128, out_features=512, bias=True)
          (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): Dropout(p=0.0, inplace=False)
          (3): ReLU()
          (4): Sequential()
        )
        (encoders): MultiSequential(
          (0): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (4): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (5): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (6): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (7): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (8): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (9): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (10): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (11): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (12): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (13): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (14): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (15): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): Linear(in_features=512, out_features=5000, bias=True)
    )
  )
)
2025-04-17 13:30:58,032 - espnet2.bin.asr_inference - INFO - Decoding device=cuda, dtype=float32
2025-04-17 13:30:58,036 - espnet2.bin.asr_inference - INFO - Text tokenizer: SentencepiecesTokenizer(model="/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/data/en_token_list/bpe_unigram5000/bpe.model")
2025-04-17 13:30:58,038 - asr_multi - INFO - Loaded ESPnet ASR model: Conformer6
2025-04-17 13:30:58,039 - asr_multi - INFO - Downloading ASR model 'Conformer8' → pyf98/librispeech_conformer
2025-04-17 13:30:58,120 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--pyf98--librispeech_conformer/snapshots/b74448c3b089eceea913735bd59a76d5252d8eec/exp/asr_train_asr_conformer8_raw_en_bpe5000_sp/config.yaml
2025-04-17 13:30:58,981 - root - INFO - Vocabulary size: 5000
2025-04-17 13:30:59,396 - root - INFO - Gradient checkpoint layers: []
2025-04-17 13:30:59,956 - espnet2.bin.asr_inference - INFO - BatchBeamSearch implementation is selected.
2025-04-17 13:30:59,968 - espnet2.bin.asr_inference - INFO - Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2025-04-17 13:30:59,969 - espnet2.bin.asr_inference - INFO - Decoding device=cuda, dtype=float32
2025-04-17 13:30:59,971 - espnet2.bin.asr_inference - INFO - Text tokenizer: SentencepiecesTokenizer(model="/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--pyf98--librispeech_conformer/snapshots/b74448c3b089eceea913735bd59a76d5252d8eec/data/en_token_list/bpe_unigram5000/bpe.model")
2025-04-17 13:30:59,971 - asr_multi - INFO - Loaded ESPnet ASR model: Conformer8
2025-04-17 13:30:59,975 - asr_multi - INFO - Loading Whisper large as gold ASR…
2025-04-17 13:31:58,835 - asr_multi - INFO - Launching Gradio app…
2025-04-17 13:32:29,089 - asr_multi - INFO - Saved sample debug_samples/input_20250417_133229.wav (3.12 s)
2025-04-17 13:32:29,090 - espnet2.bin.asr_inference - INFO - speech length: 49920
2025-04-17 13:32:29,411 - espnet.nets.beam_search - INFO - decoder input length: 48
2025-04-17 13:32:29,411 - espnet.nets.beam_search - INFO - max output length: 48
2025-04-17 13:32:29,412 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 13:32:29,925 - espnet.nets.beam_search - INFO - end detected at 13
2025-04-17 13:32:29,926 - espnet.nets.beam_search - INFO - -10.93 * 0.7 =  -7.65 for decoder
2025-04-17 13:32:29,926 - espnet.nets.beam_search - INFO -  -7.82 * 0.3 =  -2.35 for ctc
2025-04-17 13:32:29,926 - espnet.nets.beam_search - INFO - -26.35 * 1.0 = -26.35 for lm
2025-04-17 13:32:29,926 - espnet.nets.beam_search - INFO - total log probability: -36.34
2025-04-17 13:32:29,927 - espnet.nets.beam_search - INFO - normalized log probability: -4.54
2025-04-17 13:32:29,927 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 21
2025-04-17 13:32:29,927 - espnet.nets.beam_search - INFO - best hypo: ▁MY▁NAME▁IS▁GUTTY

2025-04-17 13:34:15,129 - asr_multi - INFO - Starting multi-model ASR/TTS demo
2025-04-17 13:34:15,129 - asr_multi - INFO - Using device: cuda
2025-04-17 13:34:15,137 - asr_multi - INFO - Downloading ASR model 'Conformer6' → kamo-naoyuki/librispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp_valid.acc.ave
2025-04-17 13:34:26,588 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp/config.yaml
2025-04-17 13:34:27,428 - root - INFO - Vocabulary size: 5000
2025-04-17 13:34:27,496 - root - WARNING - Using legacy_rel_pos and it will be deprecated in the future.
2025-04-17 13:34:27,553 - root - WARNING - Using legacy_rel_selfattn and it will be deprecated in the future.
2025-04-17 13:34:28,034 - root - INFO - Gradient checkpoint layers: []
2025-04-17 13:34:29,449 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/lm_train_lm_transformer2_bpe5000_scheduler_confwarmup_steps25000_batch_bins500000000_accum_grad2_use_amptrue/config.yaml
2025-04-17 13:34:30,260 - root - INFO - Vocabulary size: 5000
2025-04-17 13:34:30,264 - root - INFO - encoder self-attention layer type = self-attention
2025-04-17 13:34:30,913 - espnet2.bin.asr_inference - INFO - BatchBeamSearch implementation is selected.
2025-04-17 13:34:30,943 - espnet2.bin.asr_inference - INFO - Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (lm): TransformerLM(
      (embed): Embedding(5000, 128)
      (encoder): Encoder(
        (embed): Sequential(
          (0): Linear(in_features=128, out_features=512, bias=True)
          (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): Dropout(p=0.0, inplace=False)
          (3): ReLU()
          (4): Sequential()
        )
        (encoders): MultiSequential(
          (0): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (4): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (5): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (6): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (7): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (8): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (9): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (10): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (11): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (12): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (13): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (14): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (15): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): Linear(in_features=512, out_features=5000, bias=True)
    )
  )
)
2025-04-17 13:34:30,945 - espnet2.bin.asr_inference - INFO - Decoding device=cuda, dtype=float32
2025-04-17 13:34:30,948 - espnet2.bin.asr_inference - INFO - Text tokenizer: SentencepiecesTokenizer(model="/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/data/en_token_list/bpe_unigram5000/bpe.model")
2025-04-17 13:34:30,976 - asr_multi - INFO - Loaded ESPnet ASR model: Conformer6
2025-04-17 13:34:30,976 - asr_multi - INFO - Downloading ASR model 'Conformer8' → pyf98/librispeech_conformer
2025-04-17 13:34:31,071 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--pyf98--librispeech_conformer/snapshots/b74448c3b089eceea913735bd59a76d5252d8eec/exp/asr_train_asr_conformer8_raw_en_bpe5000_sp/config.yaml
2025-04-17 13:34:31,905 - root - INFO - Vocabulary size: 5000
2025-04-17 13:34:32,323 - root - INFO - Gradient checkpoint layers: []
2025-04-17 13:34:32,949 - espnet2.bin.asr_inference - INFO - BatchBeamSearch implementation is selected.
2025-04-17 13:34:32,961 - espnet2.bin.asr_inference - INFO - Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2025-04-17 13:34:32,962 - espnet2.bin.asr_inference - INFO - Decoding device=cuda, dtype=float32
2025-04-17 13:34:32,964 - espnet2.bin.asr_inference - INFO - Text tokenizer: SentencepiecesTokenizer(model="/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--pyf98--librispeech_conformer/snapshots/b74448c3b089eceea913735bd59a76d5252d8eec/data/en_token_list/bpe_unigram5000/bpe.model")
2025-04-17 13:34:32,965 - asr_multi - INFO - Loaded ESPnet ASR model: Conformer8
2025-04-17 13:34:32,965 - asr_multi - INFO - Loading Whisper large as gold ASR…
2025-04-17 13:34:50,797 - asr_multi - INFO - Launching Gradio app…
2025-04-17 13:35:15,764 - asr_multi - INFO - Saved sample debug_samples/input_20250417_133515.wav (3.66 s)
2025-04-17 13:35:15,765 - espnet2.bin.asr_inference - INFO - speech length: 58560
2025-04-17 13:35:16,095 - espnet.nets.beam_search - INFO - decoder input length: 56
2025-04-17 13:35:16,096 - espnet.nets.beam_search - INFO - max output length: 56
2025-04-17 13:35:16,096 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 13:35:16,465 - espnet.nets.beam_search - INFO - end detected at 15
2025-04-17 13:35:16,466 - espnet.nets.beam_search - INFO -  -3.45 * 0.7 =  -2.41 for decoder
2025-04-17 13:35:16,467 - espnet.nets.beam_search - INFO -  -3.75 * 0.3 =  -1.13 for ctc
2025-04-17 13:35:16,467 - espnet.nets.beam_search - INFO - total log probability: -3.54
2025-04-17 13:35:16,467 - espnet.nets.beam_search - INFO - normalized log probability: -0.35
2025-04-17 13:35:16,467 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 36
2025-04-17 13:35:16,468 - espnet.nets.beam_search - INFO - best hypo: ▁HALLO▁MY▁NAME▁IS▁GARRIMA

2025-04-17 13:36:32,911 - asr_multi - INFO - Saved sample debug_samples/input_20250417_133632.wav (3.66 s)
2025-04-17 13:36:32,912 - espnet2.bin.asr_inference - INFO - speech length: 58560
2025-04-17 13:36:32,962 - espnet.nets.beam_search - INFO - decoder input length: 56
2025-04-17 13:36:32,962 - espnet.nets.beam_search - INFO - max output length: 56
2025-04-17 13:36:32,963 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 13:36:33,458 - espnet.nets.beam_search - INFO - end detected at 14
2025-04-17 13:36:33,458 - espnet.nets.beam_search - INFO - -11.67 * 0.7 =  -8.17 for decoder
2025-04-17 13:36:33,458 - espnet.nets.beam_search - INFO - -11.49 * 0.3 =  -3.45 for ctc
2025-04-17 13:36:33,459 - espnet.nets.beam_search - INFO - -30.67 * 1.0 = -30.67 for lm
2025-04-17 13:36:33,459 - espnet.nets.beam_search - INFO - total log probability: -42.28
2025-04-17 13:36:33,460 - espnet.nets.beam_search - INFO - normalized log probability: -4.23
2025-04-17 13:36:33,460 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 23
2025-04-17 13:36:33,460 - espnet.nets.beam_search - INFO - best hypo: ▁HELLO▁MY▁NAME▁IS▁GARRY

2025-04-17 13:37:29,291 - asr_multi - INFO - Starting multi-model ASR/TTS demo
2025-04-17 13:37:29,291 - asr_multi - INFO - Using device: cuda
2025-04-17 13:37:29,298 - asr_multi - INFO - Downloading ASR model 'Conformer6' → kamo-naoyuki/librispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp_valid.acc.ave
2025-04-17 13:37:40,376 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp/config.yaml
2025-04-17 13:37:41,253 - root - INFO - Vocabulary size: 5000
2025-04-17 13:37:41,325 - root - WARNING - Using legacy_rel_pos and it will be deprecated in the future.
2025-04-17 13:37:41,382 - root - WARNING - Using legacy_rel_selfattn and it will be deprecated in the future.
2025-04-17 13:37:41,862 - root - INFO - Gradient checkpoint layers: []
2025-04-17 13:37:43,316 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/exp/lm_train_lm_transformer2_bpe5000_scheduler_confwarmup_steps25000_batch_bins500000000_accum_grad2_use_amptrue/config.yaml
2025-04-17 13:37:44,174 - root - INFO - Vocabulary size: 5000
2025-04-17 13:37:44,178 - root - INFO - encoder self-attention layer type = self-attention
2025-04-17 13:37:44,568 - espnet2.bin.asr_inference - INFO - BatchBeamSearch implementation is selected.
2025-04-17 13:37:44,598 - espnet2.bin.asr_inference - INFO - Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (lm): TransformerLM(
      (embed): Embedding(5000, 128)
      (encoder): Encoder(
        (embed): Sequential(
          (0): Linear(in_features=128, out_features=512, bias=True)
          (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (2): Dropout(p=0.0, inplace=False)
          (3): ReLU()
          (4): Sequential()
        )
        (encoders): MultiSequential(
          (0): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (1): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (2): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (3): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (4): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (5): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (6): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (7): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (8): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (9): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (10): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (11): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (12): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (13): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (14): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (15): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linear_q): Linear(in_features=512, out_features=512, bias=True)
              (linear_k): Linear(in_features=512, out_features=512, bias=True)
              (linear_v): Linear(in_features=512, out_features=512, bias=True)
              (linear_out): Linear(in_features=512, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (q_norm): Identity()
              (k_norm): Identity()
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (activation): ReLU()
            )
            (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
        )
        (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): Linear(in_features=512, out_features=5000, bias=True)
    )
  )
)
2025-04-17 13:37:44,600 - espnet2.bin.asr_inference - INFO - Decoding device=cuda, dtype=float32
2025-04-17 13:37:44,605 - espnet2.bin.asr_inference - INFO - Text tokenizer: SentencepiecesTokenizer(model="/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/9366b56e39a93ce1189ae2a3fccb1475/data/en_token_list/bpe_unigram5000/bpe.model")
2025-04-17 13:37:44,606 - asr_multi - INFO - Loaded ESPnet ASR model: Conformer6
2025-04-17 13:37:44,606 - asr_multi - INFO - Downloading ASR model 'Conformer8' → pyf98/librispeech_conformer
2025-04-17 13:37:44,692 - root - INFO - config file: /home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--pyf98--librispeech_conformer/snapshots/b74448c3b089eceea913735bd59a76d5252d8eec/exp/asr_train_asr_conformer8_raw_en_bpe5000_sp/config.yaml
2025-04-17 13:37:45,544 - root - INFO - Vocabulary size: 5000
2025-04-17 13:37:45,973 - root - INFO - Gradient checkpoint layers: []
2025-04-17 13:37:46,616 - espnet2.bin.asr_inference - INFO - BatchBeamSearch implementation is selected.
2025-04-17 13:37:46,628 - espnet2.bin.asr_inference - INFO - Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2025-04-17 13:37:46,629 - espnet2.bin.asr_inference - INFO - Decoding device=cuda, dtype=float32
2025-04-17 13:37:46,631 - espnet2.bin.asr_inference - INFO - Text tokenizer: SentencepiecesTokenizer(model="/home/ambuja/miniconda3/envs/emo-tts-new/lib/python3.8/site-packages/espnet_model_zoo/models--pyf98--librispeech_conformer/snapshots/b74448c3b089eceea913735bd59a76d5252d8eec/data/en_token_list/bpe_unigram5000/bpe.model")
2025-04-17 13:37:46,631 - asr_multi - INFO - Loaded ESPnet ASR model: Conformer8
2025-04-17 13:37:46,632 - asr_multi - INFO - Loading Whisper large as gold ASR…
2025-04-17 13:38:03,386 - asr_multi - INFO - Launching Gradio app…
2025-04-17 13:38:31,984 - asr_multi - INFO - Saved sample debug_samples/input_20250417_133831.wav (3.12 s)
2025-04-17 13:38:31,986 - espnet2.bin.asr_inference - INFO - speech length: 49920
2025-04-17 13:38:32,315 - espnet.nets.beam_search - INFO - decoder input length: 48
2025-04-17 13:38:32,316 - espnet.nets.beam_search - INFO - max output length: 48
2025-04-17 13:38:32,316 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 13:38:32,596 - espnet.nets.beam_search - INFO - end detected at 11
2025-04-17 13:38:32,596 - espnet.nets.beam_search - INFO -  -2.83 * 0.7 =  -1.98 for decoder
2025-04-17 13:38:32,597 - espnet.nets.beam_search - INFO -  -2.37 * 0.3 =  -0.71 for ctc
2025-04-17 13:38:32,597 - espnet.nets.beam_search - INFO - total log probability: -2.69
2025-04-17 13:38:32,597 - espnet.nets.beam_search - INFO - normalized log probability: -0.38
2025-04-17 13:38:32,598 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 29
2025-04-17 13:38:32,598 - espnet.nets.beam_search - INFO - best hypo: ▁I▁LOVE▁GOD▁IN▁MAMMA

2025-04-17 13:41:19,751 - asr_multi - INFO - Saved sample debug_samples/input_20250417_134119.wav (4.62 s)
2025-04-17 13:41:19,752 - espnet2.bin.asr_inference - INFO - speech length: 73920
2025-04-17 13:41:19,802 - espnet.nets.beam_search - INFO - decoder input length: 71
2025-04-17 13:41:19,803 - espnet.nets.beam_search - INFO - max output length: 71
2025-04-17 13:41:19,803 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 13:41:20,371 - espnet.nets.beam_search - INFO - end detected at 16
2025-04-17 13:41:20,372 - espnet.nets.beam_search - INFO -  -4.82 * 0.7 =  -3.38 for decoder
2025-04-17 13:41:20,372 - espnet.nets.beam_search - INFO -  -3.04 * 0.3 =  -0.91 for ctc
2025-04-17 13:41:20,372 - espnet.nets.beam_search - INFO - total log probability: -4.29
2025-04-17 13:41:20,373 - espnet.nets.beam_search - INFO - normalized log probability: -0.39
2025-04-17 13:41:20,373 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 40
2025-04-17 13:41:20,373 - espnet.nets.beam_search - INFO - best hypo: ▁GARRIMA▁IS▁NOT▁GOING▁TO▁GO▁UP

2025-04-17 14:20:37,548 - asr_multi - INFO - Saved sample debug_samples/input_20250417_142037.wav (2.79 s)
2025-04-17 14:20:37,548 - espnet2.bin.asr_inference - INFO - speech length: 44715
2025-04-17 14:20:37,599 - espnet.nets.beam_search - INFO - decoder input length: 43
2025-04-17 14:20:37,599 - espnet.nets.beam_search - INFO - max output length: 43
2025-04-17 14:20:37,600 - espnet.nets.beam_search - INFO - min output length: 0
2025-04-17 14:20:37,826 - espnet.nets.beam_search - INFO - end detected at 12
2025-04-17 14:20:37,827 - espnet.nets.beam_search - INFO -  -5.17 * 0.7 =  -3.62 for decoder
2025-04-17 14:20:37,827 - espnet.nets.beam_search - INFO -  -4.47 * 0.3 =  -1.34 for ctc
2025-04-17 14:20:37,828 - espnet.nets.beam_search - INFO - total log probability: -4.96
2025-04-17 14:20:37,828 - espnet.nets.beam_search - INFO - normalized log probability: -0.62
2025-04-17 14:20:37,828 - espnet.nets.beam_search - INFO - total number of ended hypotheses: 32
2025-04-17 14:20:37,829 - espnet.nets.beam_search - INFO - best hypo: ▁SHELTERED▁ALONG▁INTO▁THE▁WILDERNESS

